{"nbformat":4, "nbformat_minor":0,"metadata":{"colab":{"name":"Lecture07.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNQhAN6WCO0ELFaa5or1r7o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XaQL0MfVyXM4"},"source":["## PytorchZeroToAll. Lec07"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o3wbF02WwaOQ","executionInfo":{"status":"ok","timestamp":1632757237589,"user_tz":-540,"elapsed":479,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"762af223-3715-4cbd-ccd2-28270a37b791"},"source":["from torch import nn, optim, from_numpy\n","import numpy as np\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","xy = np.loadtxt('/content/drive/My Drive/Colab Notebooks/data/diabetes.csv.gz', delimiter=',',dtype=np.float32)\n","x_data = from_numpy(xy[:, 0:-1])\n","y_data = from_numpy(xy[:, [-1]])\n","print(f\"XW's shape: {x_data.shape} | YW's shape: {y_data.shape}\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","XW's shape: torch.Size([759, 8]) | YW's shape: torch.Size([759, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"RIn2KPHJyl9C","executionInfo":{"status":"ok","timestamp":1632757237995,"user_tz":-540,"elapsed":7,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["import torch\n","\n","class Model(nn.Module):\n","  def __init__(self):\n","    \"\"\"\n","    In the constructor we instantiate nn.Linear module\n","    \"\"\"\n","    super(Model, self).__init__()\n","    self.l1 = nn.Linear(8,6)\n","    self.l2 = nn.Linear(6,4)\n","    self.l3 = nn.Linear(4,1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    \"\"\"\n","    In the forward function we accept a Variable of input data and we must return\n","    a Variable of output data. We can use Modules defined in the constructor as \n","    well as arbitary operators on Variables.\n","    \"\"\"\n","    out1 = self.sigmoid(self.l1(x))\n","    out2 = self.sigmoid(self.l2(out1))\n","    y_pred = self.sigmoid(self.l3(out2))\n","    return y_pred\n","\n","# our model\n","model = Model()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"LgbS16Z4y0cf","executionInfo":{"status":"ok","timestamp":1632757237996,"user_tz":-540,"elapsed":7,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["# Construct our loss function and an Optimizer. The call to model.parameters()\n","# in the SGD constructor will contaion the learnable parameters of the three\n","# nn.Linear modules which are members of the model.\n","\n","criterion = torch.nn.BCELoss(reduction='mean')\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NlgeER5py-Br","executionInfo":{"status":"ok","timestamp":1632757238691,"user_tz":-540,"elapsed":701,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"2993455d-0bf2-4c3b-e310-cd5164370d2b"},"source":["# Training loop\n","for epoch in range(100):\n","  # Forward pass: Compute predicted y by passing x to the model\n","  y_pred = model(x_data)\n","\n","  # Compute and print loss\n","  loss = criterion(y_pred, y_data)\n","  print(f'Epoch: {epoch+1}/1000 | Loss: {loss.item(): .4f}')\n","  \n","  # Zero gradients, perform a backward pass, and update the weights.\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/1000 | Loss:  0.6719\n","Epoch: 2/1000 | Loss:  0.6717\n","Epoch: 3/1000 | Loss:  0.6714\n","Epoch: 4/1000 | Loss:  0.6712\n","Epoch: 5/1000 | Loss:  0.6709\n","Epoch: 6/1000 | Loss:  0.6707\n","Epoch: 7/1000 | Loss:  0.6705\n","Epoch: 8/1000 | Loss:  0.6702\n","Epoch: 9/1000 | Loss:  0.6700\n","Epoch: 10/1000 | Loss:  0.6698\n","Epoch: 11/1000 | Loss:  0.6696\n","Epoch: 12/1000 | Loss:  0.6693\n","Epoch: 13/1000 | Loss:  0.6691\n","Epoch: 14/1000 | Loss:  0.6689\n","Epoch: 15/1000 | Loss:  0.6687\n","Epoch: 16/1000 | Loss:  0.6685\n","Epoch: 17/1000 | Loss:  0.6683\n","Epoch: 18/1000 | Loss:  0.6681\n","Epoch: 19/1000 | Loss:  0.6678\n","Epoch: 20/1000 | Loss:  0.6676\n","Epoch: 21/1000 | Loss:  0.6674\n","Epoch: 22/1000 | Loss:  0.6672\n","Epoch: 23/1000 | Loss:  0.6670\n","Epoch: 24/1000 | Loss:  0.6668\n","Epoch: 25/1000 | Loss:  0.6666\n","Epoch: 26/1000 | Loss:  0.6665\n","Epoch: 27/1000 | Loss:  0.6663\n","Epoch: 28/1000 | Loss:  0.6661\n","Epoch: 29/1000 | Loss:  0.6659\n","Epoch: 30/1000 | Loss:  0.6657\n","Epoch: 31/1000 | Loss:  0.6655\n","Epoch: 32/1000 | Loss:  0.6653\n","Epoch: 33/1000 | Loss:  0.6651\n","Epoch: 34/1000 | Loss:  0.6650\n","Epoch: 35/1000 | Loss:  0.6648\n","Epoch: 36/1000 | Loss:  0.6646\n","Epoch: 37/1000 | Loss:  0.6644\n","Epoch: 38/1000 | Loss:  0.6643\n","Epoch: 39/1000 | Loss:  0.6641\n","Epoch: 40/1000 | Loss:  0.6639\n","Epoch: 41/1000 | Loss:  0.6638\n","Epoch: 42/1000 | Loss:  0.6636\n","Epoch: 43/1000 | Loss:  0.6634\n","Epoch: 44/1000 | Loss:  0.6633\n","Epoch: 45/1000 | Loss:  0.6631\n","Epoch: 46/1000 | Loss:  0.6629\n","Epoch: 47/1000 | Loss:  0.6628\n","Epoch: 48/1000 | Loss:  0.6626\n","Epoch: 49/1000 | Loss:  0.6625\n","Epoch: 50/1000 | Loss:  0.6623\n","Epoch: 51/1000 | Loss:  0.6622\n","Epoch: 52/1000 | Loss:  0.6620\n","Epoch: 53/1000 | Loss:  0.6618\n","Epoch: 54/1000 | Loss:  0.6617\n","Epoch: 55/1000 | Loss:  0.6616\n","Epoch: 56/1000 | Loss:  0.6614\n","Epoch: 57/1000 | Loss:  0.6613\n","Epoch: 58/1000 | Loss:  0.6611\n","Epoch: 59/1000 | Loss:  0.6610\n","Epoch: 60/1000 | Loss:  0.6608\n","Epoch: 61/1000 | Loss:  0.6607\n","Epoch: 62/1000 | Loss:  0.6606\n","Epoch: 63/1000 | Loss:  0.6604\n","Epoch: 64/1000 | Loss:  0.6603\n","Epoch: 65/1000 | Loss:  0.6601\n","Epoch: 66/1000 | Loss:  0.6600\n","Epoch: 67/1000 | Loss:  0.6599\n","Epoch: 68/1000 | Loss:  0.6598\n","Epoch: 69/1000 | Loss:  0.6596\n","Epoch: 70/1000 | Loss:  0.6595\n","Epoch: 71/1000 | Loss:  0.6594\n","Epoch: 72/1000 | Loss:  0.6592\n","Epoch: 73/1000 | Loss:  0.6591\n","Epoch: 74/1000 | Loss:  0.6590\n","Epoch: 75/1000 | Loss:  0.6589\n","Epoch: 76/1000 | Loss:  0.6587\n","Epoch: 77/1000 | Loss:  0.6586\n","Epoch: 78/1000 | Loss:  0.6585\n","Epoch: 79/1000 | Loss:  0.6584\n","Epoch: 80/1000 | Loss:  0.6583\n","Epoch: 81/1000 | Loss:  0.6582\n","Epoch: 82/1000 | Loss:  0.6580\n","Epoch: 83/1000 | Loss:  0.6579\n","Epoch: 84/1000 | Loss:  0.6578\n","Epoch: 85/1000 | Loss:  0.6577\n","Epoch: 86/1000 | Loss:  0.6576\n","Epoch: 87/1000 | Loss:  0.6575\n","Epoch: 88/1000 | Loss:  0.6574\n","Epoch: 89/1000 | Loss:  0.6573\n","Epoch: 90/1000 | Loss:  0.6572\n","Epoch: 91/1000 | Loss:  0.6571\n","Epoch: 92/1000 | Loss:  0.6570\n","Epoch: 93/1000 | Loss:  0.6568\n","Epoch: 94/1000 | Loss:  0.6567\n","Epoch: 95/1000 | Loss:  0.6566\n","Epoch: 96/1000 | Loss:  0.6565\n","Epoch: 97/1000 | Loss:  0.6564\n","Epoch: 98/1000 | Loss:  0.6563\n","Epoch: 99/1000 | Loss:  0.6562\n","Epoch: 100/1000 | Loss:  0.6561\n"]}]},{"cell_type":"markdown","metadata":{"id":"I20xwAwizZwp"},"source":["## PytorchZeroToAll. Lec08"]},{"cell_type":"code","metadata":{"id":"UGDJjwoizap4","executionInfo":{"status":"ok","timestamp":1632757238696,"user_tz":-540,"elapsed":20,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["from torch.utils.data import Dataset, DataLoader\n","from torch import from_numpy, tensor\n","import numpy as np\n","\n","class DiabetesDataset(Dataset):\n","  \"\"\" Diabetes Dataset \"\"\"\n","\n","  # Initialize your data, download, etc.\n","  def __init__(self):\n","    xy = np.loadtxt('/content/drive/My Drive/Colab Notebooks/data/diabetes.csv.gz', delimiter=',',dtype=np.float32)\n","    self.len = xy.shape[0]\n","    self.x_data = from_numpy(xy[:, 0:-1])\n","    self.y_data = from_numpy(xy[:, [-1]])\n","\n","  def __getitem__(self, index):\n","    return self.x_data[index], self.y_data[index]\n","\n","  def __len__(self):\n","    return self.len\n","\n","dataset = DiabetesDataset()\n","\n","train_loader = DataLoader(dataset=dataset, \n","                          batch_size=32,\n","                          shuffle=True, \n","                          num_workers=2)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xxTDPOcE0Qwv","executionInfo":{"status":"ok","timestamp":1632757239331,"user_tz":-540,"elapsed":653,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"dfdf7cc2-93c4-4ed6-9617-05ae85d2a081"},"source":["for epoch in range(2):\n","  for i, data in enumerate(train_loader, 0):\n","    # get the inputs\n","    inputs, labels = data\n","\n","    # wrap them in Variable\n","    inputs, labels = tensor(inputs), tensor(labels)\n","\n","    # Run your training process\n","    print(f'Epoch: {i} | Inputs {inputs.data} | Labels {labels.data}')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Inputs tensor([[-0.5294,  0.1558,  0.1803,  0.0000,  0.0000, -0.1386, -0.7455, -0.1667],\n","        [-0.8824, -0.0452,  0.3443, -0.4949, -0.5745,  0.0432, -0.8676, -0.2667],\n","        [-0.1765,  0.0653, -0.0164, -0.5152,  0.0000, -0.2101, -0.8138, -0.7333],\n","        [-0.4118,  0.8995,  0.0492, -0.3333, -0.2317, -0.0700, -0.5687, -0.7333],\n","        [-0.0588,  0.7688,  0.4754, -0.3131, -0.2908,  0.0045, -0.6678,  0.2333],\n","        [-0.5294,  0.1156,  0.1803, -0.0505, -0.5106,  0.1058,  0.1204,  0.1667],\n","        [-0.1765,  0.9598,  0.1475, -0.3333, -0.6572, -0.2519, -0.9274,  0.1333],\n","        [-0.5294,  0.2965, -0.0164, -0.7576, -0.4539, -0.1803, -0.6166, -0.6667],\n","        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667],\n","        [ 0.0000, -0.0553,  0.1475, -0.4545, -0.7281,  0.2966, -0.7703,  0.0000],\n","        [-0.6471, -0.0352, -0.0820, -0.3131, -0.7281, -0.2638, -0.2605, -0.4000],\n","        [-0.8824, -0.2864,  0.2787,  0.0101, -0.8936, -0.0104, -0.7062,  0.0000],\n","        [-0.1765,  0.2462,  0.1475, -0.3333, -0.4917, -0.2399, -0.9291, -0.4667],\n","        [-0.8824, -0.2060,  0.3115, -0.4949, -0.9125, -0.2429, -0.5687, -0.9667],\n","        [-0.6471,  0.8291,  0.2131,  0.0000,  0.0000, -0.0909, -0.7720, -0.7333],\n","        [-0.8824,  0.7387,  0.2131,  0.0000,  0.0000,  0.0969, -0.9915, -0.4333],\n","        [-0.4118, -0.2663, -0.0164,  0.0000,  0.0000, -0.2012, -0.8377, -0.8000],\n","        [-0.4118,  0.4774,  0.2787,  0.0000,  0.0000,  0.0045, -0.8804,  0.4667],\n","        [ 0.0000,  0.7990,  0.4754, -0.4545,  0.0000,  0.3145, -0.4808, -0.9333],\n","        [-0.7647,  0.2965,  0.3770,  0.0000,  0.0000, -0.1654, -0.8241, -0.8000],\n","        [-0.6471, -0.0050,  0.3115, -0.7778, -0.8487, -0.4247, -0.8241, -0.7000],\n","        [-0.2941,  0.3467,  0.3115, -0.2525, -0.1253,  0.3770, -0.8634, -0.1667],\n","        [-0.8824,  0.6884,  0.4426, -0.4141,  0.0000,  0.0432, -0.2938,  0.0333],\n","        [-0.6471,  0.1357, -0.1803, -0.7980, -0.7991, -0.1207, -0.5320, -0.8667],\n","        [ 0.0000, -0.0452,  0.3115, -0.0909, -0.7825,  0.0879, -0.7848, -0.8333],\n","        [-0.2941,  0.0352,  0.0820,  0.0000,  0.0000, -0.2757, -0.8540, -0.7333],\n","        [-0.6471,  0.0251,  0.2131,  0.0000,  0.0000, -0.1207, -0.9633, -0.6333],\n","        [-0.8824, -0.1156, -0.5082, -0.1515, -0.7660,  0.6393, -0.6430, -0.8333],\n","        [-0.0588,  0.5578,  0.0164, -0.4747,  0.1702,  0.0134, -0.6029, -0.1667],\n","        [-0.4118, -0.5578,  0.0164,  0.0000,  0.0000, -0.2548, -0.5653, -0.5000],\n","        [-0.5294, -0.0553,  0.0656, -0.5556,  0.0000, -0.2638, -0.9402,  0.0000],\n","        [-0.8824, -0.0653,  0.1475, -0.3737,  0.0000, -0.0939, -0.7976, -0.9333]]) | Labels tensor([[0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 1 | Inputs tensor([[-0.2941,  0.1759,  0.5738,  0.0000,  0.0000, -0.1446, -0.9325, -0.7000],\n","        [-0.0588,  0.1055,  0.2459,  0.0000,  0.0000, -0.1714, -0.8642,  0.2333],\n","        [-0.6471,  0.4171,  0.0000,  0.0000,  0.0000, -0.1058, -0.4167, -0.8000],\n","        [-0.5294,  0.2965,  0.4098, -0.5960, -0.3617,  0.0462, -0.8693, -0.9333],\n","        [ 0.0000,  0.6583,  0.2459, -0.1313, -0.3972,  0.4277, -0.8454, -0.8333],\n","        [-0.6471,  0.1156, -0.0820, -0.2121,  0.0000, -0.1028, -0.5909, -0.7000],\n","        [-0.5294, -0.0452, -0.0164, -0.3535,  0.0000,  0.0551, -0.8241, -0.7667],\n","        [-0.4118,  0.2362,  0.2131, -0.1919, -0.8180,  0.0164, -0.8369, -0.7667],\n","        [-0.4118,  0.6683,  0.2459,  0.0000,  0.0000,  0.3621, -0.7763, -0.8000],\n","        [ 0.0000,  0.0251,  0.4098, -0.6566, -0.7518, -0.1267, -0.4731, -0.8000],\n","        [-0.4118,  0.3769,  0.7705,  0.0000,  0.0000,  0.4545, -0.8728, -0.4667],\n","        [-0.0588,  0.1256,  0.1803,  0.0000,  0.0000, -0.2966, -0.3493,  0.2333],\n","        [-0.6471, -0.1558,  0.1148, -0.3939, -0.7494, -0.0492, -0.5619, -0.8667],\n","        [-0.0588, -0.0452,  0.1803,  0.0000,  0.0000,  0.0969, -0.6524,  0.2000],\n","        [-0.4118,  0.1658,  0.2131, -0.4141,  0.0000, -0.0373, -0.5030, -0.5333],\n","        [-0.0588,  0.5477,  0.2787, -0.3535,  0.0000, -0.0343, -0.6883, -0.2000],\n","        [ 0.0000,  0.2462,  0.1475, -0.5960,  0.0000, -0.1833, -0.8497, -0.5000],\n","        [ 0.5294,  0.0653,  0.1475,  0.0000,  0.0000,  0.0194, -0.8523,  0.0333],\n","        [-0.8824, -0.2864, -0.2131, -0.6364, -0.8203, -0.3920, -0.7908, -0.9667],\n","        [-0.5294,  0.8392,  0.0000,  0.0000,  0.0000, -0.1535, -0.8856, -0.5000],\n","        [-0.7647,  0.2864,  0.0492, -0.1515,  0.0000,  0.1922, -0.1264, -0.9000],\n","        [-0.1765, -0.3769,  0.2787,  0.0000,  0.0000, -0.0283, -0.7327, -0.3333],\n","        [-0.7647,  0.1156, -0.0164,  0.0000,  0.0000, -0.2191, -0.7737, -0.9333],\n","        [-0.8824,  0.4372,  0.2131, -0.5556, -0.8558, -0.2191, -0.8480,  0.0000],\n","        [-0.7647,  0.1256,  0.2295, -0.3535,  0.0000,  0.0641, -0.9402,  0.0000],\n","        [ 0.4118,  0.5176,  0.1475, -0.1919, -0.3593,  0.2459, -0.4330, -0.4333],\n","        [-0.6471,  0.1357, -0.2787, -0.7374,  0.0000, -0.3323, -0.9471, -0.9667],\n","        [ 0.0588,  0.5678,  0.4098,  0.0000,  0.0000, -0.2608, -0.8702,  0.0667],\n","        [-0.8824,  0.3970, -0.2459, -0.6162, -0.8038, -0.1446, -0.5081, -0.9667],\n","        [-0.4118,  0.3970,  0.0492, -0.2929, -0.6690, -0.1475, -0.7156, -0.8333],\n","        [ 0.1765,  0.3970,  0.3115,  0.0000,  0.0000, -0.1922,  0.1640,  0.2000],\n","        [-0.8824,  0.3065,  0.1475, -0.7374, -0.7518, -0.2280, -0.6635, -0.9667]]) | Labels tensor([[1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 2 | Inputs tensor([[-0.7647,  0.2261,  0.2459, -0.4545, -0.5272,  0.0700, -0.6541, -0.8333],\n","        [-0.4118,  0.0452,  0.2131,  0.0000,  0.0000, -0.1416, -0.9360, -0.1000],\n","        [-0.4118, -0.0352,  0.2131, -0.6364, -0.8416,  0.0015, -0.2152, -0.2667],\n","        [-0.4118,  0.2261,  0.4098,  0.0000,  0.0000,  0.0343, -0.8190, -0.6000],\n","        [-0.8824,  0.2563,  0.1475, -0.5152, -0.7400, -0.2757, -0.8779, -0.8667],\n","        [ 0.0588,  0.2462,  0.1475, -0.3333, -0.0496,  0.0551, -0.8258, -0.5667],\n","        [-0.2941, -0.1256,  0.3115,  0.0000,  0.0000, -0.3085, -0.9949, -0.6333],\n","        [ 0.0000,  0.9900,  0.0820, -0.3535, -0.3522,  0.2310, -0.6379, -0.7667],\n","        [ 0.0000, -0.0653, -0.0164,  0.0000,  0.0000,  0.0522, -0.8420, -0.8667],\n","        [-0.6471,  0.7387,  0.3770, -0.3333,  0.1206,  0.0641, -0.8463, -0.9667],\n","        [-0.2941,  0.8392,  0.5410,  0.0000,  0.0000,  0.2161,  0.1810, -0.2000],\n","        [-0.6471,  0.3065,  0.0492,  0.0000,  0.0000, -0.3115, -0.7985, -0.9667],\n","        [ 0.0000,  0.1960,  0.0820, -0.4545,  0.0000,  0.1565, -0.8454, -0.9667],\n","        [ 0.1765,  0.2261,  0.2787, -0.3737,  0.0000, -0.1773, -0.6294, -0.2000],\n","        [-0.7647,  0.9799,  0.1475,  1.0000,  0.0000,  0.0343, -0.5756,  0.3667],\n","        [ 0.0000,  0.1960,  0.0492, -0.6364, -0.7825,  0.0402, -0.4475, -0.9333],\n","        [ 0.0000,  0.0955,  0.4426, -0.3939,  0.0000, -0.0313, -0.3365, -0.4333],\n","        [-0.6471,  0.7186,  0.1803, -0.3333, -0.6809, -0.0075, -0.8967, -0.9000],\n","        [-0.5294,  0.5477,  0.1803, -0.4141, -0.7021, -0.0671, -0.7780, -0.4667],\n","        [ 0.1765, -0.0754,  0.0164,  0.0000,  0.0000, -0.2280, -0.9240, -0.6667],\n","        [-0.0588,  0.8894,  0.2787,  0.0000,  0.0000,  0.4277, -0.9496, -0.2667],\n","        [ 0.2941,  0.2060,  0.3115, -0.2525, -0.6454,  0.2608, -0.3962, -0.1000],\n","        [-0.6471,  0.2462,  0.3115, -0.3333, -0.6927, -0.0104, -0.8061, -0.8333],\n","        [-0.6471, -0.2161,  0.1475,  0.0000,  0.0000, -0.0313, -0.8360, -0.4000],\n","        [ 0.0000, -0.0452,  0.3934, -0.4949, -0.9149,  0.1148, -0.8557, -0.9000],\n","        [-0.8824,  0.0000,  0.1148, -0.2929,  0.0000, -0.0462, -0.7344, -0.9667],\n","        [-0.7647,  0.0553, -0.0492, -0.1919, -0.7778,  0.0402, -0.8745, -0.8667],\n","        [-0.2941,  0.4472,  0.1803, -0.4545, -0.4610,  0.0104, -0.8488, -0.3667],\n","        [-0.7647,  0.5578, -0.1475, -0.4545,  0.2766,  0.1535, -0.8617, -0.8667],\n","        [-0.1765, -0.1859,  0.2787, -0.1919, -0.8865,  0.3920, -0.8437, -0.3000],\n","        [-0.1765,  0.4774,  0.2459,  0.0000,  0.0000,  0.1744, -0.8471, -0.2667],\n","        [-0.7647,  0.4673,  0.2459, -0.2929, -0.5414,  0.1386, -0.7857, -0.7333]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.]])\n","Epoch: 3 | Inputs tensor([[-0.4118,  0.5578,  0.3770, -0.1111,  0.2884,  0.1535, -0.5380, -0.5667],\n","        [-0.8824,  0.2261,  0.0492, -0.3535, -0.6312,  0.0462, -0.4757, -0.7000],\n","        [-0.4118,  0.1558,  0.6066,  0.0000,  0.0000,  0.5768, -0.8881, -0.7667],\n","        [ 0.0000,  0.6281,  0.2459,  0.1313, -0.7636,  0.5857, -0.4184, -0.8667],\n","        [-0.7647,  0.5779,  0.2131, -0.2929,  0.0402,  0.1744, -0.9522, -0.7000],\n","        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],\n","        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],\n","        [-0.5294,  0.2864,  0.1475,  0.0000,  0.0000,  0.0224, -0.8079, -0.9000],\n","        [-0.8824, -0.1658,  0.1148,  0.0000,  0.0000, -0.4575, -0.5337, -0.8000],\n","        [-0.7647, -0.2462,  0.0492, -0.5152, -0.8700, -0.1148, -0.7506, -0.6000],\n","        [-0.0588,  0.0854,  0.1475,  0.0000,  0.0000, -0.0909, -0.2511, -0.6000],\n","        [-0.1765,  0.8492,  0.3770, -0.3333,  0.0000,  0.0581, -0.7635, -0.3333],\n","        [ 0.0000,  0.0151,  0.0164,  0.0000,  0.0000, -0.3472, -0.7797, -0.8667],\n","        [-0.4118, -0.2261,  0.3443, -0.1717, -0.9007,  0.0671, -0.9334, -0.5333],\n","        [ 0.1765,  0.2965,  0.0164, -0.2727,  0.0000,  0.2280, -0.6900, -0.4333],\n","        [-0.2941,  0.5176,  0.0164, -0.3737, -0.7163,  0.0581, -0.4757, -0.7667],\n","        [-0.7647,  0.1256,  0.2787,  0.0101, -0.6690,  0.1744, -0.9172, -0.9000],\n","        [ 0.5294,  0.4573,  0.3443, -0.6162, -0.7400, -0.3383, -0.8574,  0.2000],\n","        [-0.0588,  0.0553,  0.6393, -0.2727,  0.0000,  0.2906, -0.8625, -0.2000],\n","        [-0.8824,  0.3668,  0.2131,  0.0101, -0.5177,  0.1148, -0.7259, -0.9000],\n","        [ 0.0000,  0.0050,  0.1475, -0.4747, -0.8818, -0.0820, -0.5568,  0.0000],\n","        [-0.0588,  0.0955,  0.2459, -0.2121, -0.7305, -0.1684, -0.5201, -0.6667],\n","        [-0.7647,  0.4472, -0.0492, -0.3333, -0.6809, -0.0581, -0.7062, -0.8667],\n","        [-0.7647, -0.3166,  0.1475, -0.3535, -0.8440, -0.2548, -0.9069, -0.8667],\n","        [-0.6471,  0.3266,  0.3115,  0.0000,  0.0000,  0.0253, -0.7233, -0.2333],\n","        [-0.8824, -0.0452, -0.0164, -0.6364, -0.8629, -0.2876, -0.8446, -0.9667],\n","        [ 0.5294, -0.2362, -0.0164,  0.0000,  0.0000, -0.0224, -0.9129, -0.3333],\n","        [-0.8824,  0.4070,  0.2131, -0.4747, -0.5745, -0.2817, -0.3595, -0.9333],\n","        [ 0.4118,  0.2161,  0.2787, -0.6566,  0.0000, -0.2101, -0.8454,  0.3667],\n","        [-0.2941,  0.4774,  0.3115,  0.0000,  0.0000, -0.1207, -0.9146, -0.0333],\n","        [ 0.1765, -0.2462,  0.3443,  0.0000,  0.0000, -0.0075, -0.8420, -0.4333],\n","        [ 0.0000,  0.8090,  0.2787,  0.2727, -0.9669,  0.7705,  1.0000, -0.8667]]) | Labels tensor([[1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.]])\n","Epoch: 4 | Inputs tensor([[-0.5294,  0.1457,  0.0492,  0.0000,  0.0000, -0.1386, -0.9590, -0.9000],\n","        [-0.7647, -0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8070,  0.0000],\n","        [ 0.0588,  0.7186,  0.8033, -0.5152, -0.4326,  0.3532, -0.4509,  0.1000],\n","        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000],\n","        [-0.6471, -0.1256, -0.0164, -0.6364,  0.0000, -0.3502, -0.6874,  0.0000],\n","        [-0.5294,  0.3166,  0.1148, -0.5758, -0.6076, -0.0134, -0.9300, -0.7667],\n","        [-0.0588,  0.2060,  0.2787,  0.0000,  0.0000, -0.2548, -0.7173,  0.4333],\n","        [ 0.0588,  0.1960,  0.3115, -0.2929,  0.0000, -0.1356, -0.8420, -0.7333],\n","        [-0.4118,  0.4472,  0.3443, -0.4747, -0.3262, -0.0462, -0.6806,  0.2333],\n","        [-0.7647, -0.0754, -0.1475,  0.0000,  0.0000, -0.1028, -0.9462, -0.9667],\n","        [ 0.0000, -0.0452,  0.0492, -0.2121, -0.7518,  0.3294, -0.7541, -0.9667],\n","        [-0.8824,  0.0653,  0.1475, -0.4343, -0.6809,  0.0194, -0.9453, -0.9667],\n","        [-0.5294, -0.0050,  0.2459, -0.6970, -0.8794, -0.3085, -0.8762,  0.0000],\n","        [ 0.0000,  0.6281,  0.2459, -0.2727,  0.0000,  0.4784, -0.7558, -0.8333],\n","        [ 0.5294,  0.0653,  0.1803,  0.0909,  0.0000,  0.0909, -0.9146, -0.2000],\n","        [-0.2941,  0.2462,  0.1803,  0.0000,  0.0000, -0.1773, -0.7523, -0.7333],\n","        [ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],\n","        [-0.8824,  0.0754,  0.1148, -0.6162,  0.0000, -0.2101, -0.9257, -0.9000],\n","        [-0.2941,  0.2362,  0.1803, -0.0909, -0.4563,  0.0015, -0.4406, -0.5667],\n","        [-0.0588, -0.0050,  0.3770,  0.0000,  0.0000,  0.0551, -0.7353, -0.0333],\n","        [ 0.0000, -0.1357,  0.1148, -0.3535,  0.0000,  0.0671, -0.8634, -0.8667],\n","        [ 0.0000,  0.7387,  0.2787, -0.3535, -0.3735,  0.3860, -0.0769,  0.2333],\n","        [-0.2941, -0.0754,  0.5082,  0.0000,  0.0000, -0.4069, -0.9061, -0.7667],\n","        [-0.2941,  0.0553,  0.3115, -0.4343,  0.0000, -0.0313, -0.3168, -0.8333],\n","        [ 0.0000,  0.2060,  0.2131, -0.6364, -0.8511, -0.0909, -0.8232, -0.8333],\n","        [-0.7647,  0.0050, -0.1148, -0.4343, -0.7518,  0.1267, -0.6413, -0.9000],\n","        [-0.7647, -0.4372, -0.0820, -0.4343, -0.8936, -0.2787, -0.7831, -0.9667],\n","        [ 0.0000,  0.0151,  0.0656, -0.4343,  0.0000, -0.2668, -0.8642, -0.9667],\n","        [-0.0588,  0.0050,  0.2459,  0.0000,  0.0000,  0.1535, -0.9044, -0.3000],\n","        [-0.2941,  0.6683,  0.2131,  0.0000,  0.0000, -0.2072, -0.8070,  0.5000],\n","        [-0.8824,  0.1558,  0.1475, -0.3939, -0.7731,  0.0313, -0.6149, -0.6333],\n","        [-0.5294,  0.4472,  0.3443, -0.3535,  0.0000,  0.1475, -0.5935, -0.4667]]) | Labels tensor([[1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.]])\n","Epoch: 5 | Inputs tensor([[-0.7647,  0.0553,  0.3115, -0.0909, -0.5485,  0.0045, -0.4594, -0.7333],\n","        [-0.7647,  0.2060, -0.1148,  0.0000,  0.0000, -0.2012, -0.6781, -0.8000],\n","        [-0.4118,  0.0352,  0.7705, -0.2525,  0.0000,  0.1684, -0.8061,  0.4667],\n","        [ 0.0000,  0.2462, -0.0820, -0.7374, -0.7518, -0.3502, -0.6806,  0.0000],\n","        [-0.0588, -0.1457, -0.0984, -0.5960,  0.0000, -0.2727, -0.9505, -0.3000],\n","        [-0.7647,  0.0251,  0.4098, -0.2727, -0.7163,  0.3562, -0.9582, -0.9333],\n","        [ 0.0000,  0.0854,  0.1148, -0.5960,  0.0000, -0.1863, -0.3945, -0.6333],\n","        [-0.4118, -0.0050,  0.2131, -0.4545,  0.0000, -0.1356, -0.8933, -0.6333],\n","        [-0.2941,  0.0251,  0.3443,  0.0000,  0.0000, -0.0820, -0.9129, -0.5000],\n","        [-0.5294,  0.2764,  0.4426, -0.7778, -0.6336,  0.0283, -0.5559, -0.7667],\n","        [-0.7647,  0.3970,  0.2295,  0.0000,  0.0000, -0.2370, -0.9240, -0.7333],\n","        [-0.6471,  0.0754,  0.0164, -0.7374, -0.8865, -0.3174, -0.4876, -0.9333],\n","        [-0.6471,  0.2563, -0.0492,  0.0000,  0.0000, -0.0581, -0.9377, -0.9000],\n","        [-0.6471, -0.0352,  0.2787, -0.2121,  0.0000,  0.1118, -0.8634, -0.3667],\n","        [-0.8824, -0.1055, -0.6066, -0.6162, -0.9409, -0.1714, -0.5892,  0.0000],\n","        [-0.5294,  0.4573,  0.3443, -0.6364,  0.0000, -0.0313, -0.8659,  0.6333],\n","        [-0.7647, -0.1759, -0.1475, -0.5556, -0.7281, -0.1505,  0.3843, -0.8667],\n","        [-0.4118,  0.6281,  0.7049,  0.0000,  0.0000,  0.1237, -0.9377,  0.0333],\n","        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n","        [-0.6471,  0.1156,  0.4754, -0.7576, -0.8156, -0.1535, -0.6439, -0.7333],\n","        [-0.4118,  0.4372,  0.2787,  0.0000,  0.0000,  0.3413, -0.9044, -0.1333],\n","        [-0.5294,  0.5879,  0.2787,  0.0000,  0.0000, -0.0194, -0.3809, -0.6667],\n","        [-0.4118,  0.1558,  0.2459,  0.0000,  0.0000, -0.0700, -0.7737, -0.2333],\n","        [-0.7647,  0.0050,  0.1148, -0.4949, -0.8322,  0.1475, -0.7899, -0.8333],\n","        [-0.7647, -0.0754,  0.2459, -0.5960,  0.0000, -0.2787,  0.3834, -0.7667],\n","        [ 0.0000,  0.3266,  0.2787,  0.0000,  0.0000, -0.0343, -0.7310,  0.0000],\n","        [ 0.0000,  0.0754,  0.0164, -0.3939, -0.8251,  0.0909, -0.4202, -0.8667],\n","        [-0.4118,  0.1156,  0.1803, -0.4343,  0.0000, -0.2876, -0.7190, -0.8000],\n","        [-0.8824,  0.1357,  0.0492, -0.2929,  0.0000,  0.0015, -0.6029,  0.0000],\n","        [-0.0588,  0.4372,  0.0820,  0.0000,  0.0000,  0.0402, -0.9564, -0.3333],\n","        [-0.8824,  0.0050,  0.1803, -0.7576, -0.8345, -0.2459, -0.5047, -0.7667],\n","        [-0.7647, -0.1055,  0.4754, -0.3939,  0.0000, -0.0015, -0.8173, -0.3000]]) | Labels tensor([[0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.]])\n","Epoch: 6 | Inputs tensor([[-0.5294, -0.0251, -0.0164, -0.5354,  0.0000, -0.1595, -0.6883, -0.9667],\n","        [-0.7647, -0.0754,  0.0164, -0.4343,  0.0000, -0.0581, -0.9556, -0.9000],\n","        [ 0.1765,  0.0151,  0.2459, -0.0303, -0.5745, -0.0194, -0.9206,  0.4000],\n","        [ 0.1765,  0.2261,  0.1148,  0.0000,  0.0000, -0.0700, -0.8463, -0.3333],\n","        [-0.1765,  0.4271, -0.0164, -0.3333, -0.5508, -0.1416, -0.4799,  0.3333],\n","        [-0.8824,  0.0050,  0.0820, -0.4141, -0.5366, -0.0462, -0.6874, -0.3000],\n","        [-0.4118,  0.1658,  0.2131,  0.0000,  0.0000, -0.2370, -0.8950, -0.7000],\n","        [-0.8824,  0.4774,  0.5410, -0.1717,  0.0000,  0.4694, -0.7609, -0.8000],\n","        [-0.2941,  0.0955, -0.0164, -0.4545,  0.0000, -0.2548, -0.8907, -0.8000],\n","        [-0.5294, -0.0955,  0.0000,  0.0000,  0.0000, -0.1654, -0.5457, -0.6667],\n","        [-0.8824,  0.1960, -0.1148, -0.7374, -0.8818, -0.3353, -0.8915, -0.9000],\n","        [ 0.1765,  0.1156,  0.1475, -0.4545,  0.0000, -0.1803, -0.9462, -0.3667],\n","        [-0.6471,  0.7688,  0.4098, -0.4545, -0.6312, -0.0075, -0.0811,  0.0333],\n","        [-0.7647,  0.0854,  0.0164, -0.7980, -0.3428, -0.2459, -0.3143, -0.9667],\n","        [-0.8824,  0.2161,  0.2787, -0.2121, -0.8251,  0.1624, -0.8437, -0.7667],\n","        [ 0.5294,  0.5276,  0.4754, -0.3333, -0.9314, -0.2012, -0.4424, -0.2667],\n","        [-0.8824,  0.0251,  0.2131,  0.0000,  0.0000,  0.1773, -0.8164, -0.3000],\n","        [ 0.0588, -0.2764,  0.2787, -0.4949,  0.0000, -0.0581, -0.8275, -0.4333],\n","        [-0.7647,  0.2864,  0.2787, -0.2525, -0.5697,  0.2906, -0.0213, -0.6667],\n","        [-0.7647, -0.0955, -0.0164,  0.0000,  0.0000, -0.2996, -0.9035, -0.8667],\n","        [-0.7647,  0.0050,  0.0820, -0.5960, -0.7872, -0.0194, -0.3262, -0.7667],\n","        [-0.6471, -0.1658, -0.0492, -0.3737, -0.9574,  0.0224, -0.7797, -0.8667],\n","        [-0.6471, -0.0955,  0.2787,  0.0000,  0.0000,  0.2727, -0.5892,  0.0000],\n","        [-0.5294,  0.2362,  0.3115, -0.6970, -0.5839, -0.0462, -0.6883, -0.5667],\n","        [-0.1765,  0.0251,  0.2131, -0.1919, -0.7518,  0.1088, -0.8924, -0.2000],\n","        [-0.4118,  0.0653,  0.3443, -0.3939,  0.0000,  0.1773, -0.8224, -0.4333],\n","        [-0.8824, -0.0754,  0.0164, -0.4949, -0.9031, -0.4188, -0.6550, -0.8667],\n","        [-0.6471,  0.5075,  0.2459,  0.0000,  0.0000, -0.3741, -0.8898, -0.4667],\n","        [ 0.0588,  0.0653, -0.1475,  0.0000,  0.0000, -0.0700, -0.7421, -0.3000],\n","        [-0.6471,  0.2864,  0.2787,  0.0000,  0.0000, -0.3711, -0.8377,  0.1333],\n","        [-0.4118,  0.6884,  0.0492,  0.0000,  0.0000, -0.0194, -0.9513, -0.3333],\n","        [-0.8824,  0.2060,  0.3115, -0.0303, -0.5272,  0.1595, -0.0743, -0.3333]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.]])\n","Epoch: 7 | Inputs tensor([[-0.7647,  0.0050,  0.0492, -0.5354,  0.0000, -0.1148, -0.7523,  0.0000],\n","        [ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],\n","        [ 0.0000, -0.1558,  0.3443, -0.3737, -0.7045,  0.1386, -0.8676, -0.9333],\n","        [ 0.0588,  0.2060,  0.1803, -0.5556, -0.8676, -0.3800, -0.4406, -0.1000],\n","        [-0.8824,  0.2563, -0.1803, -0.1919, -0.6052, -0.0075, -0.2451, -0.7667],\n","        [-0.5294, -0.0452,  0.0492,  0.0000,  0.0000, -0.0462, -0.9291, -0.6667],\n","        [-0.5294, -0.0854,  0.1475, -0.3535, -0.7920, -0.0134, -0.6857, -0.9667],\n","        [ 0.0000,  0.3568,  0.1148, -0.1515, -0.4090,  0.2608, -0.7549, -0.9000],\n","        [-0.1765,  0.0050,  0.0000,  0.0000,  0.0000, -0.1058, -0.6533, -0.6333],\n","        [-0.5294, -0.0955,  0.4426, -0.0505, -0.8723,  0.1237, -0.7575, -0.7333],\n","        [-0.5294,  0.4874, -0.0164, -0.4545, -0.2482, -0.0790, -0.9385, -0.7333],\n","        [-0.2941,  0.5477,  0.2787, -0.1717, -0.6690,  0.3741, -0.5790, -0.8000],\n","        [-0.8824, -0.1156,  0.0164, -0.5152, -0.8960, -0.1088, -0.7062, -0.9333],\n","        [-0.8824,  0.4975,  0.1148, -0.4141, -0.6998, -0.1267, -0.7686, -0.3000],\n","        [-0.6471,  0.0854,  0.0164, -0.5152,  0.0000, -0.2250, -0.8762, -0.8667],\n","        [-0.5294,  0.3266,  0.4098, -0.3737,  0.0000, -0.1654, -0.7088,  0.4000],\n","        [-0.5294,  0.1658,  0.1803, -0.7576, -0.7943, -0.3413, -0.6712, -0.4667],\n","        [-0.8824, -0.1759,  0.0492, -0.7374, -0.7754, -0.3681, -0.7122, -0.9333],\n","        [-0.8824,  0.1658,  0.1475, -0.4343,  0.0000, -0.1833, -0.8924,  0.0000],\n","        [-0.7647,  0.0854,  0.3115,  0.0000,  0.0000, -0.1952, -0.8454,  0.0333],\n","        [-0.4118, -0.1156,  0.2787, -0.3939,  0.0000, -0.1773, -0.8463, -0.4667],\n","        [-0.8824,  0.4472,  0.3443, -0.0707, -0.5745,  0.3741, -0.7805, -0.1667],\n","        [ 0.1765, -0.3166,  0.7377, -0.5354, -0.8842,  0.0581, -0.8232, -0.1333],\n","        [-0.6471,  0.0653, -0.1148, -0.5758, -0.6265, -0.0790, -0.8173, -0.9000],\n","        [-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],\n","        [-0.4118,  0.5879,  0.3770, -0.1717, -0.5035,  0.1744, -0.7293, -0.7333],\n","        [-0.6471,  0.0653,  0.1803,  0.0000,  0.0000, -0.2310, -0.8898, -0.8000],\n","        [ 0.0588,  0.5678,  0.4098, -0.4343, -0.6336,  0.0224, -0.0512, -0.3000],\n","        [-0.8824, -0.0452,  0.2131, -0.5758, -0.8274, -0.2280, -0.4919, -0.5000],\n","        [-0.8824,  0.0854,  0.4426, -0.6162,  0.0000, -0.1922, -0.7250, -0.9000],\n","        [-0.8824, -0.2060,  0.2295, -0.3939,  0.0000, -0.0462, -0.7284, -0.9667],\n","        [-0.4118,  0.2864,  0.3115,  0.0000,  0.0000,  0.0313, -0.9436, -0.2000]]) | Labels tensor([[1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 8 | Inputs tensor([[-0.2941, -0.0854,  0.0000,  0.0000,  0.0000, -0.1118, -0.6388, -0.6667],\n","        [ 0.0000,  0.1457,  0.3115, -0.3131, -0.3262,  0.3174, -0.9240, -0.8000],\n","        [-0.7647,  0.2965,  0.2131, -0.4747, -0.5154, -0.0104, -0.5619, -0.8667],\n","        [ 0.1765,  0.2965,  0.2459, -0.4343, -0.7116,  0.0700, -0.8275, -0.4000],\n","        [-0.7647,  0.5879,  0.4754,  0.0000,  0.0000, -0.0581, -0.3792,  0.5000],\n","        [-0.0588,  0.2663,  0.2131, -0.2323, -0.8227, -0.2280, -0.9283, -0.4000],\n","        [-0.4118, -0.2161, -0.2131,  0.0000,  0.0000,  0.0045, -0.5081, -0.8667],\n","        [-0.1765,  0.3367,  0.4426, -0.6970, -0.6336, -0.0343, -0.8429, -0.4667],\n","        [-0.2941,  0.0452,  0.2131, -0.6364, -0.6312, -0.1088, -0.4500, -0.3333],\n","        [-0.1765,  0.3367,  0.3770,  0.0000,  0.0000,  0.1982, -0.4722, -0.4667],\n","        [-0.8824,  0.2864,  0.4426, -0.2121, -0.7400,  0.0879, -0.1640, -0.4667],\n","        [ 0.0588,  0.5276,  0.2787, -0.3131, -0.5957,  0.0194, -0.3040, -0.6000],\n","        [-0.7647,  0.3065,  0.5738,  0.0000,  0.0000, -0.3264, -0.8377,  0.0000],\n","        [-0.1765,  0.8794,  0.1148, -0.2121, -0.2813,  0.1237, -0.8497, -0.3333],\n","        [-0.8824,  0.0553, -0.0492,  0.0000,  0.0000, -0.2757, -0.9069,  0.0000],\n","        [-0.6471,  0.0251, -0.2787, -0.5960, -0.7778, -0.0820, -0.7250, -0.8333],\n","        [-0.6471,  0.7487, -0.0492, -0.5556, -0.5414, -0.0194, -0.5602, -0.5000],\n","        [-0.7647,  0.1457,  0.1148, -0.5556,  0.0000, -0.1446, -0.9880, -0.8667],\n","        [-0.7647,  0.0553,  0.2295,  0.0000,  0.0000, -0.3055, -0.5884,  0.0667],\n","        [-0.6471,  0.0352,  0.1803, -0.3939, -0.6407, -0.1773, -0.4432, -0.8000],\n","        [-0.2941, -0.1960,  0.3115, -0.2727,  0.0000,  0.1863, -0.9155, -0.7667],\n","        [ 0.0588,  0.6583,  0.4426,  0.0000,  0.0000, -0.0939, -0.8087, -0.0667],\n","        [-0.5294,  0.5678,  0.2295,  0.0000,  0.0000,  0.4396, -0.8634, -0.6333],\n","        [-0.7647,  0.1256,  0.1148, -0.5556, -0.7778,  0.0164, -0.7976, -0.8333],\n","        [ 0.0000,  0.2965,  0.3115,  0.0000,  0.0000, -0.0700, -0.4663, -0.7333],\n","        [ 0.0000,  0.7990, -0.1803, -0.2727, -0.6241,  0.1267, -0.6781, -0.9667],\n","        [-0.8824, -0.1256, -0.0164, -0.2525, -0.8227,  0.1088, -0.6319, -0.9667],\n","        [-0.7647,  0.0151, -0.0492, -0.6566, -0.3735, -0.2787, -0.5423, -0.9333],\n","        [ 0.0000,  0.7789, -0.0164, -0.4141,  0.1300,  0.0313, -0.1512,  0.0000],\n","        [-0.7647, -0.1859,  0.1803, -0.6970, -0.8203, -0.1028, -0.5995, -0.8667],\n","        [-0.0588,  0.9698,  0.2459, -0.4141, -0.3381,  0.1177, -0.5500,  0.2000],\n","        [-0.4118,  0.8794,  0.2459, -0.4545, -0.5106,  0.2996, -0.1836,  0.0667]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.]])\n","Epoch: 9 | Inputs tensor([[-0.5294,  0.2362,  0.0164,  0.0000,  0.0000, -0.0462, -0.8736, -0.5333],\n","        [-0.6471,  0.2965,  0.0492, -0.4141, -0.7281, -0.2131, -0.8796, -0.7667],\n","        [-0.6471,  0.4874,  0.0820, -0.4949,  0.0000, -0.0313, -0.8480, -0.9667],\n","        [-0.8824,  0.0151, -0.1803, -0.6970, -0.9149, -0.2787, -0.6174, -0.8333],\n","        [-0.8824, -0.0050, -0.0492, -0.7980,  0.0000, -0.2429, -0.5961,  0.0000],\n","        [-0.8824,  0.2261,  0.4754,  0.0303, -0.4799,  0.4814, -0.7891, -0.6667],\n","        [-0.8824,  0.6382,  0.1803,  0.0000,  0.0000,  0.1624, -0.0231, -0.6000],\n","        [-0.8824,  0.0653,  0.2459,  0.0000,  0.0000,  0.1177, -0.8984, -0.8333],\n","        [ 0.0000,  0.3769,  0.3770, -0.4545,  0.0000, -0.1863, -0.8693,  0.2667],\n","        [-0.5294, -0.0352, -0.0820, -0.6566, -0.8842, -0.3800, -0.7763, -0.8333],\n","        [-0.8824,  0.1156,  0.4098, -0.6162,  0.0000, -0.1028, -0.9445, -0.9333],\n","        [ 0.2941,  0.3869,  0.2459,  0.0000,  0.0000, -0.0104, -0.7079, -0.5333],\n","        [-0.4118, -0.0251,  0.2459, -0.4545,  0.0000,  0.0611, -0.7438,  0.0333],\n","        [-0.8824, -0.2060, -0.0164, -0.1515, -0.8865,  0.2966, -0.4876, -0.9333],\n","        [ 0.0588,  0.2362,  0.1475, -0.1111, -0.7778, -0.0134, -0.7472, -0.3667],\n","        [ 0.0000,  0.3869,  0.0000,  0.0000,  0.0000,  0.0820, -0.2699, -0.8667],\n","        [-0.5294, -0.0050,  0.1148, -0.2323,  0.0000, -0.0224, -0.9428, -0.6000],\n","        [-0.6471, -0.1859,  0.4098, -0.6768, -0.8440, -0.1803, -0.8053, -0.9667],\n","        [-0.1765,  0.5980,  0.0492,  0.0000,  0.0000, -0.1833, -0.8155, -0.3667],\n","        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],\n","        [-0.8824,  0.3166,  0.0492, -0.7172, -0.0189, -0.2936, -0.7344,  0.0000],\n","        [ 0.0000, -0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8480, -0.8667],\n","        [-0.7647, -0.0653,  0.0492, -0.3535, -0.6217,  0.1326, -0.4910, -0.9333],\n","        [-0.0588,  0.7990,  0.1803, -0.1515, -0.6927, -0.0253, -0.4526, -0.5000],\n","        [-0.5294,  0.4472, -0.0492, -0.4343, -0.6690, -0.1207, -0.8215, -0.4667],\n","        [ 0.0000,  0.3166,  0.0000,  0.0000,  0.0000,  0.2876, -0.8360, -0.8333],\n","        [-0.2941, -0.0050, -0.0164, -0.6162, -0.8723, -0.1982, -0.6422, -0.6333],\n","        [-0.0588,  0.2663,  0.4426, -0.2727, -0.7447,  0.1475, -0.7686, -0.0667],\n","        [ 0.1765, -0.0553,  0.1803, -0.6364,  0.0000, -0.3115, -0.5585,  0.1667],\n","        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],\n","        [-0.7647, -0.1256, -0.0492, -0.6768, -0.8771, -0.0253, -0.9249, -0.8667],\n","        [-0.8824,  0.1759,  0.4426, -0.5152, -0.6572,  0.0283, -0.7225, -0.3667]]) | Labels tensor([[0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","Epoch: 10 | Inputs tensor([[-0.5294,  0.1759,  0.0164, -0.7576,  0.0000, -0.1148, -0.7421, -0.7000],\n","        [-0.5294,  0.8995,  0.8033, -0.3737,  0.0000, -0.1505, -0.4859, -0.4667],\n","        [-0.5294,  0.3266,  0.0000,  0.0000,  0.0000, -0.0194, -0.8087, -0.9333],\n","        [ 0.0000,  0.0553,  0.1148, -0.5556,  0.0000, -0.4039, -0.8651, -0.9667],\n","        [-0.0588,  0.5176,  0.2787, -0.3535, -0.5035,  0.2787, -0.6260, -0.5000],\n","        [-0.4118,  0.2663,  0.2787, -0.4545, -0.9480, -0.1177, -0.6917, -0.3667],\n","        [-0.5294,  0.3467,  0.1803,  0.0000,  0.0000, -0.2906, -0.8301,  0.3000],\n","        [ 0.0000,  0.1859,  0.0492, -0.5354, -0.7896,  0.0000,  0.4116,  0.0000],\n","        [-0.7647,  0.0050,  0.1475,  0.0505, -0.8652,  0.2072, -0.4885, -0.8667],\n","        [-0.8824,  0.0754, -0.1803, -0.6162,  0.0000, -0.1565, -0.9120, -0.7333],\n","        [-0.7647, -0.1256,  0.0000, -0.5354,  0.0000, -0.1386, -0.4065, -0.8667],\n","        [-0.8824, -0.1055,  0.2459, -0.3131, -0.9125, -0.0700, -0.9026, -0.9333],\n","        [-0.7647,  0.0653,  0.0492, -0.2929, -0.7187, -0.0909,  0.1289, -0.5667],\n","        [-0.1765,  0.5075,  0.0820, -0.1515, -0.1915,  0.0343, -0.4535, -0.3000],\n","        [-0.7647,  0.0955,  0.5082,  0.0000,  0.0000,  0.2727, -0.3450,  0.1000],\n","        [ 0.0000,  0.0251,  0.2295, -0.5354,  0.0000,  0.0000, -0.5781,  0.0000],\n","        [-0.2941,  0.0754,  0.4426,  0.0000,  0.0000,  0.0969, -0.4458, -0.6667],\n","        [-0.6471,  0.2864,  0.1803, -0.4949, -0.5508, -0.0343, -0.5978, -0.8000],\n","        [-0.6471,  0.1156, -0.0492, -0.3737, -0.8960, -0.1207, -0.6994, -0.9667],\n","        [-0.0588,  0.3367,  0.1803,  0.0000,  0.0000, -0.0194, -0.8360, -0.4000],\n","        [-0.2941,  0.0000,  0.1148, -0.1717,  0.0000,  0.1624, -0.4458, -0.3333],\n","        [-0.0588,  0.2060,  0.0000,  0.0000,  0.0000, -0.1058, -0.9103, -0.4333],\n","        [-0.5294, -0.1558,  0.4754, -0.5354, -0.8676,  0.1773, -0.9308, -0.8667],\n","        [-0.8824,  0.1960, -0.2787, -0.0505, -0.8511,  0.0581, -0.8275, -0.8667],\n","        [-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],\n","        [-0.1765,  0.3668,  0.4754,  0.0000,  0.0000, -0.1088, -0.8873, -0.0333],\n","        [-0.5294,  0.5176,  0.4754, -0.2323,  0.0000, -0.1148, -0.8155, -0.5000],\n","        [ 0.0000, -0.4271, -0.0164,  0.0000,  0.0000, -0.3532, -0.4389,  0.5333],\n","        [ 0.5294,  0.5377,  0.4426, -0.2525, -0.6690,  0.2101, -0.0640, -0.4000],\n","        [-0.2941, -0.0653, -0.1803, -0.3939, -0.8487, -0.1446, -0.7626, -0.9333],\n","        [ 0.0588,  0.3065,  0.1475,  0.0000,  0.0000,  0.0194, -0.5098, -0.2000],\n","        [-0.8824, -0.0251,  0.0820, -0.6970, -0.6690, -0.3085, -0.6507, -0.9667]]) | Labels tensor([[0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.]])\n","Epoch: 11 | Inputs tensor([[-0.1765,  0.7990,  0.5574, -0.3737,  0.0000,  0.0194, -0.9266,  0.3000],\n","        [-0.0588, -0.3467,  0.1803, -0.5354,  0.0000, -0.0462, -0.5542, -0.3000],\n","        [-0.1765,  0.8794, -0.1803, -0.3333, -0.0733,  0.0104, -0.3612, -0.5667],\n","        [ 0.4118,  0.0050,  0.3770, -0.3333, -0.7518, -0.1058, -0.6499, -0.1667],\n","        [-0.8824, -0.1357,  0.0820,  0.0505, -0.8463,  0.2310, -0.2835, -0.7333],\n","        [-0.2941,  0.5477,  0.2131, -0.3535, -0.5437, -0.1267, -0.3501, -0.4000],\n","        [ 0.0000,  0.0452,  0.0492, -0.5354, -0.7258, -0.1714, -0.6789, -0.9333],\n","        [-0.5294,  0.0955,  0.0492, -0.1111, -0.7660,  0.0373, -0.2938, -0.8333],\n","        [-0.7647,  0.7588,  0.4426,  0.0000,  0.0000, -0.3174, -0.7882, -0.9667],\n","        [-0.7647, -0.1658,  0.0820, -0.5354, -0.8818, -0.0402, -0.6422, -0.9667],\n","        [-0.5294,  0.7387,  0.1475, -0.7172, -0.6028, -0.1148, -0.7583, -0.6000],\n","        [-0.0588, -0.1558,  0.2131, -0.3737,  0.0000,  0.1416, -0.6763, -0.4000],\n","        [-0.6471,  0.2060,  0.1475, -0.3939, -0.6809,  0.2787, -0.6806, -0.7000],\n","        [-0.5294,  0.1055,  0.2459, -0.5960, -0.7636, -0.1535, -0.9658, -0.8000],\n","        [-0.1765,  0.0653,  0.5082, -0.6364,  0.0000, -0.3234, -0.8659, -0.1000],\n","        [-0.1765,  0.6181,  0.4098,  0.0000,  0.0000, -0.0939, -0.9257, -0.1333],\n","        [ 0.0000,  0.1357,  0.2459,  0.0000,  0.0000, -0.0075, -0.8292, -0.9333],\n","        [ 0.0000,  0.8191,  0.4426, -0.1111,  0.2057,  0.2906, -0.8770, -0.8333],\n","        [-0.7647,  0.3467,  0.1475,  0.0000,  0.0000, -0.1386, -0.6038, -0.9333],\n","        [-0.4118,  0.0000,  0.3115, -0.3535,  0.0000,  0.2221, -0.7711, -0.4667],\n","        [-0.6471, -0.1055,  0.2131, -0.6768, -0.7991, -0.0939, -0.5961, -0.4333],\n","        [-0.2941,  0.3467,  0.1475, -0.5354, -0.6927,  0.0551, -0.6038, -0.7333],\n","        [-0.8824,  0.2864,  0.6066, -0.1717, -0.8629, -0.0462,  0.0615, -0.6000],\n","        [-0.8824, -0.1156,  0.2787, -0.4141, -0.8203, -0.0462, -0.7549, -0.7333],\n","        [-0.6471,  0.1658,  0.2131, -0.6970, -0.7518, -0.2161, -0.9752, -0.9000],\n","        [-0.5294,  0.4271,  0.4098,  0.0000,  0.0000,  0.3115, -0.5158, -0.9667],\n","        [-0.6471, -0.3869,  0.3443, -0.4343,  0.0000,  0.0253, -0.8591, -0.1667],\n","        [ 0.0588, -0.4271,  0.3115, -0.2525,  0.0000, -0.0224, -0.9846, -0.3333],\n","        [-0.5294, -0.1457, -0.0492, -0.5556, -0.8842, -0.1714, -0.8053, -0.7667],\n","        [-0.0588,  0.9799,  0.2131,  0.0000,  0.0000, -0.2280, -0.0495, -0.4000],\n","        [ 0.0588, -0.0854,  0.1148,  0.0000,  0.0000, -0.2787, -0.8958,  0.2333],\n","        [ 0.0000,  0.0452,  0.2459,  0.0000,  0.0000, -0.4516, -0.5696, -0.8000]]) | Labels tensor([[1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.]])\n","Epoch: 12 | Inputs tensor([[ 0.0000,  0.2764,  0.3115, -0.2525, -0.5035,  0.0820, -0.3800, -0.9333],\n","        [-0.1765,  0.9698,  0.4754,  0.0000,  0.0000,  0.1863, -0.6815, -0.3333],\n","        [ 0.0000,  0.2663,  0.4098, -0.4545, -0.7163, -0.1833, -0.6268,  0.0000],\n","        [-0.6471,  0.1256,  0.2131, -0.3939,  0.0000, -0.0581, -0.8984, -0.8667],\n","        [-0.5294,  0.4673,  0.5082,  0.0000,  0.0000, -0.0700, -0.6063,  0.3333],\n","        [-0.6471, -0.1759,  0.1475,  0.0000,  0.0000, -0.3711, -0.7344, -0.8667],\n","        [-0.8824, -0.1859,  0.1803, -0.6364, -0.9054, -0.2072, -0.8249, -0.9000],\n","        [-0.7647, -0.0151, -0.0164, -0.6566, -0.7163,  0.0343, -0.8975, -0.9667],\n","        [-0.0588, -0.2563,  0.1475, -0.1919, -0.8842,  0.0522, -0.4646, -0.4000],\n","        [-0.1765, -0.1658,  0.2787, -0.4747, -0.8322, -0.1267, -0.4116, -0.5000],\n","        [-0.4118, -0.1457,  0.2131, -0.5556,  0.0000, -0.1356, -0.0213, -0.6333],\n","        [-0.2941,  0.2563,  0.1148, -0.3939, -0.7163, -0.1058, -0.6704, -0.6333],\n","        [-0.8824, -0.0352,  0.0492, -0.4545, -0.7943, -0.0104, -0.8198,  0.0000],\n","        [-0.7647, -0.1658,  0.0656, -0.4343, -0.8440,  0.0969, -0.5295, -0.9000],\n","        [-0.8824,  0.4472,  0.3443, -0.1919,  0.0000,  0.2310, -0.5482, -0.7667],\n","        [ 0.4118, -0.1558,  0.1803, -0.3737,  0.0000, -0.1148, -0.8130, -0.1667],\n","        [-0.1765,  0.5075,  0.2787, -0.4141, -0.7021,  0.0492, -0.4757,  0.1000],\n","        [ 0.0000,  0.2362,  0.1803,  0.0000,  0.0000,  0.0820, -0.8463,  0.0333],\n","        [-0.7647,  0.1558,  0.0492, -0.5556,  0.0000, -0.0820, -0.7071,  0.0000],\n","        [-0.8824, -0.0251,  0.1475, -0.6970,  0.0000, -0.4575, -0.9411,  0.0000],\n","        [-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000],\n","        [-0.7647, -0.0854,  0.0164,  0.0000,  0.0000, -0.1863, -0.6183, -0.9667],\n","        [-0.7647, -0.2864,  0.1475, -0.4545,  0.0000, -0.1654, -0.5662, -0.9667],\n","        [-0.7647, -0.1156,  0.2131, -0.6162, -0.8747, -0.1356, -0.8711, -0.9667],\n","        [-0.4118, -0.1156,  0.0820, -0.5758, -0.9456, -0.2727, -0.7746, -0.7000],\n","        [-0.7647,  0.2261, -0.1475, -0.1313, -0.6265,  0.0790, -0.3698, -0.7667],\n","        [-0.6471,  0.2362,  0.6393, -0.2929, -0.4326,  0.7079, -0.3151, -0.9667],\n","        [ 0.0000,  0.3467, -0.0492, -0.5960, -0.3121, -0.2131, -0.7660,  0.0000],\n","        [ 0.0000,  0.3869, -0.0164, -0.2929, -0.6052,  0.0313, -0.6106,  0.0000],\n","        [-0.8824, -0.2261, -0.0820, -0.3939, -0.8676, -0.0075,  0.0017, -0.9000],\n","        [ 0.0000, -0.0251,  0.0492, -0.2727, -0.7636,  0.0969, -0.5542, -0.8667],\n","        [ 0.6471,  0.0050,  0.2787, -0.4949, -0.5650,  0.0909, -0.7148, -0.1667]]) | Labels tensor([[1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","Epoch: 13 | Inputs tensor([[-0.7647,  0.7487,  0.4426, -0.2525, -0.7163,  0.3264, -0.5149, -0.9000],\n","        [-0.1765,  0.9497,  0.1148, -0.4343,  0.0000,  0.0700, -0.4304, -0.3333],\n","        [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333],\n","        [-0.6471,  0.2161, -0.1475,  0.0000,  0.0000,  0.0730, -0.9582, -0.8667],\n","        [-0.5294, -0.1658,  0.4098, -0.6162,  0.0000, -0.1267, -0.7959, -0.5667],\n","        [-0.6471, -0.1960,  0.3443, -0.3737, -0.8345,  0.0194,  0.0367, -0.8000],\n","        [-0.1765,  0.0754,  0.2131,  0.0000,  0.0000, -0.1177, -0.8497, -0.6667],\n","        [-0.5294, -0.2362,  0.0164,  0.0000,  0.0000,  0.0134, -0.7327, -0.8667],\n","        [-0.7647, -0.1156, -0.0492, -0.4747, -0.9622, -0.1535, -0.4125, -0.9667],\n","        [-0.7647, -0.0050, -0.1475, -0.6970, -0.7778, -0.2668, -0.5226,  0.0000],\n","        [ 0.0000,  0.1759,  0.3115, -0.3737, -0.8747,  0.3472, -0.9906, -0.9000],\n","        [-0.2941,  0.9497,  0.2787,  0.0000,  0.0000, -0.2996, -0.9564,  0.2667],\n","        [-0.4118,  0.2161,  0.1803, -0.5354, -0.7352, -0.2191, -0.8574, -0.7000],\n","        [-0.7647,  0.2261,  0.1475, -0.4545,  0.0000,  0.0969, -0.7763, -0.8000],\n","        [-0.7647, -0.1558, -0.1803, -0.5354, -0.8203, -0.0939, -0.2400,  0.0000],\n","        [ 0.0588,  0.7085,  0.2131, -0.3737,  0.0000,  0.3115, -0.7225, -0.2667],\n","        [ 0.0000,  0.0553,  0.3770,  0.0000,  0.0000, -0.1684, -0.4338,  0.3667],\n","        [-0.2941,  0.1558, -0.0164, -0.2121,  0.0000,  0.0045, -0.8574, -0.3667],\n","        [-0.4118,  0.3668,  0.3443,  0.0000,  0.0000,  0.0000, -0.5201,  0.6000],\n","        [ 0.0588,  0.1256,  0.3443, -0.3535, -0.5863,  0.0194, -0.8446, -0.5000],\n","        [ 0.0000, -0.0854,  0.1148, -0.3535, -0.5035,  0.1893, -0.7412, -0.8667],\n","        [ 0.0000,  0.2563,  0.5738,  0.0000,  0.0000, -0.3294, -0.8429,  0.0000],\n","        [ 0.0000,  0.6181, -0.1803,  0.0000,  0.0000, -0.3472, -0.8497,  0.4667],\n","        [-0.6471,  0.0050,  0.1148, -0.5354, -0.8085, -0.0581, -0.2562, -0.7667],\n","        [ 0.0000,  0.4070,  0.0656, -0.4747, -0.6927,  0.2697, -0.6985, -0.9000],\n","        [-0.7647,  0.1759,  0.4754, -0.6162, -0.8322, -0.2489, -0.7993,  0.0000],\n","        [-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667],\n","        [-0.7647, -0.0050,  0.1475, -0.6768, -0.8960, -0.3920, -0.8659, -0.8000],\n","        [-0.8824,  0.6482,  0.3443, -0.1313, -0.8416, -0.0224, -0.7754, -0.0333],\n","        [-0.8824, -0.2663, -0.1803, -0.7980,  0.0000, -0.3145, -0.8548,  0.0000],\n","        [-0.2941, -0.0754,  0.0164, -0.3535, -0.7021, -0.0462, -0.9940, -0.1667],\n","        [-0.0588,  0.0754,  0.3115,  0.0000,  0.0000, -0.2668, -0.3356, -0.5667]]) | Labels tensor([[0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 14 | Inputs tensor([[-0.5294,  0.1055,  0.0820,  0.0000,  0.0000, -0.0492, -0.6644, -0.7333],\n","        [-0.4118,  0.3668,  0.3770, -0.1717, -0.7920,  0.0432, -0.8224, -0.5333],\n","        [-0.1765,  0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8061, -0.9000],\n","        [-0.6471, -0.1156, -0.0492, -0.7778, -0.8723, -0.2608, -0.8386, -0.9667],\n","        [-0.1765,  0.1457,  0.0492,  0.0000,  0.0000, -0.1833, -0.4415, -0.5667],\n","        [-0.8824,  0.1960,  0.4098, -0.2121, -0.4799,  0.3592, -0.3766, -0.7333],\n","        [ 0.0000, -0.0050,  0.0000,  0.0000,  0.0000, -0.2548, -0.8506, -0.9667],\n","        [-0.7647, -0.0050, -0.0164, -0.6566, -0.6217,  0.0909, -0.6798,  0.0000],\n","        [ 0.1765,  0.6281,  0.3770,  0.0000,  0.0000, -0.1744, -0.9112,  0.1000],\n","        [-0.2941,  0.6281,  0.0164,  0.0000,  0.0000, -0.2757, -0.9146, -0.0333],\n","        [ 0.0000, -0.0854,  0.3115,  0.0000,  0.0000, -0.0343, -0.5534, -0.8000],\n","        [-0.7647,  0.4673,  0.1475, -0.2323, -0.1489, -0.1654, -0.7788, -0.7333],\n","        [-0.8824,  0.0955, -0.0820, -0.5758, -0.6809, -0.2489, -0.3553, -0.9333],\n","        [-0.8824,  0.1156,  0.5410,  0.0000,  0.0000, -0.0224, -0.8403, -0.2000],\n","        [ 0.2941, -0.1457,  0.2131,  0.0000,  0.0000, -0.1028, -0.8104, -0.5333],\n","        [ 0.0588,  0.2261, -0.0820,  0.0000,  0.0000, -0.0075, -0.1153, -0.6000],\n","        [-0.4118, -0.0452,  0.1803, -0.3333,  0.0000,  0.1237, -0.7506, -0.8000],\n","        [ 0.0000,  0.2563,  0.1148,  0.0000,  0.0000, -0.2638, -0.8907,  0.0000],\n","        [ 0.0000,  0.0553,  0.0492, -0.1717, -0.6643,  0.2370, -0.9189, -0.9667],\n","        [-0.6471,  0.4271,  0.3115, -0.6970,  0.0000, -0.0343, -0.8958,  0.4000],\n","        [-0.8824,  0.8995, -0.0164, -0.5354,  1.0000, -0.1028, -0.7267,  0.2667],\n","        [-0.8824, -0.0251,  0.0492, -0.6162, -0.8061, -0.4575, -0.8113,  0.0000],\n","        [ 0.5294,  0.2965,  0.0000, -0.3939,  0.0000,  0.1893, -0.5807, -0.2333],\n","        [-0.7647,  0.4171, -0.0492, -0.3131, -0.6974, -0.2429, -0.4697, -0.9000],\n","        [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],\n","        [-0.2941, -0.0352,  0.0000,  0.0000,  0.0000, -0.2936, -0.9044, -0.7667],\n","        [ 0.0000,  0.0754,  0.2459,  0.0000,  0.0000,  0.3502, -0.4808, -0.9000],\n","        [ 0.1765,  0.7990,  0.1475,  0.0000,  0.0000,  0.0462, -0.8958, -0.4667],\n","        [-0.1765,  0.0352,  0.0820, -0.3535,  0.0000,  0.1654, -0.7728, -0.6667],\n","        [-0.8824,  0.8191,  0.2787, -0.1515, -0.3073,  0.1922,  0.0077, -0.9667],\n","        [ 0.0000,  0.0553,  0.4754,  0.0000,  0.0000, -0.1177, -0.8984, -0.1667],\n","        [ 0.0588,  0.0251,  0.2459, -0.2525,  0.0000, -0.0194, -0.4987, -0.1667]]) | Labels tensor([[1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.]])\n","Epoch: 15 | Inputs tensor([[-0.2941,  0.2563,  0.2787, -0.3737,  0.0000, -0.1773, -0.5841, -0.0667],\n","        [-0.4118,  0.0955,  0.2295, -0.4747,  0.0000,  0.0730, -0.6003,  0.3000],\n","        [ 0.0000,  0.1357,  0.3115, -0.6768,  0.0000, -0.0760, -0.3202,  0.0000],\n","        [-0.7647, -0.0955,  0.1148, -0.1515,  0.0000,  0.1386, -0.6371, -0.8000],\n","        [-0.6471,  0.1658,  0.0000,  0.0000,  0.0000, -0.2996, -0.9069, -0.9333],\n","        [ 0.0000,  0.1960,  0.0000,  0.0000,  0.0000, -0.0343, -0.9462, -0.9000],\n","        [ 0.0000,  0.4774,  0.3934,  0.0909,  0.0000,  0.2757, -0.7464, -0.9000],\n","        [-0.5294,  0.4171,  0.2131,  0.0000,  0.0000, -0.1773, -0.8582, -0.3667],\n","        [ 0.0000,  0.0653,  0.1475, -0.2525, -0.6501,  0.1744, -0.5500, -0.9667],\n","        [-0.8824,  0.8090,  0.0000,  0.0000,  0.0000,  0.2906, -0.8258, -0.3333],\n","        [ 0.0000, -0.2563, -0.1475, -0.7980, -0.9149, -0.1714, -0.8369, -0.9667],\n","        [-0.1765,  0.3769,  0.4754, -0.1717,  0.0000, -0.0462, -0.7327, -0.4000],\n","        [-0.5294,  0.9799,  0.1475, -0.2121,  0.7589,  0.0939,  0.9223, -0.6667],\n","        [-0.7647,  0.2362, -0.2131, -0.3535, -0.6099,  0.2548, -0.6225, -0.8333],\n","        [ 0.4118,  0.4070,  0.3934, -0.3333,  0.0000,  0.1148, -0.8582, -0.3333],\n","        [ 0.0000,  0.0754, -0.0164, -0.4949,  0.0000, -0.2131, -0.9530, -0.9333],\n","        [-0.8824,  0.9397, -0.1803, -0.6768, -0.1135, -0.2280, -0.5073, -0.9000],\n","        [-0.5294,  0.1457,  0.0656,  0.0000,  0.0000, -0.3472, -0.6977, -0.4667],\n","        [ 0.0588,  0.3467,  0.2131, -0.3333, -0.8582, -0.2280, -0.6738,  1.0000],\n","        [-0.5294,  0.4673,  0.2787,  0.0000,  0.0000,  0.1475, -0.6225,  0.5333],\n","        [-0.5294,  0.5477,  0.0164, -0.3737, -0.3286, -0.0224, -0.8642, -0.9333],\n","        [-0.4118,  0.4774,  0.2295,  0.0000,  0.0000, -0.1088, -0.6960, -0.7667],\n","        [-0.8824,  0.2462,  0.2131, -0.2727,  0.0000, -0.1714, -0.9812, -0.7000],\n","        [-0.7647,  0.0653, -0.0820, -0.4545, -0.6099, -0.1356, -0.7028, -0.9667],\n","        [ 0.4118, -0.1156,  0.2131, -0.1919, -0.8723,  0.0522, -0.7438, -0.1000],\n","        [-0.8824, -0.1859,  0.2131, -0.1717, -0.8652,  0.3800, -0.1307, -0.6333],\n","        [-0.7647,  0.0754,  0.2131, -0.3939, -0.7636,  0.0015, -0.7216, -0.9333],\n","        [-0.7647, -0.0352,  0.1148, -0.7374, -0.8842, -0.3711, -0.5141, -0.8333],\n","        [-0.7647, -0.3166,  0.0164, -0.7374, -0.9645, -0.4009, -0.8471, -0.9333],\n","        [-0.7647,  0.2764, -0.0492, -0.5152, -0.3499, -0.1744,  0.2997, -0.8667],\n","        [-0.7647,  0.2764, -0.2459, -0.5758, -0.2080,  0.0253, -0.9163, -0.9667],\n","        [-0.1765, -0.0553,  0.0492, -0.4949, -0.8132, -0.0075, -0.4364, -0.3333]]) | Labels tensor([[0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 16 | Inputs tensor([[ 0.0000,  0.3568,  0.5410, -0.0707, -0.6572,  0.2101, -0.8241, -0.8333],\n","        [-0.6471,  0.2965,  0.5082, -0.0101, -0.6336,  0.0849, -0.2400, -0.6333],\n","        [-0.1765,  0.1457,  0.0820,  0.0000,  0.0000, -0.0224, -0.8463, -0.3000],\n","        [-0.8824,  0.0000,  0.2131, -0.5960, -0.9456, -0.1744, -0.8113,  0.0000],\n","        [-0.6471, -0.1558,  0.1803, -0.3535,  0.0000,  0.1088, -0.8386, -0.7667],\n","        [-0.0588,  0.2462,  0.2459, -0.5152,  0.4184, -0.1446, -0.4799,  0.0333],\n","        [-0.4118,  0.1759,  0.4098, -0.3939, -0.7518,  0.1654, -0.8523, -0.3000],\n","        [ 0.1765, -0.0955,  0.3934, -0.3535,  0.0000,  0.0402, -0.3621,  0.1667],\n","        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0522, -0.9522, -0.7333],\n","        [ 0.5294,  0.5879,  0.8689,  0.0000,  0.0000,  0.2608, -0.8471, -0.2333],\n","        [-0.7647,  0.9799,  0.1475, -0.0909,  0.2837, -0.0909, -0.9317,  0.0667],\n","        [-0.7647,  0.0854, -0.1475, -0.4747, -0.8511, -0.0313, -0.7950, -0.9667],\n","        [-0.8824,  0.3970,  0.0164, -0.1717,  0.1348,  0.2131, -0.6089,  0.0000],\n","        [-0.8824, -0.0251,  0.1475, -0.1919,  0.0000,  0.1356, -0.8804, -0.7000],\n","        [-0.2941,  0.1457,  0.4426,  0.0000,  0.0000, -0.1714, -0.8557,  0.5000],\n","        [-0.7647,  0.2965,  0.0000,  0.0000,  0.0000,  0.1475, -0.8070, -0.3333],\n","        [-0.2941,  0.9095,  0.5082,  0.0000,  0.0000,  0.0581, -0.8292,  0.5000],\n","        [-0.8824,  0.2864,  0.3443, -0.6566, -0.5674, -0.1803, -0.9684, -0.9667],\n","        [-0.1765,  0.1960,  0.0000,  0.0000,  0.0000, -0.2489, -0.8881, -0.4667],\n","        [-0.6471,  0.3970, -0.1148,  0.0000,  0.0000, -0.2370, -0.7233, -0.9667],\n","        [ 0.0000,  0.3166,  0.0820, -0.1919,  0.0000,  0.0224, -0.8992, -0.9667],\n","        [-0.0588,  0.9497,  0.3115,  0.0000,  0.0000, -0.2221, -0.5961,  0.5333],\n","        [-0.4118,  0.1457,  0.2131,  0.0000,  0.0000, -0.2578, -0.4313,  0.2000],\n","        [-0.4118,  0.2462,  0.2131,  0.0000,  0.0000,  0.0134, -0.8787, -0.4333],\n","        [-0.6471, -0.0050,  0.0164, -0.6162, -0.8251, -0.3502, -0.8284, -0.8333],\n","        [-0.0588,  0.2060,  0.4098,  0.0000,  0.0000, -0.1535, -0.8454, -0.9667],\n","        [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],\n","        [-0.8824, -0.0251,  0.1148, -0.5758,  0.0000, -0.1893, -0.1315, -0.9667],\n","        [ 0.5294,  0.2663,  0.4754,  0.0000,  0.0000,  0.2936, -0.5687, -0.3000],\n","        [-0.8824,  0.5779,  0.1803, -0.5758, -0.6028, -0.2370, -0.9616, -0.9000],\n","        [-0.8824, -0.1256,  0.1148, -0.3131, -0.8180,  0.1207, -0.7242, -0.9000],\n","        [ 0.0000,  0.8995,  0.7049, -0.4949,  0.0000,  0.0224, -0.6951, -0.3333]]) | Labels tensor([[1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","Epoch: 17 | Inputs tensor([[-0.1765,  0.2965,  0.1148, -0.0101, -0.7045,  0.1475, -0.6917, -0.2667],\n","        [-0.4118,  0.0553,  0.1803, -0.4141, -0.2317,  0.0999, -0.9308, -0.7667],\n","        [-0.8824,  1.0000,  0.2459, -0.1313,  0.0000,  0.2787,  0.1238, -0.9667],\n","        [-0.8824,  0.0754,  0.1803, -0.3939, -0.8061, -0.0820, -0.3655, -0.9000],\n","        [ 0.0000,  0.3769,  0.1475, -0.2323,  0.0000, -0.0104, -0.9214, -0.9667],\n","        [-0.8824,  0.4372,  0.4098, -0.3939, -0.2199, -0.1028, -0.3049, -0.9333],\n","        [-0.4118, -0.0050, -0.1148, -0.4343, -0.8038,  0.0134, -0.6405, -0.7000],\n","        [ 0.2941,  0.3568,  0.0000,  0.0000,  0.0000,  0.5589, -0.5730, -0.3667],\n","        [-0.8824,  0.1960,  0.4426, -0.1717, -0.5981,  0.3502, -0.6336, -0.8333],\n","        [-0.8824, -0.1558,  0.0492, -0.5354, -0.7281,  0.0999, -0.6644, -0.7667],\n","        [-0.8824,  0.4372,  0.3770, -0.5354, -0.2671,  0.2638, -0.1477, -0.9667],\n","        [-0.0588,  0.8191,  0.1148, -0.2727,  0.1702, -0.1028, -0.5414,  0.3000],\n","        [-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],\n","        [ 0.0000,  0.2864,  0.1148, -0.6162, -0.5745, -0.0909,  0.1213, -0.8667],\n","        [ 0.0588,  0.6482,  0.2787,  0.0000,  0.0000, -0.0224, -0.9402, -0.2000],\n","        [-0.6471,  0.6281, -0.1475, -0.2323,  0.0000,  0.1088, -0.5098, -0.9000],\n","        [ 0.6471,  0.7588,  0.0164, -0.3939,  0.0000,  0.0015, -0.8856, -0.4333],\n","        [-0.6471,  0.5879,  0.0492, -0.7374, -0.0851, -0.0700, -0.8147, -0.9000],\n","        [ 1.0000,  0.6382,  0.1803, -0.1717, -0.7305,  0.2191, -0.3689, -0.1333],\n","        [-0.0588, -0.0854,  0.3443,  0.0000,  0.0000,  0.0611, -0.5653,  0.5667],\n","        [-0.6471,  0.5879,  0.1475, -0.3939, -0.2246,  0.0581, -0.7728, -0.5333],\n","        [-0.7647,  0.1256,  0.4098, -0.1515, -0.6217,  0.1446, -0.8565, -0.7667],\n","        [-0.8824,  0.3065, -0.0164, -0.5354, -0.5981, -0.1475, -0.4757,  0.0000],\n","        [ 0.4118,  0.4070,  0.3443, -0.1313, -0.2317,  0.1684, -0.6157,  0.2333],\n","        [ 0.0000,  0.1759,  0.0000,  0.0000,  0.0000,  0.0075, -0.2707, -0.2333],\n","        [-0.8824,  0.1759, -0.0164, -0.5354, -0.7494,  0.0075, -0.6687, -0.8000],\n","        [ 0.1765,  0.2563,  0.1475, -0.4747, -0.7281, -0.0730, -0.8915, -0.3333],\n","        [-0.8824,  0.0050,  0.2131, -0.7576, -0.8913, -0.4188, -0.9394, -0.7667],\n","        [ 0.0000,  0.5276,  0.3443, -0.2121, -0.3570,  0.2370, -0.8360, -0.8000],\n","        [-0.5294,  0.3769,  0.3770,  0.0000,  0.0000, -0.0700, -0.8514, -0.7000],\n","        [-0.0588,  0.8693,  0.4754, -0.2929, -0.4681,  0.0283, -0.7054, -0.4667],\n","        [ 0.0000,  0.0050,  0.4426,  0.2121, -0.7400,  0.3949, -0.2451, -0.6667]]) | Labels tensor([[0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.]])\n","Epoch: 18 | Inputs tensor([[ 0.0588,  0.8492,  0.3934, -0.6970,  0.0000, -0.1058, -0.0307, -0.0667],\n","        [-0.7647,  0.2563, -0.0164, -0.5960, -0.6690,  0.0075, -0.9915, -0.6667],\n","        [ 0.1765,  0.3367,  0.1148,  0.0000,  0.0000, -0.1952, -0.8574, -0.5000],\n","        [-0.6471, -0.2563,  0.1148, -0.4343, -0.8936, -0.1148, -0.8164, -0.9333],\n","        [-0.7647,  0.2060,  0.2459, -0.2525, -0.7518,  0.1833, -0.8830, -0.7333],\n","        [-0.8824,  0.3568, -0.1148,  0.0000,  0.0000, -0.2042, -0.4799,  0.3667],\n","        [-0.8824, -0.2864,  0.0164,  0.0000,  0.0000, -0.3502, -0.7114, -0.8333],\n","        [-0.2941,  0.0854, -0.2787, -0.5960, -0.6927, -0.2846, -0.3723, -0.5333],\n","        [-0.4118,  0.1256,  0.0820,  0.0000,  0.0000,  0.1267, -0.8437, -0.3333],\n","        [ 0.0588,  0.4573,  0.4426, -0.3131, -0.6099, -0.0969, -0.4082,  0.0667],\n","        [-0.8824, -0.0653, -0.0820, -0.7778,  0.0000, -0.3294, -0.7105, -0.9667],\n","        [-0.8824,  0.1859, -0.0492, -0.2727, -0.7778, -0.0075, -0.8437, -0.9333],\n","        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n","        [-0.1765,  0.4271,  0.4754, -0.5152,  0.1348, -0.0939, -0.9573, -0.2667],\n","        [-0.2941, -0.1960,  0.0820, -0.3939,  0.0000, -0.2191, -0.7993, -0.3333],\n","        [ 0.0000, -0.0653,  0.6393, -0.2121, -0.8298,  0.2936, -0.1947, -0.5333],\n","        [ 0.0000,  0.3970,  0.0164, -0.6566, -0.5035, -0.3413, -0.8898,  0.0000],\n","        [-0.8824,  0.9698,  0.2459, -0.2727, -0.4113,  0.0879, -0.3194, -0.7333],\n","        [-0.8824, -0.0452,  0.0820, -0.7374, -0.9102, -0.4158, -0.7814, -0.8667],\n","        [ 0.0588,  0.6482,  0.3770, -0.5758,  0.0000, -0.0820, -0.3570, -0.6333],\n","        [-0.5294,  0.2563,  0.3115,  0.0000,  0.0000, -0.0373, -0.6089, -0.8000],\n","        [-0.2941,  0.2563,  0.2459,  0.0000,  0.0000,  0.0075, -0.9633,  0.1000],\n","        [ 0.1765,  0.0854,  0.0820,  0.0000,  0.0000, -0.0343, -0.8343, -0.3000],\n","        [-0.7647,  0.0151, -0.0492, -0.2929, -0.7872, -0.3502, -0.9342, -0.9667],\n","        [-0.2941,  0.0553,  0.1475, -0.3535, -0.8392, -0.0820, -0.9624, -0.4667],\n","        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667],\n","        [-0.8824,  0.0955, -0.0492, -0.6364, -0.7258, -0.1505, -0.8796, -0.9667],\n","        [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],\n","        [-0.6471,  0.2261,  0.2787,  0.0000,  0.0000, -0.3145, -0.8497, -0.3667],\n","        [-0.8824,  0.8191,  0.0492, -0.3939, -0.5745,  0.0164, -0.7865, -0.4333],\n","        [ 0.1765,  0.0151,  0.4098, -0.2525,  0.0000,  0.3592, -0.0965, -0.4333],\n","        [-0.6471,  0.2663,  0.4426, -0.1717, -0.4444,  0.1714, -0.4654, -0.8000]]) | Labels tensor([[0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.]])\n","Epoch: 19 | Inputs tensor([[ 0.0000,  0.5176,  0.4754, -0.0707,  0.0000,  0.2548, -0.7498,  0.0000],\n","        [-0.2941, -0.1457,  0.2787,  0.0000,  0.0000, -0.0700, -0.7404, -0.3000],\n","        [-0.7647, -0.0955,  0.3115, -0.7172, -0.8700, -0.2727, -0.8540, -0.9000],\n","        [ 0.2941,  0.3668,  0.3770, -0.2929, -0.6927, -0.1565, -0.8446, -0.3000],\n","        [-0.8824,  0.2663, -0.0820, -0.4141, -0.6407, -0.1446, -0.3826,  0.0000],\n","        [ 0.0000,  0.0251,  0.0492, -0.0707, -0.8156,  0.2101, -0.6430,  0.0000],\n","        [-0.5294,  0.1759,  0.0492, -0.4545, -0.7163, -0.0104, -0.8702, -0.9000],\n","        [ 0.0000, -0.0151,  0.3443, -0.6970, -0.8014, -0.2489, -0.8113, -0.9667],\n","        [-0.4118,  0.5879,  0.1475,  0.0000,  0.0000, -0.1118, -0.8898,  0.4000],\n","        [ 0.0000,  0.0251,  0.2787, -0.1919, -0.7872,  0.0283, -0.8634, -0.9000],\n","        [-0.7647,  0.1055,  0.2131, -0.4141, -0.7045, -0.0343, -0.4705, -0.8000],\n","        [-0.2941,  0.0352,  0.1803, -0.3535, -0.5508,  0.1237, -0.7899,  0.1333],\n","        [ 0.0000, -0.2663,  0.0000,  0.0000,  0.0000, -0.3711, -0.7746, -0.8667],\n","        [-0.4118,  0.3266,  0.3115,  0.0000,  0.0000, -0.2012, -0.9078,  0.6000],\n","        [-0.4118, -0.1357,  0.1148, -0.4343, -0.8322, -0.0999, -0.7558, -0.9000],\n","        [-0.7647,  0.1960,  0.0000,  0.0000,  0.0000, -0.4158, -0.3561,  0.7000],\n","        [ 0.0588,  0.4573,  0.3115, -0.0707, -0.6927,  0.1297, -0.5226, -0.3667],\n","        [-0.6471,  0.6985,  0.2131, -0.6162, -0.7045, -0.1088, -0.8377, -0.6667],\n","        [-0.5294, -0.0050,  0.1803, -0.6566,  0.0000, -0.2370, -0.8155, -0.7667],\n","        [ 0.0000,  0.2161,  0.0820, -0.3939, -0.6099,  0.0224, -0.8933, -0.6000],\n","        [ 0.0000,  0.6784,  0.0000,  0.0000,  0.0000, -0.0373, -0.3501, -0.7000],\n","        [-0.7647,  0.2462,  0.1148, -0.4343, -0.5154, -0.0194, -0.3194, -0.7000],\n","        [-0.8824,  0.2663, -0.0164,  0.0000,  0.0000, -0.1028, -0.7686, -0.1333],\n","        [-0.8824, -0.1960,  0.2131, -0.7778, -0.8582, -0.1058, -0.6166, -0.9667],\n","        [ 0.4118,  0.0653,  0.3115,  0.0000,  0.0000, -0.2966, -0.9496, -0.2333],\n","        [-0.5294,  0.8492,  0.2787, -0.2121, -0.3452,  0.1028, -0.8412, -0.6667],\n","        [-0.8824, -0.1960, -0.0984,  0.0000,  0.0000, -0.4307, -0.8463,  0.0000],\n","        [-0.1765,  0.5980,  0.0820,  0.0000,  0.0000, -0.0939, -0.7395, -0.5000],\n","        [-0.4118,  0.3065,  0.3443,  0.0000,  0.0000,  0.1654, -0.2502, -0.4667],\n","        [-0.8824, -0.0352,  1.0000,  0.0000,  0.0000, -0.3323, -0.8898, -0.8000],\n","        [-0.1765,  0.6080, -0.1148, -0.3535, -0.5863, -0.0909, -0.5645, -0.4000],\n","        [-0.6471,  0.6382,  0.1475, -0.6364, -0.7518, -0.0581, -0.8377, -0.7667]]) | Labels tensor([[0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.]])\n","Epoch: 20 | Inputs tensor([[-0.1765,  0.7889,  0.3770,  0.0000,  0.0000,  0.1893, -0.7839, -0.3333],\n","        [ 0.0000,  0.2965,  0.8033, -0.0707, -0.6927,  1.0000, -0.7942, -0.8333],\n","        [-0.4118,  0.1759,  0.5082,  0.0000,  0.0000,  0.0164, -0.7788, -0.4333],\n","        [-0.5294,  0.2060,  0.1148,  0.0000,  0.0000, -0.1177, -0.4611, -0.5667],\n","        [-0.7647, -0.0553,  0.1148, -0.6364, -0.8203, -0.2250, -0.5875,  0.0000],\n","        [-0.8824,  0.4673, -0.0820,  0.0000,  0.0000, -0.1148, -0.5850, -0.7333],\n","        [-0.7647, -0.0050,  0.0000,  0.0000,  0.0000, -0.3383, -0.9744, -0.9333],\n","        [-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n","        [-0.5294,  0.2261,  0.1148,  0.0000,  0.0000,  0.0432, -0.7301, -0.7333],\n","        [ 0.0000,  0.8894,  0.3443, -0.7172, -0.5626, -0.0462, -0.4842, -0.9667],\n","        [-0.5294,  0.4673,  0.3934, -0.4545, -0.7636, -0.1386, -0.9052, -0.8000],\n","        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],\n","        [-0.2941, -0.0151, -0.0492, -0.3333, -0.5508,  0.0134, -0.6994, -0.2667],\n","        [ 0.2941,  0.1156,  0.3770, -0.1919,  0.0000,  0.3949, -0.2767, -0.2000],\n","        [-0.6471,  0.9397,  0.1475, -0.3737,  0.0000,  0.0402, -0.8608, -0.8667],\n","        [-0.6471,  0.3065,  0.2787, -0.5354, -0.8132, -0.1535, -0.7908, -0.5667],\n","        [-0.0588,  0.1859,  0.1803, -0.6162,  0.0000, -0.3115,  0.1939, -0.1667],\n","        [ 0.0000,  0.1156,  0.0656,  0.0000,  0.0000, -0.2668, -0.5030, -0.6667],\n","        [-0.6471,  0.7387,  0.3443, -0.0303,  0.0993,  0.1446,  0.7583, -0.8667],\n","        [-0.6471,  0.8794,  0.1475, -0.5556, -0.5272,  0.0849, -0.7182, -0.5000],\n","        [-0.7647, -0.0452, -0.1148, -0.7172, -0.7920, -0.2221, -0.4278, -0.9667],\n","        [ 0.0000,  0.3769,  0.1148, -0.7172, -0.6501, -0.2608, -0.9445,  0.0000],\n","        [-0.4118,  0.3970,  0.3115, -0.2929, -0.6217, -0.0581, -0.7583, -0.8667],\n","        [-0.7647, -0.0955,  0.1475, -0.6566,  0.0000, -0.1863, -0.9940, -0.9667],\n","        [-0.8824,  0.2462, -0.0164, -0.3535,  0.0000,  0.0671, -0.6277,  0.0000],\n","        [-0.0588,  0.6784,  0.7377, -0.0707, -0.4539,  0.1207, -0.9257, -0.2667],\n","        [-0.2941,  0.1156,  0.0492, -0.2121,  0.0000,  0.0194, -0.8446, -0.9000],\n","        [-0.8824,  0.5377,  0.3443, -0.1515,  0.1466,  0.2101, -0.4799, -0.9333],\n","        [ 0.0000,  0.8090,  0.4754, -0.4747, -0.7872,  0.0879, -0.7985, -0.5333],\n","        [ 0.0588,  0.1256,  0.3443, -0.5152,  0.0000, -0.1595,  0.0282, -0.0333],\n","        [-0.2941,  0.0251,  0.4754, -0.2121,  0.0000,  0.0641, -0.4910, -0.7667],\n","        [ 0.0000,  0.3166,  0.4426,  0.0000,  0.0000, -0.0581, -0.4321, -0.6333]]) | Labels tensor([[0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.]])\n","Epoch: 21 | Inputs tensor([[ 0.0000,  0.2663,  0.3770, -0.4141, -0.4917, -0.0849, -0.6225, -0.9000],\n","        [-0.8824,  0.0955, -0.3770, -0.6364, -0.7163, -0.3115, -0.7190, -0.8333],\n","        [-0.7647,  0.2261, -0.0164, -0.6364, -0.7494, -0.1118, -0.4543, -0.9667],\n","        [-0.7647,  0.5578,  0.2131, -0.6566, -0.7731, -0.2072, -0.6968, -0.8000],\n","        [-0.1765,  0.6884,  0.4426, -0.1515, -0.2411,  0.1386, -0.3945, -0.3667],\n","        [-0.8824,  0.0352,  0.3115, -0.7778, -0.8061, -0.4218, -0.6473, -0.9667],\n","        [-0.6471, -0.0050, -0.1148, -0.6162, -0.7967, -0.2370, -0.9351, -0.9000],\n","        [-0.8824, -0.1256,  0.2787, -0.4545, -0.9244,  0.0313, -0.9804, -0.9667],\n","        [-0.8824,  0.1156,  0.0164, -0.7374, -0.5697, -0.2846, -0.9488, -0.9333],\n","        [ 0.4118, -0.0754,  0.0164, -0.8586, -0.3901, -0.1773, -0.2758, -0.2333],\n","        [ 0.0000, -0.3266,  0.2459,  0.0000,  0.0000,  0.3502, -0.9009, -0.1667],\n","        [-0.8824, -0.0854, -0.1148, -0.4949, -0.7636, -0.2489, -0.8668, -0.9333],\n","        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8437, -0.7000],\n","        [-0.5294, -0.0754,  0.3115,  0.0000,  0.0000,  0.2578, -0.8642, -0.7333],\n","        [ 0.0000,  0.4171,  0.3770, -0.4747,  0.0000, -0.0343, -0.6968, -0.9667],\n","        [ 0.0000,  0.2362,  0.4426, -0.2525,  0.0000,  0.0492, -0.8984, -0.7333],\n","        [-0.8824,  0.1457,  0.0820, -0.2727, -0.5272,  0.1356, -0.8198,  0.0000],\n","        [-0.8824,  0.1256,  0.1803, -0.3939, -0.5839,  0.0253, -0.6157, -0.8667],\n","        [-0.8824,  0.0352, -0.5082, -0.2323, -0.8038,  0.2906, -0.9103, -0.6000],\n","        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667],\n","        [-0.7647, -0.2563,  0.0000,  0.0000,  0.0000,  0.0000, -0.9795, -0.9667],\n","        [-0.6471,  0.1558,  0.0820, -0.2121, -0.6690,  0.1356, -0.9385, -0.7667],\n","        [-0.0588,  0.0050,  0.2131, -0.1919, -0.4917,  0.1744, -0.5021, -0.2667],\n","        [-0.6471,  0.8090,  0.0492, -0.4949, -0.8345,  0.0134, -0.8352, -0.8333],\n","        [ 0.0000,  0.0151,  0.0492, -0.6566,  0.0000, -0.3741, -0.8514,  0.0000],\n","        [-0.7647,  0.4271,  0.3443, -0.6364, -0.8487, -0.2638, -0.4167,  0.0000],\n","        [-0.2941,  0.1457,  0.0000,  0.0000,  0.0000,  0.0000, -0.9052, -0.8333],\n","        [-0.2941,  0.1960, -0.1803, -0.5556, -0.5839, -0.1922,  0.0589, -0.6000],\n","        [ 0.1765,  0.1558,  0.6066,  0.0000,  0.0000, -0.2846, -0.1939, -0.5667],\n","        [ 0.2941,  0.3869,  0.2131, -0.4747, -0.6596,  0.0760, -0.5909, -0.0333],\n","        [ 0.0000,  0.4573,  0.0000,  0.0000,  0.0000,  0.3174, -0.5286, -0.6667],\n","        [ 0.0588,  0.4070,  0.5410,  0.0000,  0.0000, -0.0253, -0.4398, -0.2000]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.]])\n","Epoch: 22 | Inputs tensor([[-0.0588,  0.2563,  0.5738,  0.0000,  0.0000,  0.0000, -0.8685,  0.1000],\n","        [-0.4118,  0.1055,  0.1148,  0.0000,  0.0000, -0.2250, -0.8173, -0.7000],\n","        [-0.8824,  0.3869,  0.3443,  0.0000,  0.0000,  0.1952, -0.8651, -0.7667],\n","        [ 0.5294,  0.0452,  0.1803,  0.0000,  0.0000, -0.0700, -0.6695, -0.4333],\n","        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n","        [-0.5294,  0.4774,  0.2131, -0.4949, -0.3073,  0.0402, -0.7378, -0.7000],\n","        [ 0.0000,  0.0251, -0.1475,  0.0000,  0.0000, -0.2519,  0.0000,  0.0000],\n","        [-0.2941,  0.6583,  0.1148, -0.4747, -0.6028,  0.0015, -0.5278, -0.0667],\n","        [-0.8824, -0.0955,  0.0164, -0.7576, -0.8983, -0.1893, -0.5713, -0.9000],\n","        [ 0.0000,  0.0151,  0.2459,  0.0000,  0.0000,  0.0641, -0.8975, -0.8333],\n","        [-0.8824,  0.5176, -0.0164,  0.0000,  0.0000, -0.2221, -0.9137, -0.9667],\n","        [-0.6471, -0.1960,  0.0000,  0.0000,  0.0000,  0.0000, -0.9180, -0.9667],\n","        [ 0.0000,  0.8090,  0.0820, -0.2121,  0.0000,  0.2519,  0.5500, -0.8667],\n","        [-0.7647, -0.1457,  0.0656,  0.0000,  0.0000,  0.1803, -0.2724, -0.8000],\n","        [-0.8824,  0.2864, -0.2131, -0.0909, -0.5414,  0.2072, -0.5431, -0.9000],\n","        [-0.7647, -0.0553,  0.2459, -0.6364, -0.8440, -0.0581, -0.5124, -0.9333],\n","        [ 0.0000, -0.2161,  0.4426, -0.4141, -0.9054,  0.0999, -0.6960,  0.0000],\n","        [ 0.1765,  0.6181,  0.1148, -0.5354, -0.6879, -0.2399, -0.7882, -0.1333],\n","        [-0.8824,  0.7286,  0.1148, -0.0101,  0.3688,  0.2638, -0.4671, -0.7667],\n","        [-0.7647,  0.2161,  0.1475, -0.3535, -0.7754,  0.1654, -0.3100, -0.9333],\n","        [-0.6471,  0.7085,  0.0492, -0.2525, -0.4681,  0.0283, -0.7626, -0.7000],\n","        [-0.7647,  0.0854,  0.0492,  0.0000,  0.0000, -0.0820, -0.9317,  0.0000],\n","        [ 0.0000,  0.4673,  0.1475,  0.0000,  0.0000,  0.1297, -0.7814, -0.7667],\n","        [-0.4118,  0.0854,  0.1803, -0.1313, -0.8227,  0.0760, -0.8420, -0.6000],\n","        [-0.2941,  0.2965,  0.4754, -0.8586, -0.2293, -0.4158, -0.5696,  0.3000],\n","        [ 0.0000, -0.1558,  0.0492, -0.5556, -0.8440,  0.0671, -0.6012,  0.0000],\n","        [-0.8824, -0.0050,  0.1803, -0.3939, -0.9574,  0.1505, -0.7148,  0.0000],\n","        [-0.5294,  0.1055,  0.5082,  0.0000,  0.0000,  0.1207, -0.9035, -0.7000],\n","        [-0.2941,  0.9598,  0.1475,  0.0000,  0.0000, -0.0790, -0.7865, -0.6667],\n","        [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],\n","        [-0.5294,  0.0352, -0.0164, -0.3333, -0.5461, -0.2846, -0.2417, -0.6000],\n","        [-0.8824, -0.0955,  0.0164, -0.6364, -0.8605, -0.2519,  0.0162, -0.8667]]) | Labels tensor([[0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 23 | Inputs tensor([[ 0.0000, -0.0653, -0.0164, -0.4949, -0.7825, -0.1446, -0.6123, -0.9667],\n","        [-0.8824,  0.0955, -0.0164, -0.8384, -0.5697, -0.2429, -0.2579,  0.0000],\n","        [-0.8824,  0.0854, -0.0164, -0.0707, -0.5792,  0.0581, -0.7122, -0.9000],\n","        [-0.8824, -0.0854,  0.0492, -0.5152,  0.0000, -0.1297, -0.9026,  0.0000],\n","        [-0.6471,  0.5879,  0.2459, -0.2727, -0.4208, -0.0581, -0.3399, -0.7667],\n","        [-0.5294,  0.2563,  0.1475, -0.6364, -0.7116, -0.1386, -0.0897, -0.2000],\n","        [-0.8824,  0.0000, -0.2131, -0.5960,  0.0000, -0.2638, -0.9471, -0.9667],\n","        [-0.5294,  0.7186,  0.1803,  0.0000,  0.0000,  0.2996, -0.6576, -0.8333],\n","        [ 0.0000,  0.6583,  0.4754, -0.3333,  0.6076,  0.5589, -0.7020, -0.9333],\n","        [ 0.2941,  0.0352,  0.1148, -0.1919,  0.0000,  0.3770, -0.9590, -0.3000],\n","        [-0.6471,  0.9196,  0.1148, -0.6970, -0.6927, -0.0790, -0.8113, -0.5667],\n","        [-0.4118,  0.0955,  0.0164, -0.1717, -0.6950,  0.0671, -0.6277, -0.8667],\n","        [ 0.0000,  0.0452,  0.0492, -0.2525, -0.8487,  0.0015, -0.6311, -0.9667],\n","        [-0.1765,  0.5276,  0.4426, -0.1111,  0.0000,  0.4903, -0.7788, -0.5000],\n","        [-0.1765, -0.0251,  0.2459, -0.3535, -0.7849,  0.2191, -0.3228, -0.6333],\n","        [-0.6471,  0.7387,  0.2787, -0.2121, -0.5626,  0.0075, -0.2383, -0.6667],\n","        [-0.5294,  0.1859,  0.1475,  0.0000,  0.0000,  0.3264, -0.2946, -0.8333],\n","        [-0.8824, -0.0955,  0.1148, -0.8384,  0.0000, -0.2697, -0.0948, -0.5000],\n","        [-0.1765,  0.0955,  0.3115, -0.3737,  0.0000,  0.0700, -0.1042, -0.2667],\n","        [-0.5294, -0.0452,  0.1475, -0.3535,  0.0000, -0.0432, -0.5440, -0.9000],\n","        [ 0.0588,  0.5477,  0.2787, -0.3939, -0.7636, -0.0790, -0.9266, -0.2000],\n","        [-0.6471, -0.2161, -0.1803, -0.3535, -0.7920, -0.0760, -0.8548, -0.8333],\n","        [-0.6471,  0.1156,  0.0164,  0.0000,  0.0000, -0.3264, -0.9453,  0.0000]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.]])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Inputs tensor([[-0.8824,  0.1256,  0.1803, -0.3939, -0.5839,  0.0253, -0.6157, -0.8667],\n","        [-0.5294,  0.5678,  0.2295,  0.0000,  0.0000,  0.4396, -0.8634, -0.6333],\n","        [-0.6471,  0.7387,  0.2787, -0.2121, -0.5626,  0.0075, -0.2383, -0.6667],\n","        [-0.7647, -0.1256, -0.0492, -0.6768, -0.8771, -0.0253, -0.9249, -0.8667],\n","        [-0.6471,  0.8090,  0.0492, -0.4949, -0.8345,  0.0134, -0.8352, -0.8333],\n","        [-0.8824,  0.6884,  0.4426, -0.4141,  0.0000,  0.0432, -0.2938,  0.0333],\n","        [-0.4118,  0.0955,  0.2295, -0.4747,  0.0000,  0.0730, -0.6003,  0.3000],\n","        [-0.4118,  0.3266,  0.3115,  0.0000,  0.0000, -0.2012, -0.9078,  0.6000],\n","        [-0.8824,  0.4774,  0.5410, -0.1717,  0.0000,  0.4694, -0.7609, -0.8000],\n","        [-0.8824,  0.2261,  0.4754,  0.0303, -0.4799,  0.4814, -0.7891, -0.6667],\n","        [-0.6471,  0.9196,  0.1148, -0.6970, -0.6927, -0.0790, -0.8113, -0.5667],\n","        [-0.5294,  0.1055,  0.2459, -0.5960, -0.7636, -0.1535, -0.9658, -0.8000],\n","        [-0.4118,  0.0955,  0.0164, -0.1717, -0.6950,  0.0671, -0.6277, -0.8667],\n","        [-0.7647, -0.0754,  0.0164, -0.4343,  0.0000, -0.0581, -0.9556, -0.9000],\n","        [-0.1765,  0.4271,  0.4754, -0.5152,  0.1348, -0.0939, -0.9573, -0.2667],\n","        [-0.6471,  0.5075,  0.2459,  0.0000,  0.0000, -0.3741, -0.8898, -0.4667],\n","        [-0.2941,  0.0854, -0.2787, -0.5960, -0.6927, -0.2846, -0.3723, -0.5333],\n","        [-0.6471, -0.1558,  0.1148, -0.3939, -0.7494, -0.0492, -0.5619, -0.8667],\n","        [ 0.0000,  0.0452,  0.0492, -0.2525, -0.8487,  0.0015, -0.6311, -0.9667],\n","        [-0.0588,  0.1055,  0.2459,  0.0000,  0.0000, -0.1714, -0.8642,  0.2333],\n","        [-0.8824, -0.2864,  0.0164,  0.0000,  0.0000, -0.3502, -0.7114, -0.8333],\n","        [-0.2941,  0.9095,  0.5082,  0.0000,  0.0000,  0.0581, -0.8292,  0.5000],\n","        [-0.6471,  0.0352,  0.1803, -0.3939, -0.6407, -0.1773, -0.4432, -0.8000],\n","        [-0.7647,  0.4271,  0.3443, -0.6364, -0.8487, -0.2638, -0.4167,  0.0000],\n","        [-0.7647, -0.1457,  0.0656,  0.0000,  0.0000,  0.1803, -0.2724, -0.8000],\n","        [-0.8824,  0.0352, -0.5082, -0.2323, -0.8038,  0.2906, -0.9103, -0.6000],\n","        [-0.7647, -0.0754,  0.2459, -0.5960,  0.0000, -0.2787,  0.3834, -0.7667],\n","        [-0.6471,  0.7085,  0.0492, -0.2525, -0.4681,  0.0283, -0.7626, -0.7000],\n","        [-0.7647,  0.0251,  0.4098, -0.2727, -0.7163,  0.3562, -0.9582, -0.9333],\n","        [ 0.5294,  0.4573,  0.3443, -0.6162, -0.7400, -0.3383, -0.8574,  0.2000],\n","        [-0.5294, -0.0452, -0.0164, -0.3535,  0.0000,  0.0551, -0.8241, -0.7667],\n","        [ 0.4118,  0.5176,  0.1475, -0.1919, -0.3593,  0.2459, -0.4330, -0.4333]]) | Labels tensor([[1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","Epoch: 1 | Inputs tensor([[-0.7647, -0.0955, -0.0164,  0.0000,  0.0000, -0.2996, -0.9035, -0.8667],\n","        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],\n","        [-0.8824, -0.0050,  0.1803, -0.3939, -0.9574,  0.1505, -0.7148,  0.0000],\n","        [-0.8824,  0.8995, -0.0164, -0.5354,  1.0000, -0.1028, -0.7267,  0.2667],\n","        [ 0.5294,  0.2965,  0.0000, -0.3939,  0.0000,  0.1893, -0.5807, -0.2333],\n","        [-0.7647, -0.3166,  0.0164, -0.7374, -0.9645, -0.4009, -0.8471, -0.9333],\n","        [ 0.0000,  0.0251,  0.2787, -0.1919, -0.7872,  0.0283, -0.8634, -0.9000],\n","        [ 0.2941,  0.3668,  0.3770, -0.2929, -0.6927, -0.1565, -0.8446, -0.3000],\n","        [-0.6471, -0.1156, -0.0492, -0.7778, -0.8723, -0.2608, -0.8386, -0.9667],\n","        [-0.5294, -0.0050,  0.2459, -0.6970, -0.8794, -0.3085, -0.8762,  0.0000],\n","        [-0.8824,  0.1960, -0.2787, -0.0505, -0.8511,  0.0581, -0.8275, -0.8667],\n","        [-0.4118,  0.2362,  0.2131, -0.1919, -0.8180,  0.0164, -0.8369, -0.7667],\n","        [-0.5294, -0.0754,  0.3115,  0.0000,  0.0000,  0.2578, -0.8642, -0.7333],\n","        [-0.6471,  0.1156, -0.0492, -0.3737, -0.8960, -0.1207, -0.6994, -0.9667],\n","        [-0.5294, -0.1457, -0.0492, -0.5556, -0.8842, -0.1714, -0.8053, -0.7667],\n","        [-0.8824,  0.4372,  0.4098, -0.3939, -0.2199, -0.1028, -0.3049, -0.9333],\n","        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667],\n","        [ 0.2941,  0.0352,  0.1148, -0.1919,  0.0000,  0.3770, -0.9590, -0.3000],\n","        [-0.8824,  0.2462,  0.2131, -0.2727,  0.0000, -0.1714, -0.9812, -0.7000],\n","        [-0.8824,  0.2864, -0.2131, -0.0909, -0.5414,  0.2072, -0.5431, -0.9000],\n","        [-0.0588,  0.0050,  0.2131, -0.1919, -0.4917,  0.1744, -0.5021, -0.2667],\n","        [-0.6471,  0.8794,  0.1475, -0.5556, -0.5272,  0.0849, -0.7182, -0.5000],\n","        [ 0.4118,  0.2161,  0.2787, -0.6566,  0.0000, -0.2101, -0.8454,  0.3667],\n","        [ 0.0000,  0.2362,  0.4426, -0.2525,  0.0000,  0.0492, -0.8984, -0.7333],\n","        [ 0.0000,  0.6784,  0.0000,  0.0000,  0.0000, -0.0373, -0.3501, -0.7000],\n","        [ 0.0000,  0.1960,  0.0820, -0.4545,  0.0000,  0.1565, -0.8454, -0.9667],\n","        [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333],\n","        [-0.2941,  0.0553,  0.1475, -0.3535, -0.8392, -0.0820, -0.9624, -0.4667],\n","        [-0.0588,  0.2563,  0.5738,  0.0000,  0.0000,  0.0000, -0.8685,  0.1000],\n","        [-0.8824, -0.0452,  0.3443, -0.4949, -0.5745,  0.0432, -0.8676, -0.2667],\n","        [-0.6471,  0.4171,  0.0000,  0.0000,  0.0000, -0.1058, -0.4167, -0.8000],\n","        [-0.8824,  0.0955, -0.0164, -0.8384, -0.5697, -0.2429, -0.2579,  0.0000]]) | Labels tensor([[1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.]])\n","Epoch: 2 | Inputs tensor([[-0.7647, -0.0955,  0.1475, -0.6566,  0.0000, -0.1863, -0.9940, -0.9667],\n","        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n","        [ 0.0000,  0.2864,  0.1148, -0.6162, -0.5745, -0.0909,  0.1213, -0.8667],\n","        [ 0.0000, -0.0452,  0.3934, -0.4949, -0.9149,  0.1148, -0.8557, -0.9000],\n","        [-0.4118,  0.2663,  0.2787, -0.4545, -0.9480, -0.1177, -0.6917, -0.3667],\n","        [-0.5294, -0.0955,  0.0000,  0.0000,  0.0000, -0.1654, -0.5457, -0.6667],\n","        [ 0.1765, -0.2462,  0.3443,  0.0000,  0.0000, -0.0075, -0.8420, -0.4333],\n","        [ 0.0000,  0.1859,  0.0492, -0.5354, -0.7896,  0.0000,  0.4116,  0.0000],\n","        [-0.1765,  0.8794,  0.1148, -0.2121, -0.2813,  0.1237, -0.8497, -0.3333],\n","        [-0.7647,  0.1156, -0.0164,  0.0000,  0.0000, -0.2191, -0.7737, -0.9333],\n","        [-0.6471, -0.0050,  0.3115, -0.7778, -0.8487, -0.4247, -0.8241, -0.7000],\n","        [-0.8824, -0.2864,  0.2787,  0.0101, -0.8936, -0.0104, -0.7062,  0.0000],\n","        [-0.7647,  0.2965,  0.3770,  0.0000,  0.0000, -0.1654, -0.8241, -0.8000],\n","        [-0.1765,  0.6080, -0.1148, -0.3535, -0.5863, -0.0909, -0.5645, -0.4000],\n","        [-0.7647,  0.7588,  0.4426,  0.0000,  0.0000, -0.3174, -0.7882, -0.9667],\n","        [ 0.0000,  0.0854,  0.1148, -0.5960,  0.0000, -0.1863, -0.3945, -0.6333],\n","        [-0.2941,  0.2563,  0.1148, -0.3939, -0.7163, -0.1058, -0.6704, -0.6333],\n","        [-0.2941, -0.1960,  0.0820, -0.3939,  0.0000, -0.2191, -0.7993, -0.3333],\n","        [-0.1765, -0.1658,  0.2787, -0.4747, -0.8322, -0.1267, -0.4116, -0.5000],\n","        [-0.2941,  0.4774,  0.3115,  0.0000,  0.0000, -0.1207, -0.9146, -0.0333],\n","        [ 0.0000,  0.0553,  0.4754,  0.0000,  0.0000, -0.1177, -0.8984, -0.1667],\n","        [ 0.0000,  0.7990, -0.1803, -0.2727, -0.6241,  0.1267, -0.6781, -0.9667],\n","        [ 0.0588,  0.3065,  0.1475,  0.0000,  0.0000,  0.0194, -0.5098, -0.2000],\n","        [ 0.0000, -0.2563, -0.1475, -0.7980, -0.9149, -0.1714, -0.8369, -0.9667],\n","        [-0.5294,  0.2362,  0.3115, -0.6970, -0.5839, -0.0462, -0.6883, -0.5667],\n","        [-0.5294,  0.0955,  0.0492, -0.1111, -0.7660,  0.0373, -0.2938, -0.8333],\n","        [-0.6471,  0.6985,  0.2131, -0.6162, -0.7045, -0.1088, -0.8377, -0.6667],\n","        [-0.5294,  0.3769,  0.3770,  0.0000,  0.0000, -0.0700, -0.8514, -0.7000],\n","        [-0.4118,  0.0000,  0.3115, -0.3535,  0.0000,  0.2221, -0.7711, -0.4667],\n","        [-0.0588,  0.2060,  0.2787,  0.0000,  0.0000, -0.2548, -0.7173,  0.4333],\n","        [-0.8824,  0.0854,  0.4426, -0.6162,  0.0000, -0.1922, -0.7250, -0.9000],\n","        [-0.8824, -0.1357,  0.0820,  0.0505, -0.8463,  0.2310, -0.2835, -0.7333]]) | Labels tensor([[1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 3 | Inputs tensor([[ 0.0000,  0.6583,  0.4754, -0.3333,  0.6076,  0.5589, -0.7020, -0.9333],\n","        [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],\n","        [-0.5294,  0.8392,  0.0000,  0.0000,  0.0000, -0.1535, -0.8856, -0.5000],\n","        [ 0.1765,  0.2563,  0.1475, -0.4747, -0.7281, -0.0730, -0.8915, -0.3333],\n","        [-0.8824,  0.8191,  0.2787, -0.1515, -0.3073,  0.1922,  0.0077, -0.9667],\n","        [ 0.0000,  0.3166,  0.0820, -0.1919,  0.0000,  0.0224, -0.8992, -0.9667],\n","        [ 0.0000,  0.0553,  0.0492, -0.1717, -0.6643,  0.2370, -0.9189, -0.9667],\n","        [-0.8824,  0.9698,  0.2459, -0.2727, -0.4113,  0.0879, -0.3194, -0.7333],\n","        [-0.5294,  0.3266,  0.4098, -0.3737,  0.0000, -0.1654, -0.7088,  0.4000],\n","        [-0.8824,  0.8191,  0.0492, -0.3939, -0.5745,  0.0164, -0.7865, -0.4333],\n","        [-0.0588,  0.0854,  0.1475,  0.0000,  0.0000, -0.0909, -0.2511, -0.6000],\n","        [-0.2941,  0.2462,  0.1803,  0.0000,  0.0000, -0.1773, -0.7523, -0.7333],\n","        [-0.1765,  0.0352,  0.0820, -0.3535,  0.0000,  0.1654, -0.7728, -0.6667],\n","        [ 0.0588,  0.2362,  0.1475, -0.1111, -0.7778, -0.0134, -0.7472, -0.3667],\n","        [-0.7647, -0.0050,  0.0000,  0.0000,  0.0000, -0.3383, -0.9744, -0.9333],\n","        [-0.7647,  0.2362, -0.2131, -0.3535, -0.6099,  0.2548, -0.6225, -0.8333],\n","        [-0.0588,  0.9799,  0.2131,  0.0000,  0.0000, -0.2280, -0.0495, -0.4000],\n","        [-0.4118,  0.1658,  0.2131, -0.4141,  0.0000, -0.0373, -0.5030, -0.5333],\n","        [-0.6471,  0.0653,  0.1803,  0.0000,  0.0000, -0.2310, -0.8898, -0.8000],\n","        [-0.5294,  0.4673,  0.2787,  0.0000,  0.0000,  0.1475, -0.6225,  0.5333],\n","        [-0.1765, -0.0553,  0.0492, -0.4949, -0.8132, -0.0075, -0.4364, -0.3333],\n","        [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],\n","        [-0.7647,  0.0854,  0.0164, -0.7980, -0.3428, -0.2459, -0.3143, -0.9667],\n","        [-0.8824,  0.0251,  0.2131,  0.0000,  0.0000,  0.1773, -0.8164, -0.3000],\n","        [-0.0588,  0.8693,  0.4754, -0.2929, -0.4681,  0.0283, -0.7054, -0.4667],\n","        [ 0.0588,  0.6482,  0.2787,  0.0000,  0.0000, -0.0224, -0.9402, -0.2000],\n","        [-0.2941, -0.0151, -0.0492, -0.3333, -0.5508,  0.0134, -0.6994, -0.2667],\n","        [-0.8824,  0.1457,  0.0820, -0.2727, -0.5272,  0.1356, -0.8198,  0.0000],\n","        [-0.8824,  0.0000, -0.2131, -0.5960,  0.0000, -0.2638, -0.9471, -0.9667],\n","        [-0.8824,  0.0955, -0.0820, -0.5758, -0.6809, -0.2489, -0.3553, -0.9333],\n","        [-0.6471,  0.3970, -0.1148,  0.0000,  0.0000, -0.2370, -0.7233, -0.9667],\n","        [-0.7647, -0.0553,  0.2459, -0.6364, -0.8440, -0.0581, -0.5124, -0.9333]]) | Labels tensor([[1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.]])\n","Epoch: 4 | Inputs tensor([[-0.6471, -0.0352,  0.2787, -0.2121,  0.0000,  0.1118, -0.8634, -0.3667],\n","        [-0.6471,  0.0854,  0.0164, -0.5152,  0.0000, -0.2250, -0.8762, -0.8667],\n","        [-0.6471,  0.7387,  0.3770, -0.3333,  0.1206,  0.0641, -0.8463, -0.9667],\n","        [ 0.0000,  0.2462, -0.0820, -0.7374, -0.7518, -0.3502, -0.6806,  0.0000],\n","        [-0.5294,  0.1457,  0.0492,  0.0000,  0.0000, -0.1386, -0.9590, -0.9000],\n","        [-0.4118,  0.1759,  0.4098, -0.3939, -0.7518,  0.1654, -0.8523, -0.3000],\n","        [-0.7647,  0.5578, -0.1475, -0.4545,  0.2766,  0.1535, -0.8617, -0.8667],\n","        [-0.7647,  0.2060, -0.1148,  0.0000,  0.0000, -0.2012, -0.6781, -0.8000],\n","        [-0.8824,  0.2864,  0.4426, -0.2121, -0.7400,  0.0879, -0.1640, -0.4667],\n","        [-0.7647,  0.0050, -0.1148, -0.4343, -0.7518,  0.1267, -0.6413, -0.9000],\n","        [-0.4118,  0.5879,  0.3770, -0.1717, -0.5035,  0.1744, -0.7293, -0.7333],\n","        [ 0.0000,  0.7990,  0.4754, -0.4545,  0.0000,  0.3145, -0.4808, -0.9333],\n","        [ 0.0000, -0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8480, -0.8667],\n","        [-0.6471,  0.1658,  0.0000,  0.0000,  0.0000, -0.2996, -0.9069, -0.9333],\n","        [-0.7647,  0.2261, -0.1475, -0.1313, -0.6265,  0.0790, -0.3698, -0.7667],\n","        [-0.7647, -0.0050, -0.1475, -0.6970, -0.7778, -0.2668, -0.5226,  0.0000],\n","        [-0.8824,  0.0955, -0.0492, -0.6364, -0.7258, -0.1505, -0.8796, -0.9667],\n","        [-0.8824, -0.0955,  0.0164, -0.7576, -0.8983, -0.1893, -0.5713, -0.9000],\n","        [-0.7647,  0.5779,  0.2131, -0.2929,  0.0402,  0.1744, -0.9522, -0.7000],\n","        [-0.5294,  0.1055,  0.0820,  0.0000,  0.0000, -0.0492, -0.6644, -0.7333],\n","        [-0.6471, -0.1960,  0.0000,  0.0000,  0.0000,  0.0000, -0.9180, -0.9667],\n","        [-0.2941,  0.0754,  0.4426,  0.0000,  0.0000,  0.0969, -0.4458, -0.6667],\n","        [-0.7647,  0.0653, -0.0820, -0.4545, -0.6099, -0.1356, -0.7028, -0.9667],\n","        [-0.2941,  0.4472,  0.1803, -0.4545, -0.4610,  0.0104, -0.8488, -0.3667],\n","        [ 0.0588,  0.7085,  0.2131, -0.3737,  0.0000,  0.3115, -0.7225, -0.2667],\n","        [ 0.0588,  0.8492,  0.3934, -0.6970,  0.0000, -0.1058, -0.0307, -0.0667],\n","        [ 0.0000, -0.3266,  0.2459,  0.0000,  0.0000,  0.3502, -0.9009, -0.1667],\n","        [-0.2941,  0.1457,  0.0000,  0.0000,  0.0000,  0.0000, -0.9052, -0.8333],\n","        [ 0.1765,  0.0854,  0.0820,  0.0000,  0.0000, -0.0343, -0.8343, -0.3000],\n","        [-0.1765,  0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8061, -0.9000],\n","        [ 0.2941,  0.1156,  0.3770, -0.1919,  0.0000,  0.3949, -0.2767, -0.2000],\n","        [-0.6471,  0.2965,  0.0492, -0.4141, -0.7281, -0.2131, -0.8796, -0.7667]]) | Labels tensor([[1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.]])\n","Epoch: 5 | Inputs tensor([[-0.5294,  0.1859,  0.1475,  0.0000,  0.0000,  0.3264, -0.2946, -0.8333],\n","        [-0.8824, -0.1859,  0.2131, -0.1717, -0.8652,  0.3800, -0.1307, -0.6333],\n","        [-0.0588,  0.1256,  0.1803,  0.0000,  0.0000, -0.2966, -0.3493,  0.2333],\n","        [-0.0588,  0.1859,  0.1803, -0.6162,  0.0000, -0.3115,  0.1939, -0.1667],\n","        [-0.7647, -0.1256,  0.0000, -0.5354,  0.0000, -0.1386, -0.4065, -0.8667],\n","        [ 0.2941,  0.3568,  0.0000,  0.0000,  0.0000,  0.5589, -0.5730, -0.3667],\n","        [-0.6471, -0.1759,  0.1475,  0.0000,  0.0000, -0.3711, -0.7344, -0.8667],\n","        [-0.5294, -0.0452,  0.1475, -0.3535,  0.0000, -0.0432, -0.5440, -0.9000],\n","        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n","        [-0.7647,  0.2965,  0.0000,  0.0000,  0.0000,  0.1475, -0.8070, -0.3333],\n","        [ 0.0000,  0.4573,  0.0000,  0.0000,  0.0000,  0.3174, -0.5286, -0.6667],\n","        [-0.7647,  0.0151, -0.0492, -0.2929, -0.7872, -0.3502, -0.9342, -0.9667],\n","        [-0.6471, -0.2161, -0.1803, -0.3535, -0.7920, -0.0760, -0.8548, -0.8333],\n","        [-0.6471,  0.8291,  0.2131,  0.0000,  0.0000, -0.0909, -0.7720, -0.7333],\n","        [-0.4118, -0.0452,  0.1803, -0.3333,  0.0000,  0.1237, -0.7506, -0.8000],\n","        [-0.4118,  0.3668,  0.3770, -0.1717, -0.7920,  0.0432, -0.8224, -0.5333],\n","        [-0.8824, -0.2864, -0.2131, -0.6364, -0.8203, -0.3920, -0.7908, -0.9667],\n","        [ 0.5294,  0.0452,  0.1803,  0.0000,  0.0000, -0.0700, -0.6695, -0.4333],\n","        [-0.7647,  0.0050,  0.1148, -0.4949, -0.8322,  0.1475, -0.7899, -0.8333],\n","        [-0.2941, -0.0352,  0.0000,  0.0000,  0.0000, -0.2936, -0.9044, -0.7667],\n","        [ 0.0000,  0.1960,  0.0000,  0.0000,  0.0000, -0.0343, -0.9462, -0.9000],\n","        [ 0.0000,  0.8090,  0.2787,  0.2727, -0.9669,  0.7705,  1.0000, -0.8667],\n","        [-0.7647, -0.1156,  0.2131, -0.6162, -0.8747, -0.1356, -0.8711, -0.9667],\n","        [ 0.0000,  0.2663,  0.4098, -0.4545, -0.7163, -0.1833, -0.6268,  0.0000],\n","        [-0.7647,  0.5879,  0.4754,  0.0000,  0.0000, -0.0581, -0.3792,  0.5000],\n","        [ 0.0000,  0.2060,  0.2131, -0.6364, -0.8511, -0.0909, -0.8232, -0.8333],\n","        [ 0.0000,  0.1960,  0.0492, -0.6364, -0.7825,  0.0402, -0.4475, -0.9333],\n","        [-0.4118,  0.0653,  0.3443, -0.3939,  0.0000,  0.1773, -0.8224, -0.4333],\n","        [-0.5294,  0.3266,  0.0000,  0.0000,  0.0000, -0.0194, -0.8087, -0.9333],\n","        [-0.1765,  0.1960,  0.0000,  0.0000,  0.0000, -0.2489, -0.8881, -0.4667],\n","        [-0.8824, -0.1256,  0.2787, -0.4545, -0.9244,  0.0313, -0.9804, -0.9667],\n","        [-0.1765,  0.1457,  0.0492,  0.0000,  0.0000, -0.1833, -0.4415, -0.5667]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","Epoch: 6 | Inputs tensor([[-0.7647, -0.0854,  0.0164,  0.0000,  0.0000, -0.1863, -0.6183, -0.9667],\n","        [-0.7647,  0.1256,  0.4098, -0.1515, -0.6217,  0.1446, -0.8565, -0.7667],\n","        [-0.2941,  0.0251,  0.4754, -0.2121,  0.0000,  0.0641, -0.4910, -0.7667],\n","        [-0.7647,  0.4171, -0.0492, -0.3131, -0.6974, -0.2429, -0.4697, -0.9000],\n","        [-0.5294,  0.1055,  0.5082,  0.0000,  0.0000,  0.1207, -0.9035, -0.7000],\n","        [-0.1765,  0.9698,  0.4754,  0.0000,  0.0000,  0.1863, -0.6815, -0.3333],\n","        [ 1.0000,  0.6382,  0.1803, -0.1717, -0.7305,  0.2191, -0.3689, -0.1333],\n","        [-0.1765,  0.3668,  0.4754,  0.0000,  0.0000, -0.1088, -0.8873, -0.0333],\n","        [-0.8824,  0.0754,  0.1803, -0.3939, -0.8061, -0.0820, -0.3655, -0.9000],\n","        [-0.7647, -0.3166,  0.1475, -0.3535, -0.8440, -0.2548, -0.9069, -0.8667],\n","        [-0.7647, -0.1658,  0.0656, -0.4343, -0.8440,  0.0969, -0.5295, -0.9000],\n","        [-0.0588, -0.1457, -0.0984, -0.5960,  0.0000, -0.2727, -0.9505, -0.3000],\n","        [-0.8824,  0.1357,  0.0492, -0.2929,  0.0000,  0.0015, -0.6029,  0.0000],\n","        [-0.1765,  0.0955,  0.3115, -0.3737,  0.0000,  0.0700, -0.1042, -0.2667],\n","        [ 0.0000,  0.0050,  0.1475, -0.4747, -0.8818, -0.0820, -0.5568,  0.0000],\n","        [-0.5294, -0.2362,  0.0164,  0.0000,  0.0000,  0.0134, -0.7327, -0.8667],\n","        [-0.7647,  0.2261, -0.0164, -0.6364, -0.7494, -0.1118, -0.4543, -0.9667],\n","        [-0.7647,  0.5578,  0.2131, -0.6566, -0.7731, -0.2072, -0.6968, -0.8000],\n","        [-0.8824,  0.2864,  0.6066, -0.1717, -0.8629, -0.0462,  0.0615, -0.6000],\n","        [-0.2941,  0.0251,  0.3443,  0.0000,  0.0000, -0.0820, -0.9129, -0.5000],\n","        [-0.7647,  0.1256,  0.2787,  0.0101, -0.6690,  0.1744, -0.9172, -0.9000],\n","        [-0.4118,  0.3668,  0.3443,  0.0000,  0.0000,  0.0000, -0.5201,  0.6000],\n","        [-0.8824, -0.1960, -0.0984,  0.0000,  0.0000, -0.4307, -0.8463,  0.0000],\n","        [ 0.0588,  0.2462,  0.1475, -0.3333, -0.0496,  0.0551, -0.8258, -0.5667],\n","        [-0.4118,  0.3970,  0.3115, -0.2929, -0.6217, -0.0581, -0.7583, -0.8667],\n","        [-0.5294,  0.1156,  0.1803, -0.0505, -0.5106,  0.1058,  0.1204,  0.1667],\n","        [ 0.4118,  0.0050,  0.3770, -0.3333, -0.7518, -0.1058, -0.6499, -0.1667],\n","        [ 0.1765,  0.0151,  0.4098, -0.2525,  0.0000,  0.3592, -0.0965, -0.4333],\n","        [-0.7647, -0.0553,  0.1148, -0.6364, -0.8203, -0.2250, -0.5875,  0.0000],\n","        [ 0.4118, -0.1558,  0.1803, -0.3737,  0.0000, -0.1148, -0.8130, -0.1667],\n","        [-0.8824, -0.1658,  0.1148,  0.0000,  0.0000, -0.4575, -0.5337, -0.8000],\n","        [-0.8824,  0.1558,  0.1475, -0.3939, -0.7731,  0.0313, -0.6149, -0.6333]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.]])\n","Epoch: 7 | Inputs tensor([[-0.8824, -0.1759,  0.0492, -0.7374, -0.7754, -0.3681, -0.7122, -0.9333],\n","        [ 0.0000,  0.0050,  0.4426,  0.2121, -0.7400,  0.3949, -0.2451, -0.6667],\n","        [-0.8824,  0.0050,  0.0820, -0.4141, -0.5366, -0.0462, -0.6874, -0.3000],\n","        [-0.8824, -0.2663, -0.1803, -0.7980,  0.0000, -0.3145, -0.8548,  0.0000],\n","        [ 0.0000,  0.8191,  0.4426, -0.1111,  0.2057,  0.2906, -0.8770, -0.8333],\n","        [-0.5294, -0.0553,  0.0656, -0.5556,  0.0000, -0.2638, -0.9402,  0.0000],\n","        [-0.8824,  0.7387,  0.2131,  0.0000,  0.0000,  0.0969, -0.9915, -0.4333],\n","        [ 0.0000,  0.3568,  0.5410, -0.0707, -0.6572,  0.2101, -0.8241, -0.8333],\n","        [ 0.0000,  0.2965,  0.8033, -0.0707, -0.6927,  1.0000, -0.7942, -0.8333],\n","        [-0.8824,  0.0754, -0.1803, -0.6162,  0.0000, -0.1565, -0.9120, -0.7333],\n","        [-0.5294,  0.3467,  0.1803,  0.0000,  0.0000, -0.2906, -0.8301,  0.3000],\n","        [-0.4118, -0.1357,  0.1148, -0.4343, -0.8322, -0.0999, -0.7558, -0.9000],\n","        [-0.6471,  0.2864,  0.2787,  0.0000,  0.0000, -0.3711, -0.8377,  0.1333],\n","        [-0.6471,  0.2563, -0.0492,  0.0000,  0.0000, -0.0581, -0.9377, -0.9000],\n","        [-0.0588,  0.7990,  0.1803, -0.1515, -0.6927, -0.0253, -0.4526, -0.5000],\n","        [-0.8824,  0.3668,  0.2131,  0.0101, -0.5177,  0.1148, -0.7259, -0.9000],\n","        [-0.8824,  0.4975,  0.1148, -0.4141, -0.6998, -0.1267, -0.7686, -0.3000],\n","        [ 0.0000,  0.9900,  0.0820, -0.3535, -0.3522,  0.2310, -0.6379, -0.7667],\n","        [-0.5294,  0.0352, -0.0164, -0.3333, -0.5461, -0.2846, -0.2417, -0.6000],\n","        [-0.8824,  0.0050,  0.2131, -0.7576, -0.8913, -0.4188, -0.9394, -0.7667],\n","        [-0.5294,  0.9799,  0.1475, -0.2121,  0.7589,  0.0939,  0.9223, -0.6667],\n","        [ 0.0000,  0.6281,  0.2459, -0.2727,  0.0000,  0.4784, -0.7558, -0.8333],\n","        [-0.6471, -0.0955,  0.2787,  0.0000,  0.0000,  0.2727, -0.5892,  0.0000],\n","        [-0.8824,  0.1759,  0.4426, -0.5152, -0.6572,  0.0283, -0.7225, -0.3667],\n","        [ 0.4118, -0.0754,  0.0164, -0.8586, -0.3901, -0.1773, -0.2758, -0.2333],\n","        [-0.5294,  0.4171,  0.2131,  0.0000,  0.0000, -0.1773, -0.8582, -0.3667],\n","        [-0.5294, -0.0050,  0.1148, -0.2323,  0.0000, -0.0224, -0.9428, -0.6000],\n","        [-0.2941,  0.6281,  0.0164,  0.0000,  0.0000, -0.2757, -0.9146, -0.0333],\n","        [ 0.0000,  0.6583,  0.2459, -0.1313, -0.3972,  0.4277, -0.8454, -0.8333],\n","        [-0.2941,  0.2965,  0.4754, -0.8586, -0.2293, -0.4158, -0.5696,  0.3000],\n","        [-0.7647, -0.1156, -0.0492, -0.4747, -0.9622, -0.1535, -0.4125, -0.9667],\n","        [-0.5294, -0.0050,  0.1803, -0.6566,  0.0000, -0.2370, -0.8155, -0.7667]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 8 | Inputs tensor([[ 0.0000,  0.1357,  0.2459,  0.0000,  0.0000, -0.0075, -0.8292, -0.9333],\n","        [-0.0588, -0.2563,  0.1475, -0.1919, -0.8842,  0.0522, -0.4646, -0.4000],\n","        [-0.4118,  0.8794,  0.2459, -0.4545, -0.5106,  0.2996, -0.1836,  0.0667],\n","        [-0.5294,  0.1658,  0.1803, -0.7576, -0.7943, -0.3413, -0.6712, -0.4667],\n","        [-0.5294,  0.8995,  0.8033, -0.3737,  0.0000, -0.1505, -0.4859, -0.4667],\n","        [-0.7647, -0.0452, -0.1148, -0.7172, -0.7920, -0.2221, -0.4278, -0.9667],\n","        [-0.4118,  0.1457,  0.2131,  0.0000,  0.0000, -0.2578, -0.4313,  0.2000],\n","        [-0.6471,  0.2362,  0.6393, -0.2929, -0.4326,  0.7079, -0.3151, -0.9667],\n","        [-0.1765,  0.6884,  0.4426, -0.1515, -0.2411,  0.1386, -0.3945, -0.3667],\n","        [ 0.0000,  0.0653,  0.1475, -0.2525, -0.6501,  0.1744, -0.5500, -0.9667],\n","        [ 0.2941, -0.1457,  0.2131,  0.0000,  0.0000, -0.1028, -0.8104, -0.5333],\n","        [-0.5294,  0.2764,  0.4426, -0.7778, -0.6336,  0.0283, -0.5559, -0.7667],\n","        [ 0.0000,  0.1357,  0.3115, -0.6768,  0.0000, -0.0760, -0.3202,  0.0000],\n","        [-0.0588, -0.0854,  0.3443,  0.0000,  0.0000,  0.0611, -0.5653,  0.5667],\n","        [-0.1765,  0.3769,  0.4754, -0.1717,  0.0000, -0.0462, -0.7327, -0.4000],\n","        [-0.5294,  0.1558,  0.1803,  0.0000,  0.0000, -0.1386, -0.7455, -0.1667],\n","        [-0.6471, -0.2161,  0.1475,  0.0000,  0.0000, -0.0313, -0.8360, -0.4000],\n","        [-0.5294,  0.1759,  0.0164, -0.7576,  0.0000, -0.1148, -0.7421, -0.7000],\n","        [-0.8824,  0.0553, -0.0492,  0.0000,  0.0000, -0.2757, -0.9069,  0.0000],\n","        [-0.0588,  0.3367,  0.1803,  0.0000,  0.0000, -0.0194, -0.8360, -0.4000],\n","        [ 0.0000,  0.3568,  0.1148, -0.1515, -0.4090,  0.2608, -0.7549, -0.9000],\n","        [-0.8824, -0.1960,  0.2131, -0.7778, -0.8582, -0.1058, -0.6166, -0.9667],\n","        [-0.8824,  0.2462, -0.0164, -0.3535,  0.0000,  0.0671, -0.6277,  0.0000],\n","        [-0.4118,  0.3769,  0.7705,  0.0000,  0.0000,  0.4545, -0.8728, -0.4667],\n","        [-0.8824,  0.1156,  0.0164, -0.7374, -0.5697, -0.2846, -0.9488, -0.9333],\n","        [-0.4118, -0.0251,  0.2459, -0.4545,  0.0000,  0.0611, -0.7438,  0.0333],\n","        [-0.6471,  0.0653, -0.1148, -0.5758, -0.6265, -0.0790, -0.8173, -0.9000],\n","        [-0.5294,  0.2563,  0.3115,  0.0000,  0.0000, -0.0373, -0.6089, -0.8000],\n","        [-0.4118, -0.0050, -0.1148, -0.4343, -0.8038,  0.0134, -0.6405, -0.7000],\n","        [-0.2941, -0.1960,  0.3115, -0.2727,  0.0000,  0.1863, -0.9155, -0.7667],\n","        [-0.1765, -0.3769,  0.2787,  0.0000,  0.0000, -0.0283, -0.7327, -0.3333],\n","        [-0.4118,  0.6281,  0.7049,  0.0000,  0.0000,  0.1237, -0.9377,  0.0333]]) | Labels tensor([[0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","Epoch: 9 | Inputs tensor([[-0.2941,  0.3467,  0.3115, -0.2525, -0.1253,  0.3770, -0.8634, -0.1667],\n","        [ 0.1765,  0.3367,  0.1148,  0.0000,  0.0000, -0.1952, -0.8574, -0.5000],\n","        [-0.6471,  0.0251, -0.2787, -0.5960, -0.7778, -0.0820, -0.7250, -0.8333],\n","        [-0.7647,  0.0653,  0.0492, -0.2929, -0.7187, -0.0909,  0.1289, -0.5667],\n","        [-0.8824, -0.0955,  0.0164, -0.6364, -0.8605, -0.2519,  0.0162, -0.8667],\n","        [-0.4118,  0.2462,  0.2131,  0.0000,  0.0000,  0.0134, -0.8787, -0.4333],\n","        [-0.6471,  0.2161, -0.1475,  0.0000,  0.0000,  0.0730, -0.9582, -0.8667],\n","        [-0.7647,  0.0050,  0.1475,  0.0505, -0.8652,  0.2072, -0.4885, -0.8667],\n","        [-0.7647, -0.4372, -0.0820, -0.4343, -0.8936, -0.2787, -0.7831, -0.9667],\n","        [-0.8824,  0.1859, -0.0492, -0.2727, -0.7778, -0.0075, -0.8437, -0.9333],\n","        [-0.6471, -0.1055,  0.2131, -0.6768, -0.7991, -0.0939, -0.5961, -0.4333],\n","        [ 0.0000, -0.0653,  0.6393, -0.2121, -0.8298,  0.2936, -0.1947, -0.5333],\n","        [ 0.0000,  0.2563,  0.1148,  0.0000,  0.0000, -0.2638, -0.8907,  0.0000],\n","        [-0.5294,  0.8492,  0.2787, -0.2121, -0.3452,  0.1028, -0.8412, -0.6667],\n","        [-0.7647,  0.2965,  0.2131, -0.4747, -0.5154, -0.0104, -0.5619, -0.8667],\n","        [ 0.0000,  0.3869,  0.0000,  0.0000,  0.0000,  0.0820, -0.2699, -0.8667],\n","        [-0.7647,  0.0553,  0.3115, -0.0909, -0.5485,  0.0045, -0.4594, -0.7333],\n","        [-0.8824, -0.0251,  0.0492, -0.6162, -0.8061, -0.4575, -0.8113,  0.0000],\n","        [-0.6471, -0.2563,  0.1148, -0.4343, -0.8936, -0.1148, -0.8164, -0.9333],\n","        [ 0.0588,  0.2060,  0.1803, -0.5556, -0.8676, -0.3800, -0.4406, -0.1000],\n","        [-0.5294,  0.1457,  0.0656,  0.0000,  0.0000, -0.3472, -0.6977, -0.4667],\n","        [-0.8824,  0.0050,  0.1803, -0.7576, -0.8345, -0.2459, -0.5047, -0.7667],\n","        [-0.8824,  0.3065,  0.1475, -0.7374, -0.7518, -0.2280, -0.6635, -0.9667],\n","        [-0.5294,  0.1759,  0.0492, -0.4545, -0.7163, -0.0104, -0.8702, -0.9000],\n","        [-0.0588,  0.0955,  0.2459, -0.2121, -0.7305, -0.1684, -0.5201, -0.6667],\n","        [-0.0588,  0.9698,  0.2459, -0.4141, -0.3381,  0.1177, -0.5500,  0.2000],\n","        [-0.6471,  0.9397,  0.1475, -0.3737,  0.0000,  0.0402, -0.8608, -0.8667],\n","        [ 0.0588,  0.2261, -0.0820,  0.0000,  0.0000, -0.0075, -0.1153, -0.6000],\n","        [-0.6471,  0.4271,  0.3115, -0.6970,  0.0000, -0.0343, -0.8958,  0.4000],\n","        [-0.7647, -0.0955,  0.3115, -0.7172, -0.8700, -0.2727, -0.8540, -0.9000],\n","        [-0.7647,  0.2864,  0.2787, -0.2525, -0.5697,  0.2906, -0.0213, -0.6667],\n","        [-0.6471,  0.0251,  0.2131,  0.0000,  0.0000, -0.1207, -0.9633, -0.6333]]) | Labels tensor([[0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.]])\n","Epoch: 10 | Inputs tensor([[-0.6471, -0.0050,  0.0164, -0.6162, -0.8251, -0.3502, -0.8284, -0.8333],\n","        [ 0.0000, -0.2161,  0.4426, -0.4141, -0.9054,  0.0999, -0.6960,  0.0000],\n","        [ 0.0000,  0.4171,  0.3770, -0.4747,  0.0000, -0.0343, -0.6968, -0.9667],\n","        [-0.5294,  0.4472,  0.3443, -0.3535,  0.0000,  0.1475, -0.5935, -0.4667],\n","        [-0.8824,  0.4372,  0.2131, -0.5556, -0.8558, -0.2191, -0.8480,  0.0000],\n","        [ 0.1765,  0.6281,  0.3770,  0.0000,  0.0000, -0.1744, -0.9112,  0.1000],\n","        [-0.7647,  0.1256,  0.1148, -0.5556, -0.7778,  0.0164, -0.7976, -0.8333],\n","        [-0.4118, -0.2663, -0.0164,  0.0000,  0.0000, -0.2012, -0.8377, -0.8000],\n","        [-0.0588,  0.8894,  0.2787,  0.0000,  0.0000,  0.4277, -0.9496, -0.2667],\n","        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],\n","        [ 0.0000, -0.0452,  0.0492, -0.2121, -0.7518,  0.3294, -0.7541, -0.9667],\n","        [-0.8824, -0.0352,  0.0492, -0.4545, -0.7943, -0.0104, -0.8198,  0.0000],\n","        [ 0.2941,  0.3869,  0.2131, -0.4747, -0.6596,  0.0760, -0.5909, -0.0333],\n","        [-0.8824,  0.8090,  0.0000,  0.0000,  0.0000,  0.2906, -0.8258, -0.3333],\n","        [-0.8824, -0.0955,  0.1148, -0.8384,  0.0000, -0.2697, -0.0948, -0.5000],\n","        [-0.8824, -0.0251,  0.1475, -0.1919,  0.0000,  0.1356, -0.8804, -0.7000],\n","        [-0.6471,  0.5879,  0.1475, -0.3939, -0.2246,  0.0581, -0.7728, -0.5333],\n","        [-0.8824,  0.6482,  0.3443, -0.1313, -0.8416, -0.0224, -0.7754, -0.0333],\n","        [-0.6471,  0.5879,  0.0492, -0.7374, -0.0851, -0.0700, -0.8147, -0.9000],\n","        [-0.1765,  0.5980,  0.0492,  0.0000,  0.0000, -0.1833, -0.8155, -0.3667],\n","        [-0.8824,  0.3568, -0.1148,  0.0000,  0.0000, -0.2042, -0.4799,  0.3667],\n","        [ 0.0000,  0.3970,  0.0164, -0.6566, -0.5035, -0.3413, -0.8898,  0.0000],\n","        [-0.6471,  0.2663,  0.4426, -0.1717, -0.4444,  0.1714, -0.4654, -0.8000],\n","        [-0.8824, -0.0251,  0.1475, -0.6970,  0.0000, -0.4575, -0.9411,  0.0000],\n","        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667],\n","        [-0.4118, -0.0352,  0.2131, -0.6364, -0.8416,  0.0015, -0.2152, -0.2667],\n","        [ 0.0588, -0.2764,  0.2787, -0.4949,  0.0000, -0.0581, -0.8275, -0.4333],\n","        [-0.4118,  0.1558,  0.6066,  0.0000,  0.0000,  0.5768, -0.8881, -0.7667],\n","        [-0.1765,  0.7889,  0.3770,  0.0000,  0.0000,  0.1893, -0.7839, -0.3333],\n","        [-0.7647,  0.2060,  0.2459, -0.2525, -0.7518,  0.1833, -0.8830, -0.7333],\n","        [ 0.0588,  0.5678,  0.4098, -0.4343, -0.6336,  0.0224, -0.0512, -0.3000],\n","        [-0.4118,  0.8995,  0.0492, -0.3333, -0.2317, -0.0700, -0.5687, -0.7333]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.]])\n","Epoch: 11 | Inputs tensor([[-0.4118,  0.4372,  0.2787,  0.0000,  0.0000,  0.3413, -0.9044, -0.1333],\n","        [ 0.0000,  0.5176,  0.4754, -0.0707,  0.0000,  0.2548, -0.7498,  0.0000],\n","        [-0.7647,  0.2764, -0.2459, -0.5758, -0.2080,  0.0253, -0.9163, -0.9667],\n","        [ 0.0588,  0.3467,  0.2131, -0.3333, -0.8582, -0.2280, -0.6738,  1.0000],\n","        [-0.7647, -0.2563,  0.0000,  0.0000,  0.0000,  0.0000, -0.9795, -0.9667],\n","        [ 0.4118, -0.1156,  0.2131, -0.1919, -0.8723,  0.0522, -0.7438, -0.1000],\n","        [-0.8824,  0.4372,  0.3770, -0.5354, -0.2671,  0.2638, -0.1477, -0.9667],\n","        [-0.0588, -0.3467,  0.1803, -0.5354,  0.0000, -0.0462, -0.5542, -0.3000],\n","        [-0.0588, -0.0050,  0.3770,  0.0000,  0.0000,  0.0551, -0.7353, -0.0333],\n","        [-0.7647, -0.1558, -0.1803, -0.5354, -0.8203, -0.0939, -0.2400,  0.0000],\n","        [-0.8824,  0.3869,  0.3443,  0.0000,  0.0000,  0.1952, -0.8651, -0.7667],\n","        [-0.8824, -0.2060, -0.0164, -0.1515, -0.8865,  0.2966, -0.4876, -0.9333],\n","        [-0.7647, -0.1759, -0.1475, -0.5556, -0.7281, -0.1505,  0.3843, -0.8667],\n","        [-0.0588, -0.1558,  0.2131, -0.3737,  0.0000,  0.1416, -0.6763, -0.4000],\n","        [-0.1765,  0.5075,  0.2787, -0.4141, -0.7021,  0.0492, -0.4757,  0.1000],\n","        [ 0.0000,  0.0553,  0.3770,  0.0000,  0.0000, -0.1684, -0.4338,  0.3667],\n","        [ 0.0588,  0.6482,  0.3770, -0.5758,  0.0000, -0.0820, -0.3570, -0.6333],\n","        [-0.8824,  0.0854, -0.0164, -0.0707, -0.5792,  0.0581, -0.7122, -0.9000],\n","        [ 0.0000, -0.1357,  0.1148, -0.3535,  0.0000,  0.0671, -0.8634, -0.8667],\n","        [-0.8824,  1.0000,  0.2459, -0.1313,  0.0000,  0.2787,  0.1238, -0.9667],\n","        [-0.8824, -0.0050, -0.0492, -0.7980,  0.0000, -0.2429, -0.5961,  0.0000],\n","        [-0.7647, -0.0955,  0.1148, -0.1515,  0.0000,  0.1386, -0.6371, -0.8000],\n","        [-0.8824,  0.2563, -0.1803, -0.1919, -0.6052, -0.0075, -0.2451, -0.7667],\n","        [ 0.0000,  0.1457,  0.3115, -0.3131, -0.3262,  0.3174, -0.9240, -0.8000],\n","        [-0.5294,  0.4673,  0.3934, -0.4545, -0.7636, -0.1386, -0.9052, -0.8000],\n","        [ 0.0000, -0.4271, -0.0164,  0.0000,  0.0000, -0.3532, -0.4389,  0.5333],\n","        [-0.6471,  0.0754,  0.0164, -0.7374, -0.8865, -0.3174, -0.4876, -0.9333],\n","        [-0.8824, -0.2060,  0.3115, -0.4949, -0.9125, -0.2429, -0.5687, -0.9667],\n","        [-0.6471,  0.7487, -0.0492, -0.5556, -0.5414, -0.0194, -0.5602, -0.5000],\n","        [ 0.0000, -0.0653, -0.0164,  0.0000,  0.0000,  0.0522, -0.8420, -0.8667],\n","        [ 0.0588,  0.5477,  0.2787, -0.3939, -0.7636, -0.0790, -0.9266, -0.2000],\n","        [-0.1765,  0.5980,  0.0820,  0.0000,  0.0000, -0.0939, -0.7395, -0.5000]]) | Labels tensor([[1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","Epoch: 12 | Inputs tensor([[-0.5294,  0.4271,  0.4098,  0.0000,  0.0000,  0.3115, -0.5158, -0.9667],\n","        [-0.5294,  0.4573,  0.3443, -0.6364,  0.0000, -0.0313, -0.8659,  0.6333],\n","        [-0.5294,  0.4874, -0.0164, -0.4545, -0.2482, -0.0790, -0.9385, -0.7333],\n","        [-0.6471,  0.1357, -0.1803, -0.7980, -0.7991, -0.1207, -0.5320, -0.8667],\n","        [-0.7647,  0.4673,  0.1475, -0.2323, -0.1489, -0.1654, -0.7788, -0.7333],\n","        [-0.8824, -0.0452,  0.0820, -0.7374, -0.9102, -0.4158, -0.7814, -0.8667],\n","        [ 0.1765,  0.6181,  0.1148, -0.5354, -0.6879, -0.2399, -0.7882, -0.1333],\n","        [-0.2941,  0.0452,  0.2131, -0.6364, -0.6312, -0.1088, -0.4500, -0.3333],\n","        [-0.2941, -0.0854,  0.0000,  0.0000,  0.0000, -0.1118, -0.6388, -0.6667],\n","        [-0.4118,  0.0452,  0.2131,  0.0000,  0.0000, -0.1416, -0.9360, -0.1000],\n","        [-0.8824,  0.3970, -0.2459, -0.6162, -0.8038, -0.1446, -0.5081, -0.9667],\n","        [ 0.0588,  0.1256,  0.3443, -0.3535, -0.5863,  0.0194, -0.8446, -0.5000],\n","        [-0.8824, -0.0754,  0.0164, -0.4949, -0.9031, -0.4188, -0.6550, -0.8667],\n","        [ 0.5294,  0.2663,  0.4754,  0.0000,  0.0000,  0.2936, -0.5687, -0.3000],\n","        [-0.4118,  0.6683,  0.2459,  0.0000,  0.0000,  0.3621, -0.7763, -0.8000],\n","        [ 0.0000,  0.0553,  0.1148, -0.5556,  0.0000, -0.4039, -0.8651, -0.9667],\n","        [ 0.1765,  0.1156,  0.1475, -0.4545,  0.0000, -0.1803, -0.9462, -0.3667],\n","        [-0.7647,  0.0151, -0.0492, -0.6566, -0.3735, -0.2787, -0.5423, -0.9333],\n","        [-0.1765, -0.1859,  0.2787, -0.1919, -0.8865,  0.3920, -0.8437, -0.3000],\n","        [ 0.0588, -0.0854,  0.1148,  0.0000,  0.0000, -0.2787, -0.8958,  0.2333],\n","        [-0.6471, -0.1658, -0.0492, -0.3737, -0.9574,  0.0224, -0.7797, -0.8667],\n","        [-0.0588,  0.9497,  0.3115,  0.0000,  0.0000, -0.2221, -0.5961,  0.5333],\n","        [-0.0588,  0.0754,  0.3115,  0.0000,  0.0000, -0.2668, -0.3356, -0.5667],\n","        [-0.2941,  0.5477,  0.2787, -0.1717, -0.6690,  0.3741, -0.5790, -0.8000],\n","        [-0.8824,  0.2864,  0.3443, -0.6566, -0.5674, -0.1803, -0.9684, -0.9667],\n","        [ 0.0000,  0.3769,  0.1148, -0.7172, -0.6501, -0.2608, -0.9445,  0.0000],\n","        [-0.0588,  0.5578,  0.0164, -0.4747,  0.1702,  0.0134, -0.6029, -0.1667],\n","        [-0.7647,  0.0050,  0.0492, -0.5354,  0.0000, -0.1148, -0.7523,  0.0000],\n","        [-0.1765,  0.2462,  0.1475, -0.3333, -0.4917, -0.2399, -0.9291, -0.4667],\n","        [-0.6471,  0.2261,  0.2787,  0.0000,  0.0000, -0.3145, -0.8497, -0.3667],\n","        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],\n","        [-0.8824,  0.3970,  0.0164, -0.1717,  0.1348,  0.2131, -0.6089,  0.0000]]) | Labels tensor([[0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 13 | Inputs tensor([[ 0.1765,  0.2261,  0.1148,  0.0000,  0.0000, -0.0700, -0.8463, -0.3333],\n","        [-0.7647,  0.0050,  0.0820, -0.5960, -0.7872, -0.0194, -0.3262, -0.7667],\n","        [-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],\n","        [-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],\n","        [-0.8824,  0.0754,  0.1148, -0.6162,  0.0000, -0.2101, -0.9257, -0.9000],\n","        [-0.2941,  0.0352,  0.1803, -0.3535, -0.5508,  0.1237, -0.7899,  0.1333],\n","        [-0.7647, -0.1658,  0.0820, -0.5354, -0.8818, -0.0402, -0.6422, -0.9667],\n","        [-0.8824,  0.1658,  0.1475, -0.4343,  0.0000, -0.1833, -0.8924,  0.0000],\n","        [ 0.0000,  0.4774,  0.3934,  0.0909,  0.0000,  0.2757, -0.7464, -0.9000],\n","        [-0.7647,  0.3970,  0.2295,  0.0000,  0.0000, -0.2370, -0.9240, -0.7333],\n","        [-0.1765,  0.2965,  0.1148, -0.0101, -0.7045,  0.1475, -0.6917, -0.2667],\n","        [-0.6471,  0.1156, -0.0820, -0.2121,  0.0000, -0.1028, -0.5909, -0.7000],\n","        [-0.1765,  0.8492,  0.3770, -0.3333,  0.0000,  0.0581, -0.7635, -0.3333],\n","        [-0.8824, -0.1859,  0.1803, -0.6364, -0.9054, -0.2072, -0.8249, -0.9000],\n","        [-0.0588,  0.2462,  0.2459, -0.5152,  0.4184, -0.1446, -0.4799,  0.0333],\n","        [-0.8824,  0.5779,  0.1803, -0.5758, -0.6028, -0.2370, -0.9616, -0.9000],\n","        [-0.2941,  0.2362,  0.1803, -0.0909, -0.4563,  0.0015, -0.4406, -0.5667],\n","        [-0.6471,  0.0050,  0.1148, -0.5354, -0.8085, -0.0581, -0.2562, -0.7667],\n","        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0522, -0.9522, -0.7333],\n","        [-0.8824,  0.5377,  0.3443, -0.1515,  0.1466,  0.2101, -0.4799, -0.9333],\n","        [-0.7647,  0.0553, -0.0492, -0.1919, -0.7778,  0.0402, -0.8745, -0.8667],\n","        [ 0.0000,  0.8995,  0.7049, -0.4949,  0.0000,  0.0224, -0.6951, -0.3333],\n","        [-0.7647,  0.2261,  0.2459, -0.4545, -0.5272,  0.0700, -0.6541, -0.8333],\n","        [ 0.0000,  0.0754, -0.0164, -0.4949,  0.0000, -0.2131, -0.9530, -0.9333],\n","        [ 0.0000,  0.2161,  0.0820, -0.3939, -0.6099,  0.0224, -0.8933, -0.6000],\n","        [-0.6471,  0.1256,  0.2131, -0.3939,  0.0000, -0.0581, -0.8984, -0.8667],\n","        [ 0.1765, -0.0754,  0.0164,  0.0000,  0.0000, -0.2280, -0.9240, -0.6667],\n","        [-0.8824,  0.2563,  0.1475, -0.5152, -0.7400, -0.2757, -0.8779, -0.8667],\n","        [-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],\n","        [-0.4118,  0.1256,  0.0820,  0.0000,  0.0000,  0.1267, -0.8437, -0.3333],\n","        [-0.6471,  0.2462,  0.3115, -0.3333, -0.6927, -0.0104, -0.8061, -0.8333],\n","        [-0.4118,  0.1055,  0.1148,  0.0000,  0.0000, -0.2250, -0.8173, -0.7000]]) | Labels tensor([[1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.]])\n","Epoch: 14 | Inputs tensor([[-0.6471,  0.1658,  0.2131, -0.6970, -0.7518, -0.2161, -0.9752, -0.9000],\n","        [-0.7647,  0.0854,  0.0492,  0.0000,  0.0000, -0.0820, -0.9317,  0.0000],\n","        [-0.2941, -0.0050, -0.0164, -0.6162, -0.8723, -0.1982, -0.6422, -0.6333],\n","        [-0.5294,  0.3166,  0.1148, -0.5758, -0.6076, -0.0134, -0.9300, -0.7667],\n","        [ 0.0000,  0.8894,  0.3443, -0.7172, -0.5626, -0.0462, -0.4842, -0.9667],\n","        [-0.5294, -0.1558,  0.4754, -0.5354, -0.8676,  0.1773, -0.9308, -0.8667],\n","        [-0.8824,  0.6382,  0.1803,  0.0000,  0.0000,  0.1624, -0.0231, -0.6000],\n","        [-0.8824, -0.1256, -0.0164, -0.2525, -0.8227,  0.1088, -0.6319, -0.9667],\n","        [ 0.5294, -0.2362, -0.0164,  0.0000,  0.0000, -0.0224, -0.9129, -0.3333],\n","        [ 0.0000,  0.3166,  0.0000,  0.0000,  0.0000,  0.2876, -0.8360, -0.8333],\n","        [ 0.0000,  0.2362,  0.1803,  0.0000,  0.0000,  0.0820, -0.8463,  0.0333],\n","        [ 0.0588,  0.5678,  0.4098,  0.0000,  0.0000, -0.2608, -0.8702,  0.0667],\n","        [-0.7647,  0.4673,  0.2459, -0.2929, -0.5414,  0.1386, -0.7857, -0.7333],\n","        [-0.5294, -0.0955,  0.4426, -0.0505, -0.8723,  0.1237, -0.7575, -0.7333],\n","        [ 0.0000,  0.2663,  0.3770, -0.4141, -0.4917, -0.0849, -0.6225, -0.9000],\n","        [-0.7647,  0.0955,  0.5082,  0.0000,  0.0000,  0.2727, -0.3450,  0.1000],\n","        [-0.7647,  0.1055,  0.2131, -0.4141, -0.7045, -0.0343, -0.4705, -0.8000],\n","        [-0.7647,  0.0754,  0.2131, -0.3939, -0.7636,  0.0015, -0.7216, -0.9333],\n","        [-0.4118,  0.0854,  0.1803, -0.1313, -0.8227,  0.0760, -0.8420, -0.6000],\n","        [-0.2941,  0.0553,  0.3115, -0.4343,  0.0000, -0.0313, -0.3168, -0.8333],\n","        [-0.6471, -0.1256, -0.0164, -0.6364,  0.0000, -0.3502, -0.6874,  0.0000],\n","        [ 0.0000,  0.0754,  0.2459,  0.0000,  0.0000,  0.3502, -0.4808, -0.9000],\n","        [-0.8824,  0.2663, -0.0820, -0.4141, -0.6407, -0.1446, -0.3826,  0.0000],\n","        [ 0.0000, -0.1558,  0.0492, -0.5556, -0.8440,  0.0671, -0.6012,  0.0000],\n","        [-0.2941,  0.5477,  0.2131, -0.3535, -0.5437, -0.1267, -0.3501, -0.4000],\n","        [-0.7647, -0.0352,  0.1148, -0.7374, -0.8842, -0.3711, -0.5141, -0.8333],\n","        [-0.2941,  0.2563,  0.2459,  0.0000,  0.0000,  0.0075, -0.9633,  0.1000],\n","        [-0.5294,  0.4673,  0.5082,  0.0000,  0.0000, -0.0700, -0.6063,  0.3333],\n","        [ 0.0000,  0.0452,  0.2459,  0.0000,  0.0000, -0.4516, -0.5696, -0.8000],\n","        [ 0.0000,  0.1759,  0.0000,  0.0000,  0.0000,  0.0075, -0.2707, -0.2333],\n","        [-0.1765,  0.0653,  0.5082, -0.6364,  0.0000, -0.3234, -0.8659, -0.1000],\n","        [-0.5294,  0.2261,  0.1148,  0.0000,  0.0000,  0.0432, -0.7301, -0.7333]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 15 | Inputs tensor([[-0.6471,  0.6382,  0.1475, -0.6364, -0.7518, -0.0581, -0.8377, -0.7667],\n","        [-0.8824,  0.1156,  0.4098, -0.6162,  0.0000, -0.1028, -0.9445, -0.9333],\n","        [-0.0588,  0.5477,  0.2787, -0.3535,  0.0000, -0.0343, -0.6883, -0.2000],\n","        [-0.0588,  0.2060,  0.0000,  0.0000,  0.0000, -0.1058, -0.9103, -0.4333],\n","        [ 0.6471,  0.7588,  0.0164, -0.3939,  0.0000,  0.0015, -0.8856, -0.4333],\n","        [-0.8824, -0.0352,  1.0000,  0.0000,  0.0000, -0.3323, -0.8898, -0.8000],\n","        [-0.8824,  0.0000,  0.1148, -0.2929,  0.0000, -0.0462, -0.7344, -0.9667],\n","        [-0.2941,  0.1759,  0.5738,  0.0000,  0.0000, -0.1446, -0.9325, -0.7000],\n","        [-0.8824, -0.0452,  0.2131, -0.5758, -0.8274, -0.2280, -0.4919, -0.5000],\n","        [ 0.0000, -0.2663,  0.0000,  0.0000,  0.0000, -0.3711, -0.7746, -0.8667],\n","        [ 0.0588,  0.0251,  0.2459, -0.2525,  0.0000, -0.0194, -0.4987, -0.1667],\n","        [-0.0588,  0.7688,  0.4754, -0.3131, -0.2908,  0.0045, -0.6678,  0.2333],\n","        [-0.0588,  0.2663,  0.4426, -0.2727, -0.7447,  0.1475, -0.7686, -0.0667],\n","        [-0.6471,  0.3266,  0.3115,  0.0000,  0.0000,  0.0253, -0.7233, -0.2333],\n","        [ 0.0588, -0.4271,  0.3115, -0.2525,  0.0000, -0.0224, -0.9846, -0.3333],\n","        [-0.8824,  0.5176, -0.0164,  0.0000,  0.0000, -0.2221, -0.9137, -0.9667],\n","        [-0.5294, -0.0352, -0.0820, -0.6566, -0.8842, -0.3800, -0.7763, -0.8333],\n","        [-0.8824,  0.4673, -0.0820,  0.0000,  0.0000, -0.1148, -0.5850, -0.7333],\n","        [-0.8824,  0.4472,  0.3443, -0.1919,  0.0000,  0.2310, -0.5482, -0.7667],\n","        [ 0.0000,  0.0955,  0.4426, -0.3939,  0.0000, -0.0313, -0.3365, -0.4333],\n","        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],\n","        [-0.7647, -0.0151, -0.0164, -0.6566, -0.7163,  0.0343, -0.8975, -0.9667],\n","        [ 0.0000,  0.2563,  0.5738,  0.0000,  0.0000, -0.3294, -0.8429,  0.0000],\n","        [-0.6471,  0.7688,  0.4098, -0.4545, -0.6312, -0.0075, -0.0811,  0.0333],\n","        [-0.4118,  0.1658,  0.2131,  0.0000,  0.0000, -0.2370, -0.8950, -0.7000],\n","        [-0.4118, -0.2161, -0.2131,  0.0000,  0.0000,  0.0045, -0.5081, -0.8667],\n","        [-0.8824, -0.0251,  0.0820, -0.6970, -0.6690, -0.3085, -0.6507, -0.9667],\n","        [ 0.0000,  0.0151,  0.0492, -0.6566,  0.0000, -0.3741, -0.8514,  0.0000],\n","        [-0.5294,  0.7186,  0.1803,  0.0000,  0.0000,  0.2996, -0.6576, -0.8333],\n","        [-0.7647, -0.0050, -0.0164, -0.6566, -0.6217,  0.0909, -0.6798,  0.0000],\n","        [-0.4118,  0.3065,  0.3443,  0.0000,  0.0000,  0.1654, -0.2502, -0.4667],\n","        [-0.1765,  0.0653, -0.0164, -0.5152,  0.0000, -0.2101, -0.8138, -0.7333]]) | Labels tensor([[0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.]])\n","Epoch: 16 | Inputs tensor([[-0.8824,  0.0352,  0.3115, -0.7778, -0.8061, -0.4218, -0.6473, -0.9667],\n","        [ 0.0000, -0.0653, -0.0164, -0.4949, -0.7825, -0.1446, -0.6123, -0.9667],\n","        [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],\n","        [ 0.0000,  0.6281,  0.2459,  0.1313, -0.7636,  0.5857, -0.4184, -0.8667],\n","        [-0.1765, -0.0251,  0.2459, -0.3535, -0.7849,  0.2191, -0.3228, -0.6333],\n","        [ 0.5294,  0.5276,  0.4754, -0.3333, -0.9314, -0.2012, -0.4424, -0.2667],\n","        [ 0.1765,  0.2965,  0.2459, -0.4343, -0.7116,  0.0700, -0.8275, -0.4000],\n","        [-0.6471,  0.3065,  0.2787, -0.5354, -0.8132, -0.1535, -0.7908, -0.5667],\n","        [-0.7647,  0.2261,  0.1475, -0.4545,  0.0000,  0.0969, -0.7763, -0.8000],\n","        [-0.6471,  0.1156,  0.0164,  0.0000,  0.0000, -0.3264, -0.9453,  0.0000],\n","        [-0.8824,  0.1759, -0.0164, -0.5354, -0.7494,  0.0075, -0.6687, -0.8000],\n","        [-0.2941, -0.1256,  0.3115,  0.0000,  0.0000, -0.3085, -0.9949, -0.6333],\n","        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],\n","        [-0.2941,  0.0352,  0.0820,  0.0000,  0.0000, -0.2757, -0.8540, -0.7333],\n","        [-0.5294,  0.7387,  0.1475, -0.7172, -0.6028, -0.1148, -0.7583, -0.6000],\n","        [ 0.0588,  0.4573,  0.4426, -0.3131, -0.6099, -0.0969, -0.4082,  0.0667],\n","        [-0.7647,  0.1558,  0.0492, -0.5556,  0.0000, -0.0820, -0.7071,  0.0000],\n","        [-0.1765,  0.5276,  0.4426, -0.1111,  0.0000,  0.4903, -0.7788, -0.5000],\n","        [-0.8824, -0.1156, -0.5082, -0.1515, -0.7660,  0.6393, -0.6430, -0.8333],\n","        [-0.7647, -0.2864,  0.1475, -0.4545,  0.0000, -0.1654, -0.5662, -0.9667],\n","        [ 0.0000,  0.1156,  0.0656,  0.0000,  0.0000, -0.2668, -0.5030, -0.6667],\n","        [-0.8824,  0.0000,  0.2131, -0.5960, -0.9456, -0.1744, -0.8113,  0.0000],\n","        [ 0.0000,  0.3769,  0.3770, -0.4545,  0.0000, -0.1863, -0.8693,  0.2667],\n","        [-0.2941,  0.0955, -0.0164, -0.4545,  0.0000, -0.2548, -0.8907, -0.8000],\n","        [-0.8824, -0.1156,  0.0164, -0.5152, -0.8960, -0.1088, -0.7062, -0.9333],\n","        [ 0.0000,  0.0251,  0.0492, -0.0707, -0.8156,  0.2101, -0.6430,  0.0000],\n","        [-0.8824, -0.0653, -0.0820, -0.7778,  0.0000, -0.3294, -0.7105, -0.9667],\n","        [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],\n","        [-0.7647,  0.0553,  0.2295,  0.0000,  0.0000, -0.3055, -0.5884,  0.0667],\n","        [-0.7647,  0.3065,  0.5738,  0.0000,  0.0000, -0.3264, -0.8377,  0.0000],\n","        [-0.6471,  0.5879,  0.2459, -0.2727, -0.4208, -0.0581, -0.3399, -0.7667],\n","        [ 0.0000,  0.2764,  0.3115, -0.2525, -0.5035,  0.0820, -0.3800, -0.9333]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.]])\n","Epoch: 17 | Inputs tensor([[ 0.0588,  0.0653, -0.1475,  0.0000,  0.0000, -0.0700, -0.7421, -0.3000],\n","        [ 0.1765, -0.0553,  0.1803, -0.6364,  0.0000, -0.3115, -0.5585,  0.1667],\n","        [-0.8824,  0.7286,  0.1148, -0.0101,  0.3688,  0.2638, -0.4671, -0.7667],\n","        [ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],\n","        [-0.2941,  0.1960, -0.1803, -0.5556, -0.5839, -0.1922,  0.0589, -0.6000],\n","        [-0.0588,  0.2663,  0.2131, -0.2323, -0.8227, -0.2280, -0.9283, -0.4000],\n","        [-0.6471,  0.7186,  0.1803, -0.3333, -0.6809, -0.0075, -0.8967, -0.9000],\n","        [ 0.0000,  0.0754,  0.0164, -0.3939, -0.8251,  0.0909, -0.4202, -0.8667],\n","        [-0.4118, -0.0050,  0.2131, -0.4545,  0.0000, -0.1356, -0.8933, -0.6333],\n","        [-0.5294, -0.0251, -0.0164, -0.5354,  0.0000, -0.1595, -0.6883, -0.9667],\n","        [-0.7647,  0.2563, -0.0164, -0.5960, -0.6690,  0.0075, -0.9915, -0.6667],\n","        [-0.5294,  0.5477,  0.1803, -0.4141, -0.7021, -0.0671, -0.7780, -0.4667],\n","        [-0.8824, -0.0251,  0.1148, -0.5758,  0.0000, -0.1893, -0.1315, -0.9667],\n","        [ 0.0588,  0.1960,  0.3115, -0.2929,  0.0000, -0.1356, -0.8420, -0.7333],\n","        [-0.4118,  0.1558,  0.2459,  0.0000,  0.0000, -0.0700, -0.7737, -0.2333],\n","        [ 0.0000,  0.4673,  0.1475,  0.0000,  0.0000,  0.1297, -0.7814, -0.7667],\n","        [-0.4118,  0.2864,  0.3115,  0.0000,  0.0000,  0.0313, -0.9436, -0.2000],\n","        [-0.4118,  0.4774,  0.2295,  0.0000,  0.0000, -0.1088, -0.6960, -0.7667],\n","        [-0.8824, -0.0452, -0.0164, -0.6364, -0.8629, -0.2876, -0.8446, -0.9667],\n","        [-0.2941,  0.3467,  0.1475, -0.5354, -0.6927,  0.0551, -0.6038, -0.7333],\n","        [ 0.6471,  0.0050,  0.2787, -0.4949, -0.5650,  0.0909, -0.7148, -0.1667],\n","        [-0.8824,  0.1960,  0.4098, -0.2121, -0.4799,  0.3592, -0.3766, -0.7333],\n","        [ 0.0000, -0.0854,  0.3115,  0.0000,  0.0000, -0.0343, -0.5534, -0.8000],\n","        [-0.0588,  0.4372,  0.0820,  0.0000,  0.0000,  0.0402, -0.9564, -0.3333],\n","        [-0.5294,  0.4472, -0.0492, -0.4343, -0.6690, -0.1207, -0.8215, -0.4667],\n","        [-0.2941,  0.5176,  0.0164, -0.3737, -0.7163,  0.0581, -0.4757, -0.7667],\n","        [-0.2941, -0.0754,  0.5082,  0.0000,  0.0000, -0.4069, -0.9061, -0.7667],\n","        [-0.1765,  0.0251,  0.2131, -0.1919, -0.7518,  0.1088, -0.8924, -0.2000],\n","        [-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667],\n","        [-0.6471,  0.1357, -0.2787, -0.7374,  0.0000, -0.3323, -0.9471, -0.9667],\n","        [ 0.0000, -0.0452,  0.3115, -0.0909, -0.7825,  0.0879, -0.7848, -0.8333],\n","        [-0.1765,  0.9497,  0.1148, -0.4343,  0.0000,  0.0700, -0.4304, -0.3333]]) | Labels tensor([[1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","Epoch: 18 | Inputs tensor([[-0.0588,  0.0050,  0.2459,  0.0000,  0.0000,  0.1535, -0.9044, -0.3000],\n","        [-0.2941,  0.8392,  0.5410,  0.0000,  0.0000,  0.2161,  0.1810, -0.2000],\n","        [-0.6471,  0.2060,  0.1475, -0.3939, -0.6809,  0.2787, -0.6806, -0.7000],\n","        [-0.1765,  0.5075,  0.0820, -0.1515, -0.1915,  0.0343, -0.4535, -0.3000],\n","        [-0.2941, -0.1457,  0.2787,  0.0000,  0.0000, -0.0700, -0.7404, -0.3000],\n","        [ 0.0000,  0.0251, -0.1475,  0.0000,  0.0000, -0.2519,  0.0000,  0.0000],\n","        [-0.2941,  0.9598,  0.1475,  0.0000,  0.0000, -0.0790, -0.7865, -0.6667],\n","        [-0.8824,  0.2261,  0.0492, -0.3535, -0.6312,  0.0462, -0.4757, -0.7000],\n","        [-0.5294, -0.0854,  0.1475, -0.3535, -0.7920, -0.0134, -0.6857, -0.9667],\n","        [ 0.0588,  0.6583,  0.4426,  0.0000,  0.0000, -0.0939, -0.8087, -0.0667],\n","        [-0.7647,  0.9799,  0.1475, -0.0909,  0.2837, -0.0909, -0.9317,  0.0667],\n","        [-0.2941,  0.6683,  0.2131,  0.0000,  0.0000, -0.2072, -0.8070,  0.5000],\n","        [-0.6471,  0.4874,  0.0820, -0.4949,  0.0000, -0.0313, -0.8480, -0.9667],\n","        [-0.6471, -0.0050, -0.1148, -0.6162, -0.7967, -0.2370, -0.9351, -0.9000],\n","        [-0.4118, -0.1156,  0.2787, -0.3939,  0.0000, -0.1773, -0.8463, -0.4667],\n","        [-0.2941,  0.0000,  0.1148, -0.1717,  0.0000,  0.1624, -0.4458, -0.3333],\n","        [-0.6471,  0.6281, -0.1475, -0.2323,  0.0000,  0.1088, -0.5098, -0.9000],\n","        [ 0.0000,  0.3166,  0.4426,  0.0000,  0.0000, -0.0581, -0.4321, -0.6333],\n","        [-0.8824, -0.0854,  0.0492, -0.5152,  0.0000, -0.1297, -0.9026,  0.0000],\n","        [-0.2941,  0.1156,  0.0492, -0.2121,  0.0000,  0.0194, -0.8446, -0.9000],\n","        [-0.7647,  0.2864,  0.0492, -0.1515,  0.0000,  0.1922, -0.1264, -0.9000],\n","        [-0.0588,  0.2060,  0.4098,  0.0000,  0.0000, -0.1535, -0.8454, -0.9667],\n","        [-0.8824, -0.1055,  0.2459, -0.3131, -0.9125, -0.0700, -0.9026, -0.9333],\n","        [-0.5294,  0.2060,  0.1148,  0.0000,  0.0000, -0.1177, -0.4611, -0.5667],\n","        [ 0.0000,  0.3467, -0.0492, -0.5960, -0.3121, -0.2131, -0.7660,  0.0000],\n","        [-0.0588, -0.0452,  0.1803,  0.0000,  0.0000,  0.0969, -0.6524,  0.2000],\n","        [-0.7647,  0.1759,  0.4754, -0.6162, -0.8322, -0.2489, -0.7993,  0.0000],\n","        [-0.8824, -0.0854, -0.1148, -0.4949, -0.7636, -0.2489, -0.8668, -0.9333],\n","        [-0.7647,  0.2764, -0.0492, -0.5152, -0.3499, -0.1744,  0.2997, -0.8667],\n","        [-0.2941, -0.0754,  0.0164, -0.3535, -0.7021, -0.0462, -0.9940, -0.1667],\n","        [ 0.5294,  0.0653,  0.1803,  0.0909,  0.0000,  0.0909, -0.9146, -0.2000],\n","        [ 0.0000, -0.0050,  0.0000,  0.0000,  0.0000, -0.2548, -0.8506, -0.9667]]) | Labels tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 19 | Inputs tensor([[ 0.1765, -0.0955,  0.3934, -0.3535,  0.0000,  0.0402, -0.3621,  0.1667],\n","        [ 0.0000,  0.5276,  0.3443, -0.2121, -0.3570,  0.2370, -0.8360, -0.8000],\n","        [-0.5294,  0.5477,  0.0164, -0.3737, -0.3286, -0.0224, -0.8642, -0.9333],\n","        [ 0.1765,  0.1558,  0.6066,  0.0000,  0.0000, -0.2846, -0.1939, -0.5667],\n","        [-0.8824, -0.1558,  0.0492, -0.5354, -0.7281,  0.0999, -0.6644, -0.7667],\n","        [-0.8824, -0.1256,  0.1148, -0.3131, -0.8180,  0.1207, -0.7242, -0.9000],\n","        [-0.8824, -0.2261, -0.0820, -0.3939, -0.8676, -0.0075,  0.0017, -0.9000],\n","        [-0.7647,  0.7487,  0.4426, -0.2525, -0.7163,  0.3264, -0.5149, -0.9000],\n","        [-0.0588,  0.5176,  0.2787, -0.3535, -0.5035,  0.2787, -0.6260, -0.5000],\n","        [-0.5294,  0.4774,  0.2131, -0.4949, -0.3073,  0.0402, -0.7378, -0.7000],\n","        [ 0.4118,  0.0653,  0.3115,  0.0000,  0.0000, -0.2966, -0.9496, -0.2333],\n","        [ 0.0000,  0.8090,  0.4754, -0.4747, -0.7872,  0.0879, -0.7985, -0.5333],\n","        [-0.8824,  0.1960, -0.1148, -0.7374, -0.8818, -0.3353, -0.8915, -0.9000],\n","        [-0.8824,  0.1960,  0.4426, -0.1717, -0.5981,  0.3502, -0.6336, -0.8333],\n","        [-0.6471,  0.1156,  0.4754, -0.7576, -0.8156, -0.1535, -0.6439, -0.7333],\n","        [-0.6471,  0.7387,  0.3443, -0.0303,  0.0993,  0.1446,  0.7583, -0.8667],\n","        [-0.8824,  0.0653,  0.2459,  0.0000,  0.0000,  0.1177, -0.8984, -0.8333],\n","        [-0.4118,  0.0553,  0.1803, -0.4141, -0.2317,  0.0999, -0.9308, -0.7667],\n","        [-0.8824,  0.2663, -0.0164,  0.0000,  0.0000, -0.1028, -0.7686, -0.1333],\n","        [-0.5294,  0.2563,  0.1475, -0.6364, -0.7116, -0.1386, -0.0897, -0.2000],\n","        [-0.4118, -0.2261,  0.3443, -0.1717, -0.9007,  0.0671, -0.9334, -0.5333],\n","        [-0.2941,  0.2563,  0.2787, -0.3737,  0.0000, -0.1773, -0.5841, -0.0667],\n","        [ 0.0000,  0.3266,  0.2787,  0.0000,  0.0000, -0.0343, -0.7310,  0.0000],\n","        [-0.4118,  0.1156,  0.1803, -0.4343,  0.0000, -0.2876, -0.7190, -0.8000],\n","        [ 0.0000,  0.3769,  0.1475, -0.2323,  0.0000, -0.0104, -0.9214, -0.9667],\n","        [-0.8824,  0.3166,  0.0492, -0.7172, -0.0189, -0.2936, -0.7344,  0.0000],\n","        [ 0.5294,  0.5377,  0.4426, -0.2525, -0.6690,  0.2101, -0.0640, -0.4000],\n","        [-0.8824, -0.0653,  0.1475, -0.3737,  0.0000, -0.0939, -0.7976, -0.9333],\n","        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n","        [-0.7647, -0.1055,  0.4754, -0.3939,  0.0000, -0.0015, -0.8173, -0.3000],\n","        [ 0.4118,  0.4070,  0.3443, -0.1313, -0.2317,  0.1684, -0.6157,  0.2333],\n","        [-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000]]) | Labels tensor([[0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.]])\n","Epoch: 20 | Inputs tensor([[-0.1765,  0.1457,  0.0820,  0.0000,  0.0000, -0.0224, -0.8463, -0.3000],\n","        [-0.1765,  0.6181,  0.4098,  0.0000,  0.0000, -0.0939, -0.9257, -0.1333],\n","        [ 0.1765,  0.7990,  0.1475,  0.0000,  0.0000,  0.0462, -0.8958, -0.4667],\n","        [ 0.0000,  0.7387,  0.2787, -0.3535, -0.3735,  0.3860, -0.0769,  0.2333],\n","        [-0.6471, -0.1859,  0.4098, -0.6768, -0.8440, -0.1803, -0.8053, -0.9667],\n","        [-0.2941, -0.0653, -0.1803, -0.3939, -0.8487, -0.1446, -0.7626, -0.9333],\n","        [ 0.0000, -0.0553,  0.1475, -0.4545, -0.7281,  0.2966, -0.7703,  0.0000],\n","        [-0.8824,  0.0653,  0.1475, -0.4343, -0.6809,  0.0194, -0.9453, -0.9667],\n","        [ 0.1765,  0.2965,  0.0164, -0.2727,  0.0000,  0.2280, -0.6900, -0.4333],\n","        [ 0.0000,  0.0452,  0.0492, -0.5354, -0.7258, -0.1714, -0.6789, -0.9333],\n","        [-0.7647,  0.2462,  0.1148, -0.4343, -0.5154, -0.0194, -0.3194, -0.7000],\n","        [-0.7647, -0.2462,  0.0492, -0.5152, -0.8700, -0.1148, -0.7506, -0.6000],\n","        [-0.6471, -0.0352, -0.0820, -0.3131, -0.7281, -0.2638, -0.2605, -0.4000],\n","        [-0.1765,  0.8794, -0.1803, -0.3333, -0.0733,  0.0104, -0.3612, -0.5667],\n","        [-0.7647,  0.0854, -0.1475, -0.4747, -0.8511, -0.0313, -0.7950, -0.9667],\n","        [-0.1765,  0.4774,  0.2459,  0.0000,  0.0000,  0.1744, -0.8471, -0.2667],\n","        [-0.2941,  0.6583,  0.1148, -0.4747, -0.6028,  0.0015, -0.5278, -0.0667],\n","        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8437, -0.7000],\n","        [-0.5294, -0.0452,  0.0492,  0.0000,  0.0000, -0.0462, -0.9291, -0.6667],\n","        [ 0.0588,  0.4070,  0.5410,  0.0000,  0.0000, -0.0253, -0.4398, -0.2000],\n","        [ 0.0000,  0.3869, -0.0164, -0.2929, -0.6052,  0.0313, -0.6106,  0.0000],\n","        [ 0.0588,  0.1256,  0.3443, -0.5152,  0.0000, -0.1595,  0.0282, -0.0333],\n","        [ 0.0588,  0.5276,  0.2787, -0.3131, -0.5957,  0.0194, -0.3040, -0.6000],\n","        [-0.5294,  0.2965, -0.0164, -0.7576, -0.4539, -0.1803, -0.6166, -0.6667],\n","        [-0.8824,  0.2060,  0.3115, -0.0303, -0.5272,  0.1595, -0.0743, -0.3333],\n","        [-0.7647,  0.1457,  0.1148, -0.5556,  0.0000, -0.1446, -0.9880, -0.8667],\n","        [-0.4118,  0.5578,  0.3770, -0.1111,  0.2884,  0.1535, -0.5380, -0.5667],\n","        [-0.1765,  0.0754,  0.2131,  0.0000,  0.0000, -0.1177, -0.8497, -0.6667],\n","        [-0.8824,  0.1156,  0.5410,  0.0000,  0.0000, -0.0224, -0.8403, -0.2000],\n","        [-0.4118,  0.3970,  0.0492, -0.2929, -0.6690, -0.1475, -0.7156, -0.8333],\n","        [-0.4118,  0.1759,  0.5082,  0.0000,  0.0000,  0.0164, -0.7788, -0.4333],\n","        [ 0.0000,  0.8090,  0.0820, -0.2121,  0.0000,  0.2519,  0.5500, -0.8667]]) | Labels tensor([[0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.]])\n","Epoch: 21 | Inputs tensor([[-0.7647,  0.1256,  0.2295, -0.3535,  0.0000,  0.0641, -0.9402,  0.0000],\n","        [-0.4118,  0.6884,  0.0492,  0.0000,  0.0000, -0.0194, -0.9513, -0.3333],\n","        [ 0.0000, -0.1558,  0.3443, -0.3737, -0.7045,  0.1386, -0.8676, -0.9333],\n","        [ 0.0588,  0.4573,  0.3115, -0.0707, -0.6927,  0.1297, -0.5226, -0.3667],\n","        [-0.6471, -0.1558,  0.1803, -0.3535,  0.0000,  0.1088, -0.8386, -0.7667],\n","        [ 0.4118,  0.4070,  0.3934, -0.3333,  0.0000,  0.1148, -0.8582, -0.3333],\n","        [-0.8824,  0.9397, -0.1803, -0.6768, -0.1135, -0.2280, -0.5073, -0.9000],\n","        [-0.7647, -0.0754, -0.1475,  0.0000,  0.0000, -0.1028, -0.9462, -0.9667],\n","        [-0.5294,  0.2965,  0.4098, -0.5960, -0.3617,  0.0462, -0.8693, -0.9333],\n","        [-0.6471, -0.1960,  0.3443, -0.3737, -0.8345,  0.0194,  0.0367, -0.8000],\n","        [-0.6471, -0.3869,  0.3443, -0.4343,  0.0000,  0.0253, -0.8591, -0.1667],\n","        [-0.1765,  0.0050,  0.0000,  0.0000,  0.0000, -0.1058, -0.6533, -0.6333],\n","        [-0.6471,  0.3065,  0.0492,  0.0000,  0.0000, -0.3115, -0.7985, -0.9667],\n","        [-0.6471,  0.2965,  0.5082, -0.0101, -0.6336,  0.0849, -0.2400, -0.6333],\n","        [-0.4118, -0.5578,  0.0164,  0.0000,  0.0000, -0.2548, -0.5653, -0.5000],\n","        [-0.5294,  0.2864,  0.1475,  0.0000,  0.0000,  0.0224, -0.8079, -0.9000],\n","        [-0.7647,  0.1960,  0.0000,  0.0000,  0.0000, -0.4158, -0.3561,  0.7000],\n","        [ 0.0000,  0.0151,  0.0656, -0.4343,  0.0000, -0.2668, -0.8642, -0.9667],\n","        [-0.7647,  0.9799,  0.1475,  1.0000,  0.0000,  0.0343, -0.5756,  0.3667],\n","        [ 0.0000,  0.0251,  0.2295, -0.5354,  0.0000,  0.0000, -0.5781,  0.0000],\n","        [ 0.0000,  0.0251,  0.4098, -0.6566, -0.7518, -0.1267, -0.4731, -0.8000],\n","        [-0.8824, -0.1055, -0.6066, -0.6162, -0.9409, -0.1714, -0.5892,  0.0000],\n","        [ 0.0000,  0.6181, -0.1803,  0.0000,  0.0000, -0.3472, -0.8497,  0.4667],\n","        [ 0.0000,  0.0151,  0.2459,  0.0000,  0.0000,  0.0641, -0.8975, -0.8333],\n","        [-0.4118,  0.2261,  0.4098,  0.0000,  0.0000,  0.0343, -0.8190, -0.6000],\n","        [ 0.0588,  0.7186,  0.8033, -0.5152, -0.4326,  0.3532, -0.4509,  0.1000],\n","        [ 0.5294,  0.5879,  0.8689,  0.0000,  0.0000,  0.2608, -0.8471, -0.2333],\n","        [-0.8824,  0.2161,  0.2787, -0.2121, -0.8251,  0.1624, -0.8437, -0.7667],\n","        [-0.1765,  0.9598,  0.1475, -0.3333, -0.6572, -0.2519, -0.9274,  0.1333],\n","        [-0.7647, -0.0050,  0.1475, -0.6768, -0.8960, -0.3920, -0.8659, -0.8000],\n","        [ 0.0000, -0.0854,  0.1148, -0.3535, -0.5035,  0.1893, -0.7412, -0.8667],\n","        [ 0.5294,  0.0653,  0.1475,  0.0000,  0.0000,  0.0194, -0.8523,  0.0333]]) | Labels tensor([[1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch: 22 | Inputs tensor([[-0.7647,  0.3467,  0.1475,  0.0000,  0.0000, -0.1386, -0.6038, -0.9333],\n","        [-0.4118,  0.5879,  0.1475,  0.0000,  0.0000, -0.1118, -0.8898,  0.4000],\n","        [ 0.0000,  0.1759,  0.3115, -0.3737, -0.8747,  0.3472, -0.9906, -0.9000],\n","        [ 0.2941,  0.3869,  0.2459,  0.0000,  0.0000, -0.0104, -0.7079, -0.5333],\n","        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667],\n","        [-0.8824,  0.0955, -0.3770, -0.6364, -0.7163, -0.3115, -0.7190, -0.8333],\n","        [-0.0588,  0.0553,  0.6393, -0.2727,  0.0000,  0.2906, -0.8625, -0.2000],\n","        [-0.7647,  0.4472, -0.0492, -0.3333, -0.6809, -0.0581, -0.7062, -0.8667],\n","        [-0.8824,  0.0151, -0.1803, -0.6970, -0.9149, -0.2787, -0.6174, -0.8333],\n","        [-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n","        [-0.7647,  0.0854,  0.3115,  0.0000,  0.0000, -0.1952, -0.8454,  0.0333],\n","        [-0.0588,  0.6784,  0.7377, -0.0707, -0.4539,  0.1207, -0.9257, -0.2667],\n","        [-0.4118,  0.4472,  0.3443, -0.4747, -0.3262, -0.0462, -0.6806,  0.2333],\n","        [-0.1765,  0.3367,  0.4426, -0.6970, -0.6336, -0.0343, -0.8429, -0.4667],\n","        [-0.4118,  0.4774,  0.2787,  0.0000,  0.0000,  0.0045, -0.8804,  0.4667],\n","        [ 0.0000, -0.0151,  0.3443, -0.6970, -0.8014, -0.2489, -0.8113, -0.9667],\n","        [-0.2941,  0.9497,  0.2787,  0.0000,  0.0000, -0.2996, -0.9564,  0.2667],\n","        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000],\n","        [-0.8824,  0.4070,  0.2131, -0.4747, -0.5745, -0.2817, -0.3595, -0.9333],\n","        [-0.0588,  0.8191,  0.1148, -0.2727,  0.1702, -0.1028, -0.5414,  0.3000],\n","        [-0.5294,  0.5176,  0.4754, -0.2323,  0.0000, -0.1148, -0.8155, -0.5000],\n","        [-0.7647, -0.0653,  0.0492, -0.3535, -0.6217,  0.1326, -0.4910, -0.9333],\n","        [ 0.1765,  0.2261,  0.2787, -0.3737,  0.0000, -0.1773, -0.6294, -0.2000],\n","        [-0.6471,  0.1558,  0.0820, -0.2121, -0.6690,  0.1356, -0.9385, -0.7667],\n","        [-0.4118,  0.2161,  0.1803, -0.5354, -0.7352, -0.2191, -0.8574, -0.7000],\n","        [-0.7647, -0.1859,  0.1803, -0.6970, -0.8203, -0.1028, -0.5995, -0.8667],\n","        [-0.1765,  0.4271, -0.0164, -0.3333, -0.5508, -0.1416, -0.4799,  0.3333],\n","        [-0.4118, -0.1156,  0.0820, -0.5758, -0.9456, -0.2727, -0.7746, -0.7000],\n","        [ 0.1765,  0.3970,  0.3115,  0.0000,  0.0000, -0.1922,  0.1640,  0.2000],\n","        [-0.5294,  0.5879,  0.2787,  0.0000,  0.0000, -0.0194, -0.3809, -0.6667],\n","        [ 0.1765,  0.0151,  0.2459, -0.0303, -0.5745, -0.0194, -0.9206,  0.4000],\n","        [ 0.1765, -0.3166,  0.7377, -0.5354, -0.8842,  0.0581, -0.8232, -0.1333]]) | Labels tensor([[0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.]])\n","Epoch: 23 | Inputs tensor([[-0.8824, -0.1156,  0.2787, -0.4141, -0.8203, -0.0462, -0.7549, -0.7333],\n","        [ 0.0000,  0.4070,  0.0656, -0.4747, -0.6927,  0.2697, -0.6985, -0.9000],\n","        [-0.1765,  0.7990,  0.5574, -0.3737,  0.0000,  0.0194, -0.9266,  0.3000],\n","        [-0.2941,  0.1457,  0.4426,  0.0000,  0.0000, -0.1714, -0.8557,  0.5000],\n","        [ 0.0000, -0.0251,  0.0492, -0.2727, -0.7636,  0.0969, -0.5542, -0.8667],\n","        [ 0.0000,  0.7789, -0.0164, -0.4141,  0.1300,  0.0313, -0.1512,  0.0000],\n","        [-0.4118, -0.1457,  0.2131, -0.5556,  0.0000, -0.1356, -0.0213, -0.6333],\n","        [-0.7647, -0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8070,  0.0000],\n","        [ 0.2941,  0.2060,  0.3115, -0.2525, -0.6454,  0.2608, -0.3962, -0.1000],\n","        [ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],\n","        [-0.8824,  0.3065, -0.0164, -0.5354, -0.5981, -0.1475, -0.4757,  0.0000],\n","        [ 0.0000,  0.2965,  0.3115,  0.0000,  0.0000, -0.0700, -0.4663, -0.7333],\n","        [-0.6471,  0.2864,  0.1803, -0.4949, -0.5508, -0.0343, -0.5978, -0.8000],\n","        [ 0.0000,  0.2462,  0.1475, -0.5960,  0.0000, -0.1833, -0.8497, -0.5000],\n","        [-0.7647,  0.2161,  0.1475, -0.3535, -0.7754,  0.1654, -0.3100, -0.9333],\n","        [-0.8824,  0.4472,  0.3443, -0.0707, -0.5745,  0.3741, -0.7805, -0.1667],\n","        [ 0.0000,  0.0151,  0.0164,  0.0000,  0.0000, -0.3472, -0.7797, -0.8667],\n","        [-0.5294, -0.1658,  0.4098, -0.6162,  0.0000, -0.1267, -0.7959, -0.5667],\n","        [-0.5294,  0.2362,  0.0164,  0.0000,  0.0000, -0.0462, -0.8736, -0.5333],\n","        [-0.8824, -0.2060,  0.2295, -0.3939,  0.0000, -0.0462, -0.7284, -0.9667],\n","        [-0.4118,  0.0352,  0.7705, -0.2525,  0.0000,  0.1684, -0.8061,  0.4667],\n","        [-0.1765,  0.3367,  0.3770,  0.0000,  0.0000,  0.1982, -0.4722, -0.4667],\n","        [-0.2941,  0.1558, -0.0164, -0.2121,  0.0000,  0.0045, -0.8574, -0.3667]]) | Labels tensor([[1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.]])\n"]}]},{"cell_type":"code","metadata":{"id":"fnb6B79e0bRd","executionInfo":{"status":"ok","timestamp":1632757239333,"user_tz":-540,"elapsed":51,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["class Model(nn.Module):\n","\n","  def __init__(self):\n","    \"\"\"\n","    In the constructor we instantiate nn.Linear module\n","    \"\"\"\n","    super(Model, self).__init__()\n","    self.l1 = nn.Linear(8,6)\n","    self.l2 = nn.Linear(6,4)\n","    self.l3 = nn.Linear(4,1)\n","\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    \"\"\"\n","    In the forward function we accept a Variable of input data and we must return\n","    a Variable of output data. We can use Modules defined in the constructor as \n","    well as arbitary operators on Variables.\n","    \"\"\"\n","    out1 = self.sigmoid(self.l1(x))\n","    out2 = self.sigmoid(self.l2(out1))\n","    y_pred = self.sigmoid(self.l3(out2))\n","\n","    return y_pred\n","\n","# our model\n","model = Model()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBXQsVdK1CR8","executionInfo":{"status":"ok","timestamp":1632757253109,"user_tz":-540,"elapsed":13826,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"5fb8dd29-83d8-4e4e-863d-6baf06a94a22"},"source":["# Construct our loss function and an Optimizer. The call to model.parameters()\n","# in the SGD constructor will contaion the learnable parameters of the three\n","# nn.Linear modules which are members of the model.\n","\n","criterion = torch.nn.BCELoss(reduction='sum')\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","\n","# Training loop\n","for epoch in range(100):\n","  for i, data in enumerate(train_loader, 0): \n","    # get the inputs\n","    inputs, labels = data\n","\n","    # Forward pass: Compute predicted y by passing x to the model\n","    y_pred = model(inputs)\n","\n","    # Compute and print loss\n","    loss = criterion(y_pred, labels)\n","    print(f'Epoch {epoch+1} | Batch: {i+1} | Loss: {loss.item(): .4f}')\n","\n","    # Zero gradients, perform a backward pass, and update the weights\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 | Batch: 1 | Loss:  23.1261\n","Epoch 1 | Batch: 2 | Loss:  22.0861\n","Epoch 1 | Batch: 3 | Loss:  20.5000\n","Epoch 1 | Batch: 4 | Loss:  26.2923\n","Epoch 1 | Batch: 5 | Loss:  22.6727\n","Epoch 1 | Batch: 6 | Loss:  25.3414\n","Epoch 1 | Batch: 7 | Loss:  21.6732\n","Epoch 1 | Batch: 8 | Loss:  21.9570\n","Epoch 1 | Batch: 9 | Loss:  20.6208\n","Epoch 1 | Batch: 10 | Loss:  19.9732\n","Epoch 1 | Batch: 11 | Loss:  17.3877\n","Epoch 1 | Batch: 12 | Loss:  23.4270\n","Epoch 1 | Batch: 13 | Loss:  23.0617\n","Epoch 1 | Batch: 14 | Loss:  22.2084\n","Epoch 1 | Batch: 15 | Loss:  23.0267\n","Epoch 1 | Batch: 16 | Loss:  22.2321\n","Epoch 1 | Batch: 17 | Loss:  21.4538\n","Epoch 1 | Batch: 18 | Loss:  20.2430\n","Epoch 1 | Batch: 19 | Loss:  20.6934\n","Epoch 1 | Batch: 20 | Loss:  20.5848\n","Epoch 1 | Batch: 21 | Loss:  20.6077\n","Epoch 1 | Batch: 22 | Loss:  23.1805\n","Epoch 1 | Batch: 23 | Loss:  22.0665\n","Epoch 1 | Batch: 24 | Loss:  14.8799\n","Epoch 2 | Batch: 1 | Loss:  19.2869\n","Epoch 2 | Batch: 2 | Loss:  20.9371\n","Epoch 2 | Batch: 3 | Loss:  21.2088\n","Epoch 2 | Batch: 4 | Loss:  19.1922\n","Epoch 2 | Batch: 5 | Loss:  20.2393\n","Epoch 2 | Batch: 6 | Loss:  22.2209\n","Epoch 2 | Batch: 7 | Loss:  20.9753\n","Epoch 2 | Batch: 8 | Loss:  17.1623\n","Epoch 2 | Batch: 9 | Loss:  21.1138\n","Epoch 2 | Batch: 10 | Loss:  24.7225\n","Epoch 2 | Batch: 11 | Loss:  22.6263\n","Epoch 2 | Batch: 12 | Loss:  24.0876\n","Epoch 2 | Batch: 13 | Loss:  23.1789\n","Epoch 2 | Batch: 14 | Loss:  19.4656\n","Epoch 2 | Batch: 15 | Loss:  23.6651\n","Epoch 2 | Batch: 16 | Loss:  21.4824\n","Epoch 2 | Batch: 17 | Loss:  20.1128\n","Epoch 2 | Batch: 18 | Loss:  19.8639\n","Epoch 2 | Batch: 19 | Loss:  17.5126\n","Epoch 2 | Batch: 20 | Loss:  21.6866\n","Epoch 2 | Batch: 21 | Loss:  23.9525\n","Epoch 2 | Batch: 22 | Loss:  22.3890\n","Epoch 2 | Batch: 23 | Loss:  21.2613\n","Epoch 2 | Batch: 24 | Loss:  12.8556\n","Epoch 3 | Batch: 1 | Loss:  21.4307\n","Epoch 3 | Batch: 2 | Loss:  21.9545\n","Epoch 3 | Batch: 3 | Loss:  20.4858\n","Epoch 3 | Batch: 4 | Loss:  21.4213\n","Epoch 3 | Batch: 5 | Loss:  19.6072\n","Epoch 3 | Batch: 6 | Loss:  19.0134\n","Epoch 3 | Batch: 7 | Loss:  19.9429\n","Epoch 3 | Batch: 8 | Loss:  22.2612\n","Epoch 3 | Batch: 9 | Loss:  20.4487\n","Epoch 3 | Batch: 10 | Loss:  20.6552\n","Epoch 3 | Batch: 11 | Loss:  19.9338\n","Epoch 3 | Batch: 12 | Loss:  24.5638\n","Epoch 3 | Batch: 13 | Loss:  22.2953\n","Epoch 3 | Batch: 14 | Loss:  20.0660\n","Epoch 3 | Batch: 15 | Loss:  23.1534\n","Epoch 3 | Batch: 16 | Loss:  20.5808\n","Epoch 3 | Batch: 17 | Loss:  18.1162\n","Epoch 3 | Batch: 18 | Loss:  20.1004\n","Epoch 3 | Batch: 19 | Loss:  24.8247\n","Epoch 3 | Batch: 20 | Loss:  22.0892\n","Epoch 3 | Batch: 21 | Loss:  22.1641\n","Epoch 3 | Batch: 22 | Loss:  20.1079\n","Epoch 3 | Batch: 23 | Loss:  22.5415\n","Epoch 3 | Batch: 24 | Loss:  15.4436\n","Epoch 4 | Batch: 1 | Loss:  21.1488\n","Epoch 4 | Batch: 2 | Loss:  21.1539\n","Epoch 4 | Batch: 3 | Loss:  20.6554\n","Epoch 4 | Batch: 4 | Loss:  18.0241\n","Epoch 4 | Batch: 5 | Loss:  22.6049\n","Epoch 4 | Batch: 6 | Loss:  20.5909\n","Epoch 4 | Batch: 7 | Loss:  24.4011\n","Epoch 4 | Batch: 8 | Loss:  22.7681\n","Epoch 4 | Batch: 9 | Loss:  16.9671\n","Epoch 4 | Batch: 10 | Loss:  24.4670\n","Epoch 4 | Batch: 11 | Loss:  18.5747\n","Epoch 4 | Batch: 12 | Loss:  20.2543\n","Epoch 4 | Batch: 13 | Loss:  24.1539\n","Epoch 4 | Batch: 14 | Loss:  20.4136\n","Epoch 4 | Batch: 15 | Loss:  21.6355\n","Epoch 4 | Batch: 16 | Loss:  23.6689\n","Epoch 4 | Batch: 17 | Loss:  21.7502\n","Epoch 4 | Batch: 18 | Loss:  22.3353\n","Epoch 4 | Batch: 19 | Loss:  20.8124\n","Epoch 4 | Batch: 20 | Loss:  26.2992\n","Epoch 4 | Batch: 21 | Loss:  21.3413\n","Epoch 4 | Batch: 22 | Loss:  19.6980\n","Epoch 4 | Batch: 23 | Loss:  19.8757\n","Epoch 4 | Batch: 24 | Loss:  14.9196\n","Epoch 5 | Batch: 1 | Loss:  17.7804\n","Epoch 5 | Batch: 2 | Loss:  24.7520\n","Epoch 5 | Batch: 3 | Loss:  20.0135\n","Epoch 5 | Batch: 4 | Loss:  21.5548\n","Epoch 5 | Batch: 5 | Loss:  19.4566\n","Epoch 5 | Batch: 6 | Loss:  21.6055\n","Epoch 5 | Batch: 7 | Loss:  18.2987\n","Epoch 5 | Batch: 8 | Loss:  26.7635\n","Epoch 5 | Batch: 9 | Loss:  21.5916\n","Epoch 5 | Batch: 10 | Loss:  21.1739\n","Epoch 5 | Batch: 11 | Loss:  20.1417\n","Epoch 5 | Batch: 12 | Loss:  21.3277\n","Epoch 5 | Batch: 13 | Loss:  20.6148\n","Epoch 5 | Batch: 14 | Loss:  21.8303\n","Epoch 5 | Batch: 15 | Loss:  23.6650\n","Epoch 5 | Batch: 16 | Loss:  22.3014\n","Epoch 5 | Batch: 17 | Loss:  22.2508\n","Epoch 5 | Batch: 18 | Loss:  21.7246\n","Epoch 5 | Batch: 19 | Loss:  20.7882\n","Epoch 5 | Batch: 20 | Loss:  18.7253\n","Epoch 5 | Batch: 21 | Loss:  18.9987\n","Epoch 5 | Batch: 22 | Loss:  16.1220\n","Epoch 5 | Batch: 23 | Loss:  18.0653\n","Epoch 5 | Batch: 24 | Loss:  14.3770\n","Epoch 6 | Batch: 1 | Loss:  23.8602\n","Epoch 6 | Batch: 2 | Loss:  19.7479\n","Epoch 6 | Batch: 3 | Loss:  21.8272\n","Epoch 6 | Batch: 4 | Loss:  18.1894\n","Epoch 6 | Batch: 5 | Loss:  15.7454\n","Epoch 6 | Batch: 6 | Loss:  22.1554\n","Epoch 6 | Batch: 7 | Loss:  22.2872\n","Epoch 6 | Batch: 8 | Loss:  21.1154\n","Epoch 6 | Batch: 9 | Loss:  23.6470\n","Epoch 6 | Batch: 10 | Loss:  22.0575\n","Epoch 6 | Batch: 11 | Loss:  23.5052\n","Epoch 6 | Batch: 12 | Loss:  17.8824\n","Epoch 6 | Batch: 13 | Loss:  23.9906\n","Epoch 6 | Batch: 14 | Loss:  18.3858\n","Epoch 6 | Batch: 15 | Loss:  21.2108\n","Epoch 6 | Batch: 16 | Loss:  19.8426\n","Epoch 6 | Batch: 17 | Loss:  22.2135\n","Epoch 6 | Batch: 18 | Loss:  20.6895\n","Epoch 6 | Batch: 19 | Loss:  19.3245\n","Epoch 6 | Batch: 20 | Loss:  22.4280\n","Epoch 6 | Batch: 21 | Loss:  19.7333\n","Epoch 6 | Batch: 22 | Loss:  23.1955\n","Epoch 6 | Batch: 23 | Loss:  20.1675\n","Epoch 6 | Batch: 24 | Loss:  15.7602\n","Epoch 7 | Batch: 1 | Loss:  21.1445\n","Epoch 7 | Batch: 2 | Loss:  19.0148\n","Epoch 7 | Batch: 3 | Loss:  20.8611\n","Epoch 7 | Batch: 4 | Loss:  21.2499\n","Epoch 7 | Batch: 5 | Loss:  19.9891\n","Epoch 7 | Batch: 6 | Loss:  19.0521\n","Epoch 7 | Batch: 7 | Loss:  18.9101\n","Epoch 7 | Batch: 8 | Loss:  16.0998\n","Epoch 7 | Batch: 9 | Loss:  25.7812\n","Epoch 7 | Batch: 10 | Loss:  19.7474\n","Epoch 7 | Batch: 11 | Loss:  19.8080\n","Epoch 7 | Batch: 12 | Loss:  20.5580\n","Epoch 7 | Batch: 13 | Loss:  23.1638\n","Epoch 7 | Batch: 14 | Loss:  21.5135\n","Epoch 7 | Batch: 15 | Loss:  21.5496\n","Epoch 7 | Batch: 16 | Loss:  21.9589\n","Epoch 7 | Batch: 17 | Loss:  20.7692\n","Epoch 7 | Batch: 18 | Loss:  21.8593\n","Epoch 7 | Batch: 19 | Loss:  18.8248\n","Epoch 7 | Batch: 20 | Loss:  24.3497\n","Epoch 7 | Batch: 21 | Loss:  21.1335\n","Epoch 7 | Batch: 22 | Loss:  20.5912\n","Epoch 7 | Batch: 23 | Loss:  19.1830\n","Epoch 7 | Batch: 24 | Loss:  18.4535\n","Epoch 8 | Batch: 1 | Loss:  20.4226\n","Epoch 8 | Batch: 2 | Loss:  22.3269\n","Epoch 8 | Batch: 3 | Loss:  21.9353\n","Epoch 8 | Batch: 4 | Loss:  20.9107\n","Epoch 8 | Batch: 5 | Loss:  17.9171\n","Epoch 8 | Batch: 6 | Loss:  23.3801\n","Epoch 8 | Batch: 7 | Loss:  19.6613\n","Epoch 8 | Batch: 8 | Loss:  21.3055\n","Epoch 8 | Batch: 9 | Loss:  22.1033\n","Epoch 8 | Batch: 10 | Loss:  20.4780\n","Epoch 8 | Batch: 11 | Loss:  18.9714\n","Epoch 8 | Batch: 12 | Loss:  24.8687\n","Epoch 8 | Batch: 13 | Loss:  21.5177\n","Epoch 8 | Batch: 14 | Loss:  18.9632\n","Epoch 8 | Batch: 15 | Loss:  15.9440\n","Epoch 8 | Batch: 16 | Loss:  23.2386\n","Epoch 8 | Batch: 17 | Loss:  22.1346\n","Epoch 8 | Batch: 18 | Loss:  21.1151\n","Epoch 8 | Batch: 19 | Loss:  19.3070\n","Epoch 8 | Batch: 20 | Loss:  18.9062\n","Epoch 8 | Batch: 21 | Loss:  20.4875\n","Epoch 8 | Batch: 22 | Loss:  20.2709\n","Epoch 8 | Batch: 23 | Loss:  20.1165\n","Epoch 8 | Batch: 24 | Loss:  16.4965\n","Epoch 9 | Batch: 1 | Loss:  21.6892\n","Epoch 9 | Batch: 2 | Loss:  21.3990\n","Epoch 9 | Batch: 3 | Loss:  20.1794\n","Epoch 9 | Batch: 4 | Loss:  17.9138\n","Epoch 9 | Batch: 5 | Loss:  23.7478\n","Epoch 9 | Batch: 6 | Loss:  20.8834\n","Epoch 9 | Batch: 7 | Loss:  19.9372\n","Epoch 9 | Batch: 8 | Loss:  22.3522\n","Epoch 9 | Batch: 9 | Loss:  20.8018\n","Epoch 9 | Batch: 10 | Loss:  22.7051\n","Epoch 9 | Batch: 11 | Loss:  21.9910\n","Epoch 9 | Batch: 12 | Loss:  17.3681\n","Epoch 9 | Batch: 13 | Loss:  20.1174\n","Epoch 9 | Batch: 14 | Loss:  18.4042\n","Epoch 9 | Batch: 15 | Loss:  17.4429\n","Epoch 9 | Batch: 16 | Loss:  21.3150\n","Epoch 9 | Batch: 17 | Loss:  20.0095\n","Epoch 9 | Batch: 18 | Loss:  20.0885\n","Epoch 9 | Batch: 19 | Loss:  21.3127\n","Epoch 9 | Batch: 20 | Loss:  21.5740\n","Epoch 9 | Batch: 21 | Loss:  13.9111\n","Epoch 9 | Batch: 22 | Loss:  12.5545\n","Epoch 9 | Batch: 23 | Loss:  25.3683\n","Epoch 9 | Batch: 24 | Loss:  15.2686\n","Epoch 10 | Batch: 1 | Loss:  18.9435\n","Epoch 10 | Batch: 2 | Loss:  19.3009\n","Epoch 10 | Batch: 3 | Loss:  20.6719\n","Epoch 10 | Batch: 4 | Loss:  19.6193\n","Epoch 10 | Batch: 5 | Loss:  18.0920\n","Epoch 10 | Batch: 6 | Loss:  20.3174\n","Epoch 10 | Batch: 7 | Loss:  18.6193\n","Epoch 10 | Batch: 8 | Loss:  17.5021\n","Epoch 10 | Batch: 9 | Loss:  19.8656\n","Epoch 10 | Batch: 10 | Loss:  18.0547\n","Epoch 10 | Batch: 11 | Loss:  12.9964\n","Epoch 10 | Batch: 12 | Loss:  21.1740\n","Epoch 10 | Batch: 13 | Loss:  15.8901\n","Epoch 10 | Batch: 14 | Loss:  26.0391\n","Epoch 10 | Batch: 15 | Loss:  21.0822\n","Epoch 10 | Batch: 16 | Loss:  18.2216\n","Epoch 10 | Batch: 17 | Loss:  16.4228\n","Epoch 10 | Batch: 18 | Loss:  15.3920\n","Epoch 10 | Batch: 19 | Loss:  23.4426\n","Epoch 10 | Batch: 20 | Loss:  20.7664\n","Epoch 10 | Batch: 21 | Loss:  16.2580\n","Epoch 10 | Batch: 22 | Loss:  16.9254\n","Epoch 10 | Batch: 23 | Loss:  17.0280\n","Epoch 10 | Batch: 24 | Loss:  12.2961\n","Epoch 11 | Batch: 1 | Loss:  14.5318\n","Epoch 11 | Batch: 2 | Loss:  12.9105\n","Epoch 11 | Batch: 3 | Loss:  15.9191\n","Epoch 11 | Batch: 4 | Loss:  17.9198\n","Epoch 11 | Batch: 5 | Loss:  16.2396\n","Epoch 11 | Batch: 6 | Loss:  16.7844\n","Epoch 11 | Batch: 7 | Loss:  11.1687\n","Epoch 11 | Batch: 8 | Loss:  13.0414\n","Epoch 11 | Batch: 9 | Loss:  21.9603\n","Epoch 11 | Batch: 10 | Loss:  22.0572\n","Epoch 11 | Batch: 11 | Loss:  20.2706\n","Epoch 11 | Batch: 12 | Loss:  22.4840\n","Epoch 11 | Batch: 13 | Loss:  12.5355\n","Epoch 11 | Batch: 14 | Loss:  15.9891\n","Epoch 11 | Batch: 15 | Loss:  15.3781\n","Epoch 11 | Batch: 16 | Loss:  14.5530\n","Epoch 11 | Batch: 17 | Loss:  15.3681\n","Epoch 11 | Batch: 18 | Loss:  16.9636\n","Epoch 11 | Batch: 19 | Loss:  19.2949\n","Epoch 11 | Batch: 20 | Loss:  17.7789\n","Epoch 11 | Batch: 21 | Loss:  20.4922\n","Epoch 11 | Batch: 22 | Loss:  21.1312\n","Epoch 11 | Batch: 23 | Loss:  29.8293\n","Epoch 11 | Batch: 24 | Loss:  18.2580\n","Epoch 12 | Batch: 1 | Loss:  12.9689\n","Epoch 12 | Batch: 2 | Loss:  23.3204\n","Epoch 12 | Batch: 3 | Loss:  15.2847\n","Epoch 12 | Batch: 4 | Loss:  22.2365\n","Epoch 12 | Batch: 5 | Loss:  21.3370\n","Epoch 12 | Batch: 6 | Loss:  15.6703\n","Epoch 12 | Batch: 7 | Loss:  12.3452\n","Epoch 12 | Batch: 8 | Loss:  16.9239\n","Epoch 12 | Batch: 9 | Loss:  17.3453\n","Epoch 12 | Batch: 10 | Loss:  15.4189\n","Epoch 12 | Batch: 11 | Loss:  14.1754\n","Epoch 12 | Batch: 12 | Loss:  20.4855\n","Epoch 12 | Batch: 13 | Loss:  20.5483\n","Epoch 12 | Batch: 14 | Loss:  17.6866\n","Epoch 12 | Batch: 15 | Loss:  17.4226\n","Epoch 12 | Batch: 16 | Loss:  16.3721\n","Epoch 12 | Batch: 17 | Loss:  17.0568\n","Epoch 12 | Batch: 18 | Loss:  13.6839\n","Epoch 12 | Batch: 19 | Loss:  15.2253\n","Epoch 12 | Batch: 20 | Loss:  16.7524\n","Epoch 12 | Batch: 21 | Loss:  18.2926\n","Epoch 12 | Batch: 22 | Loss:  14.0004\n","Epoch 12 | Batch: 23 | Loss:  14.3115\n","Epoch 12 | Batch: 24 | Loss:  18.5483\n","Epoch 13 | Batch: 1 | Loss:  18.8371\n","Epoch 13 | Batch: 2 | Loss:  17.0833\n","Epoch 13 | Batch: 3 | Loss:  12.8065\n","Epoch 13 | Batch: 4 | Loss:  17.3335\n","Epoch 13 | Batch: 5 | Loss:  20.8526\n","Epoch 13 | Batch: 6 | Loss:  15.4328\n","Epoch 13 | Batch: 7 | Loss:  14.9135\n","Epoch 13 | Batch: 8 | Loss:  21.1223\n","Epoch 13 | Batch: 9 | Loss:  15.6890\n","Epoch 13 | Batch: 10 | Loss:  15.4795\n","Epoch 13 | Batch: 11 | Loss:  15.1225\n","Epoch 13 | Batch: 12 | Loss:  19.2330\n","Epoch 13 | Batch: 13 | Loss:  16.9548\n","Epoch 13 | Batch: 14 | Loss:  18.3843\n","Epoch 13 | Batch: 15 | Loss:  16.4862\n","Epoch 13 | Batch: 16 | Loss:  19.6745\n","Epoch 13 | Batch: 17 | Loss:  13.8935\n","Epoch 13 | Batch: 18 | Loss:  14.5159\n","Epoch 13 | Batch: 19 | Loss:  14.4773\n","Epoch 13 | Batch: 20 | Loss:  17.0018\n","Epoch 13 | Batch: 21 | Loss:  21.2357\n","Epoch 13 | Batch: 22 | Loss:  27.7590\n","Epoch 13 | Batch: 23 | Loss:  13.5207\n","Epoch 13 | Batch: 24 | Loss:  12.4180\n","Epoch 14 | Batch: 1 | Loss:  17.5936\n","Epoch 14 | Batch: 2 | Loss:  15.4450\n","Epoch 14 | Batch: 3 | Loss:  17.7760\n","Epoch 14 | Batch: 4 | Loss:  16.7883\n","Epoch 14 | Batch: 5 | Loss:  17.8760\n","Epoch 14 | Batch: 6 | Loss:  13.8234\n","Epoch 14 | Batch: 7 | Loss:  18.7436\n","Epoch 14 | Batch: 8 | Loss:  14.7050\n","Epoch 14 | Batch: 9 | Loss:  10.9087\n","Epoch 14 | Batch: 10 | Loss:  18.2870\n","Epoch 14 | Batch: 11 | Loss:  15.1029\n","Epoch 14 | Batch: 12 | Loss:  21.4140\n","Epoch 14 | Batch: 13 | Loss:  13.3857\n","Epoch 14 | Batch: 14 | Loss:  9.3312\n","Epoch 14 | Batch: 15 | Loss:  20.3601\n","Epoch 14 | Batch: 16 | Loss:  19.4367\n","Epoch 14 | Batch: 17 | Loss:  18.0742\n","Epoch 14 | Batch: 18 | Loss:  16.3465\n","Epoch 14 | Batch: 19 | Loss:  13.2307\n","Epoch 14 | Batch: 20 | Loss:  11.5471\n","Epoch 14 | Batch: 21 | Loss:  15.6573\n","Epoch 14 | Batch: 22 | Loss:  16.9789\n","Epoch 14 | Batch: 23 | Loss:  18.2777\n","Epoch 14 | Batch: 24 | Loss:  17.1856\n","Epoch 15 | Batch: 1 | Loss:  25.6846\n","Epoch 15 | Batch: 2 | Loss:  23.0504\n","Epoch 15 | Batch: 3 | Loss:  16.6896\n","Epoch 15 | Batch: 4 | Loss:  21.2696\n","Epoch 15 | Batch: 5 | Loss:  16.4340\n","Epoch 15 | Batch: 6 | Loss:  15.3794\n","Epoch 15 | Batch: 7 | Loss:  14.0120\n","Epoch 15 | Batch: 8 | Loss:  15.6605\n","Epoch 15 | Batch: 9 | Loss:  17.5723\n","Epoch 15 | Batch: 10 | Loss:  19.1747\n","Epoch 15 | Batch: 11 | Loss:  13.7161\n","Epoch 15 | Batch: 12 | Loss:  13.3052\n","Epoch 15 | Batch: 13 | Loss:  20.6923\n","Epoch 15 | Batch: 14 | Loss:  14.6117\n","Epoch 15 | Batch: 15 | Loss:  8.3200\n","Epoch 15 | Batch: 16 | Loss:  14.3990\n","Epoch 15 | Batch: 17 | Loss:  18.8734\n","Epoch 15 | Batch: 18 | Loss:  16.7943\n","Epoch 15 | Batch: 19 | Loss:  15.3448\n","Epoch 15 | Batch: 20 | Loss:  15.8118\n","Epoch 15 | Batch: 21 | Loss:  13.9345\n","Epoch 15 | Batch: 22 | Loss:  12.9956\n","Epoch 15 | Batch: 23 | Loss:  20.6850\n","Epoch 15 | Batch: 24 | Loss:  11.1424\n","Epoch 16 | Batch: 1 | Loss:  16.8173\n","Epoch 16 | Batch: 2 | Loss:  22.3349\n","Epoch 16 | Batch: 3 | Loss:  24.6596\n","Epoch 16 | Batch: 4 | Loss:  16.6682\n","Epoch 16 | Batch: 5 | Loss:  18.3755\n","Epoch 16 | Batch: 6 | Loss:  20.4125\n","Epoch 16 | Batch: 7 | Loss:  15.3540\n","Epoch 16 | Batch: 8 | Loss:  15.7245\n","Epoch 16 | Batch: 9 | Loss:  17.4184\n","Epoch 16 | Batch: 10 | Loss:  16.2926\n","Epoch 16 | Batch: 11 | Loss:  17.6539\n","Epoch 16 | Batch: 12 | Loss:  14.2756\n","Epoch 16 | Batch: 13 | Loss:  14.8583\n","Epoch 16 | Batch: 14 | Loss:  20.2282\n","Epoch 16 | Batch: 15 | Loss:  15.8059\n","Epoch 16 | Batch: 16 | Loss:  12.1846\n","Epoch 16 | Batch: 17 | Loss:  13.4785\n","Epoch 16 | Batch: 18 | Loss:  12.1624\n","Epoch 16 | Batch: 19 | Loss:  13.0437\n","Epoch 16 | Batch: 20 | Loss:  14.9624\n","Epoch 16 | Batch: 21 | Loss:  17.1028\n","Epoch 16 | Batch: 22 | Loss:  16.0885\n","Epoch 16 | Batch: 23 | Loss:  14.4515\n","Epoch 16 | Batch: 24 | Loss:  16.1057\n","Epoch 17 | Batch: 1 | Loss:  20.5296\n","Epoch 17 | Batch: 2 | Loss:  15.3619\n","Epoch 17 | Batch: 3 | Loss:  17.9090\n","Epoch 17 | Batch: 4 | Loss:  17.0267\n","Epoch 17 | Batch: 5 | Loss:  18.1066\n","Epoch 17 | Batch: 6 | Loss:  18.1477\n","Epoch 17 | Batch: 7 | Loss:  13.5276\n","Epoch 17 | Batch: 8 | Loss:  14.8258\n","Epoch 17 | Batch: 9 | Loss:  16.1856\n","Epoch 17 | Batch: 10 | Loss:  14.3140\n","Epoch 17 | Batch: 11 | Loss:  9.3429\n","Epoch 17 | Batch: 12 | Loss:  19.3062\n","Epoch 17 | Batch: 13 | Loss:  16.6895\n","Epoch 17 | Batch: 14 | Loss:  15.6053\n","Epoch 17 | Batch: 15 | Loss:  14.8723\n","Epoch 17 | Batch: 16 | Loss:  17.9794\n","Epoch 17 | Batch: 17 | Loss:  15.5965\n","Epoch 17 | Batch: 18 | Loss:  15.3210\n","Epoch 17 | Batch: 19 | Loss:  18.8459\n","Epoch 17 | Batch: 20 | Loss:  16.6487\n","Epoch 17 | Batch: 21 | Loss:  18.7867\n","Epoch 17 | Batch: 22 | Loss:  12.6558\n","Epoch 17 | Batch: 23 | Loss:  18.5932\n","Epoch 17 | Batch: 24 | Loss:  6.7682\n","Epoch 18 | Batch: 1 | Loss:  17.8769\n","Epoch 18 | Batch: 2 | Loss:  16.3659\n","Epoch 18 | Batch: 3 | Loss:  14.7224\n","Epoch 18 | Batch: 4 | Loss:  9.3592\n","Epoch 18 | Batch: 5 | Loss:  22.2701\n","Epoch 18 | Batch: 6 | Loss:  15.7790\n","Epoch 18 | Batch: 7 | Loss:  13.8945\n","Epoch 18 | Batch: 8 | Loss:  18.4262\n","Epoch 18 | Batch: 9 | Loss:  15.6242\n","Epoch 18 | Batch: 10 | Loss:  15.2800\n","Epoch 18 | Batch: 11 | Loss:  14.2678\n","Epoch 18 | Batch: 12 | Loss:  23.5676\n","Epoch 18 | Batch: 13 | Loss:  13.0422\n","Epoch 18 | Batch: 14 | Loss:  17.9902\n","Epoch 18 | Batch: 15 | Loss:  14.7753\n","Epoch 18 | Batch: 16 | Loss:  20.2677\n","Epoch 18 | Batch: 17 | Loss:  15.8160\n","Epoch 18 | Batch: 18 | Loss:  11.9987\n","Epoch 18 | Batch: 19 | Loss:  15.6570\n","Epoch 18 | Batch: 20 | Loss:  13.8396\n","Epoch 18 | Batch: 21 | Loss:  16.3622\n","Epoch 18 | Batch: 22 | Loss:  19.6432\n","Epoch 18 | Batch: 23 | Loss:  16.0071\n","Epoch 18 | Batch: 24 | Loss:  9.4106\n","Epoch 19 | Batch: 1 | Loss:  13.2023\n","Epoch 19 | Batch: 2 | Loss:  22.7605\n","Epoch 19 | Batch: 3 | Loss:  19.2516\n","Epoch 19 | Batch: 4 | Loss:  15.6627\n","Epoch 19 | Batch: 5 | Loss:  14.8412\n","Epoch 19 | Batch: 6 | Loss:  9.9242\n","Epoch 19 | Batch: 7 | Loss:  13.4837\n","Epoch 19 | Batch: 8 | Loss:  12.1894\n","Epoch 19 | Batch: 9 | Loss:  13.6104\n","Epoch 19 | Batch: 10 | Loss:  11.9861\n","Epoch 19 | Batch: 11 | Loss:  15.6150\n","Epoch 19 | Batch: 12 | Loss:  17.7917\n","Epoch 19 | Batch: 13 | Loss:  16.0020\n","Epoch 19 | Batch: 14 | Loss:  22.7759\n","Epoch 19 | Batch: 15 | Loss:  15.6629\n","Epoch 19 | Batch: 16 | Loss:  19.7134\n","Epoch 19 | Batch: 17 | Loss:  17.6119\n","Epoch 19 | Batch: 18 | Loss:  17.3725\n","Epoch 19 | Batch: 19 | Loss:  18.5425\n","Epoch 19 | Batch: 20 | Loss:  13.7010\n","Epoch 19 | Batch: 21 | Loss:  13.7359\n","Epoch 19 | Batch: 22 | Loss:  17.5315\n","Epoch 19 | Batch: 23 | Loss:  26.0409\n","Epoch 19 | Batch: 24 | Loss:  12.2477\n","Epoch 20 | Batch: 1 | Loss:  12.7103\n","Epoch 20 | Batch: 2 | Loss:  11.1537\n","Epoch 20 | Batch: 3 | Loss:  12.4481\n","Epoch 20 | Batch: 4 | Loss:  15.4191\n","Epoch 20 | Batch: 5 | Loss:  14.7999\n","Epoch 20 | Batch: 6 | Loss:  16.1275\n","Epoch 20 | Batch: 7 | Loss:  15.9530\n","Epoch 20 | Batch: 8 | Loss:  19.3337\n","Epoch 20 | Batch: 9 | Loss:  16.6279\n","Epoch 20 | Batch: 10 | Loss:  19.3475\n","Epoch 20 | Batch: 11 | Loss:  15.1520\n","Epoch 20 | Batch: 12 | Loss:  13.2921\n","Epoch 20 | Batch: 13 | Loss:  16.4782\n","Epoch 20 | Batch: 14 | Loss:  16.9439\n","Epoch 20 | Batch: 15 | Loss:  19.9921\n","Epoch 20 | Batch: 16 | Loss:  20.0624\n","Epoch 20 | Batch: 17 | Loss:  14.4971\n","Epoch 20 | Batch: 18 | Loss:  14.8509\n","Epoch 20 | Batch: 19 | Loss:  16.2337\n","Epoch 20 | Batch: 20 | Loss:  13.3073\n","Epoch 20 | Batch: 21 | Loss:  16.3605\n","Epoch 20 | Batch: 22 | Loss:  12.8909\n","Epoch 20 | Batch: 23 | Loss:  17.7685\n","Epoch 20 | Batch: 24 | Loss:  8.1777\n","Epoch 21 | Batch: 1 | Loss:  13.3374\n","Epoch 21 | Batch: 2 | Loss:  15.8736\n","Epoch 21 | Batch: 3 | Loss:  11.0088\n","Epoch 21 | Batch: 4 | Loss:  13.7685\n","Epoch 21 | Batch: 5 | Loss:  8.8873\n","Epoch 21 | Batch: 6 | Loss:  17.0732\n","Epoch 21 | Batch: 7 | Loss:  14.8305\n","Epoch 21 | Batch: 8 | Loss:  15.0188\n","Epoch 21 | Batch: 9 | Loss:  15.8713\n","Epoch 21 | Batch: 10 | Loss:  19.4040\n","Epoch 21 | Batch: 11 | Loss:  19.0537\n","Epoch 21 | Batch: 12 | Loss:  29.8412\n","Epoch 21 | Batch: 13 | Loss:  27.3740\n","Epoch 21 | Batch: 14 | Loss:  20.5402\n","Epoch 21 | Batch: 15 | Loss:  21.0729\n","Epoch 21 | Batch: 16 | Loss:  15.1660\n","Epoch 21 | Batch: 17 | Loss:  13.7735\n","Epoch 21 | Batch: 18 | Loss:  15.5599\n","Epoch 21 | Batch: 19 | Loss:  15.3863\n","Epoch 21 | Batch: 20 | Loss:  18.9697\n","Epoch 21 | Batch: 21 | Loss:  14.2602\n","Epoch 21 | Batch: 22 | Loss:  10.2485\n","Epoch 21 | Batch: 23 | Loss:  13.8708\n","Epoch 21 | Batch: 24 | Loss:  11.1221\n","Epoch 22 | Batch: 1 | Loss:  14.3637\n","Epoch 22 | Batch: 2 | Loss:  16.2901\n","Epoch 22 | Batch: 3 | Loss:  12.8072\n","Epoch 22 | Batch: 4 | Loss:  17.3981\n","Epoch 22 | Batch: 5 | Loss:  13.7849\n","Epoch 22 | Batch: 6 | Loss:  19.4556\n","Epoch 22 | Batch: 7 | Loss:  18.5356\n","Epoch 22 | Batch: 8 | Loss:  15.4904\n","Epoch 22 | Batch: 9 | Loss:  13.6653\n","Epoch 22 | Batch: 10 | Loss:  15.3628\n","Epoch 22 | Batch: 11 | Loss:  17.3200\n","Epoch 22 | Batch: 12 | Loss:  17.3600\n","Epoch 22 | Batch: 13 | Loss:  20.8865\n","Epoch 22 | Batch: 14 | Loss:  22.0334\n","Epoch 22 | Batch: 15 | Loss:  21.1280\n","Epoch 22 | Batch: 16 | Loss:  15.0758\n","Epoch 22 | Batch: 17 | Loss:  14.1327\n","Epoch 22 | Batch: 18 | Loss:  21.5043\n","Epoch 22 | Batch: 19 | Loss:  13.0670\n","Epoch 22 | Batch: 20 | Loss:  12.5639\n","Epoch 22 | Batch: 21 | Loss:  14.9636\n","Epoch 22 | Batch: 22 | Loss:  12.7196\n","Epoch 22 | Batch: 23 | Loss:  16.4092\n","Epoch 22 | Batch: 24 | Loss:  10.6467\n","Epoch 23 | Batch: 1 | Loss:  15.3139\n","Epoch 23 | Batch: 2 | Loss:  9.0578\n","Epoch 23 | Batch: 3 | Loss:  18.5047\n","Epoch 23 | Batch: 4 | Loss:  17.2681\n","Epoch 23 | Batch: 5 | Loss:  15.5900\n","Epoch 23 | Batch: 6 | Loss:  16.1208\n","Epoch 23 | Batch: 7 | Loss:  10.0567\n","Epoch 23 | Batch: 8 | Loss:  10.0370\n","Epoch 23 | Batch: 9 | Loss:  18.8159\n","Epoch 23 | Batch: 10 | Loss:  15.9570\n","Epoch 23 | Batch: 11 | Loss:  10.1175\n","Epoch 23 | Batch: 12 | Loss:  11.5075\n","Epoch 23 | Batch: 13 | Loss:  23.1493\n","Epoch 23 | Batch: 14 | Loss:  17.6315\n","Epoch 23 | Batch: 15 | Loss:  17.4438\n","Epoch 23 | Batch: 16 | Loss:  15.7863\n","Epoch 23 | Batch: 17 | Loss:  15.3352\n","Epoch 23 | Batch: 18 | Loss:  12.5473\n","Epoch 23 | Batch: 19 | Loss:  13.9964\n","Epoch 23 | Batch: 20 | Loss:  15.6293\n","Epoch 23 | Batch: 21 | Loss:  14.9905\n","Epoch 23 | Batch: 22 | Loss:  16.4420\n","Epoch 23 | Batch: 23 | Loss:  14.7710\n","Epoch 23 | Batch: 24 | Loss:  24.0694\n","Epoch 24 | Batch: 1 | Loss:  20.7165\n","Epoch 24 | Batch: 2 | Loss:  15.6241\n","Epoch 24 | Batch: 3 | Loss:  17.6943\n","Epoch 24 | Batch: 4 | Loss:  15.4508\n","Epoch 24 | Batch: 5 | Loss:  17.3455\n","Epoch 24 | Batch: 6 | Loss:  16.8884\n","Epoch 24 | Batch: 7 | Loss:  15.9844\n","Epoch 24 | Batch: 8 | Loss:  12.0271\n","Epoch 24 | Batch: 9 | Loss:  17.0584\n","Epoch 24 | Batch: 10 | Loss:  13.0955\n","Epoch 24 | Batch: 11 | Loss:  17.2963\n","Epoch 24 | Batch: 12 | Loss:  16.3508\n","Epoch 24 | Batch: 13 | Loss:  12.8613\n","Epoch 24 | Batch: 14 | Loss:  13.6193\n","Epoch 24 | Batch: 15 | Loss:  19.8366\n","Epoch 24 | Batch: 16 | Loss:  15.0667\n","Epoch 24 | Batch: 17 | Loss:  15.6968\n","Epoch 24 | Batch: 18 | Loss:  14.5200\n","Epoch 24 | Batch: 19 | Loss:  16.1380\n","Epoch 24 | Batch: 20 | Loss:  12.9956\n","Epoch 24 | Batch: 21 | Loss:  16.9425\n","Epoch 24 | Batch: 22 | Loss:  15.3137\n","Epoch 24 | Batch: 23 | Loss:  16.0359\n","Epoch 24 | Batch: 24 | Loss:  9.7787\n","Epoch 25 | Batch: 1 | Loss:  17.1940\n","Epoch 25 | Batch: 2 | Loss:  14.7429\n","Epoch 25 | Batch: 3 | Loss:  17.1159\n","Epoch 25 | Batch: 4 | Loss:  15.2841\n","Epoch 25 | Batch: 5 | Loss:  18.5110\n","Epoch 25 | Batch: 6 | Loss:  11.8412\n","Epoch 25 | Batch: 7 | Loss:  7.1575\n","Epoch 25 | Batch: 8 | Loss:  21.9207\n","Epoch 25 | Batch: 9 | Loss:  11.7215\n","Epoch 25 | Batch: 10 | Loss:  16.0366\n","Epoch 25 | Batch: 11 | Loss:  14.8428\n","Epoch 25 | Batch: 12 | Loss:  24.5790\n","Epoch 25 | Batch: 13 | Loss:  21.8644\n","Epoch 25 | Batch: 14 | Loss:  11.7051\n","Epoch 25 | Batch: 15 | Loss:  17.1934\n","Epoch 25 | Batch: 16 | Loss:  16.1910\n","Epoch 25 | Batch: 17 | Loss:  13.5604\n","Epoch 25 | Batch: 18 | Loss:  14.8003\n","Epoch 25 | Batch: 19 | Loss:  13.2027\n","Epoch 25 | Batch: 20 | Loss:  16.1286\n","Epoch 25 | Batch: 21 | Loss:  12.9802\n","Epoch 25 | Batch: 22 | Loss:  16.5768\n","Epoch 25 | Batch: 23 | Loss:  13.4800\n","Epoch 25 | Batch: 24 | Loss:  14.2537\n","Epoch 26 | Batch: 1 | Loss:  15.2435\n","Epoch 26 | Batch: 2 | Loss:  17.6140\n","Epoch 26 | Batch: 3 | Loss:  19.7268\n","Epoch 26 | Batch: 4 | Loss:  15.8871\n","Epoch 26 | Batch: 5 | Loss:  12.2967\n","Epoch 26 | Batch: 6 | Loss:  14.9816\n","Epoch 26 | Batch: 7 | Loss:  17.8694\n","Epoch 26 | Batch: 8 | Loss:  21.5518\n","Epoch 26 | Batch: 9 | Loss:  17.6375\n","Epoch 26 | Batch: 10 | Loss:  15.5780\n","Epoch 26 | Batch: 11 | Loss:  12.5733\n","Epoch 26 | Batch: 12 | Loss:  15.3760\n","Epoch 26 | Batch: 13 | Loss:  17.3670\n","Epoch 26 | Batch: 14 | Loss:  16.2068\n","Epoch 26 | Batch: 15 | Loss:  19.3894\n","Epoch 26 | Batch: 16 | Loss:  16.8955\n","Epoch 26 | Batch: 17 | Loss:  14.0091\n","Epoch 26 | Batch: 18 | Loss:  18.1129\n","Epoch 26 | Batch: 19 | Loss:  11.5397\n","Epoch 26 | Batch: 20 | Loss:  14.8560\n","Epoch 26 | Batch: 21 | Loss:  11.9272\n","Epoch 26 | Batch: 22 | Loss:  12.6344\n","Epoch 26 | Batch: 23 | Loss:  10.3828\n","Epoch 26 | Batch: 24 | Loss:  9.6888\n","Epoch 27 | Batch: 1 | Loss:  15.0644\n","Epoch 27 | Batch: 2 | Loss:  14.7411\n","Epoch 27 | Batch: 3 | Loss:  14.1526\n","Epoch 27 | Batch: 4 | Loss:  16.2893\n","Epoch 27 | Batch: 5 | Loss:  16.4008\n","Epoch 27 | Batch: 6 | Loss:  16.0189\n","Epoch 27 | Batch: 7 | Loss:  18.2265\n","Epoch 27 | Batch: 8 | Loss:  19.2910\n","Epoch 27 | Batch: 9 | Loss:  16.6911\n","Epoch 27 | Batch: 10 | Loss:  16.8130\n","Epoch 27 | Batch: 11 | Loss:  16.1656\n","Epoch 27 | Batch: 12 | Loss:  16.2967\n","Epoch 27 | Batch: 13 | Loss:  13.3201\n","Epoch 27 | Batch: 14 | Loss:  16.0758\n","Epoch 27 | Batch: 15 | Loss:  13.4249\n","Epoch 27 | Batch: 16 | Loss:  15.4417\n","Epoch 27 | Batch: 17 | Loss:  15.9093\n","Epoch 27 | Batch: 18 | Loss:  19.5969\n","Epoch 27 | Batch: 19 | Loss:  15.8748\n","Epoch 27 | Batch: 20 | Loss:  15.0558\n","Epoch 27 | Batch: 21 | Loss:  21.1803\n","Epoch 27 | Batch: 22 | Loss:  12.1992\n","Epoch 27 | Batch: 23 | Loss:  13.1518\n","Epoch 27 | Batch: 24 | Loss:  7.4643\n","Epoch 28 | Batch: 1 | Loss:  13.5701\n","Epoch 28 | Batch: 2 | Loss:  17.0109\n","Epoch 28 | Batch: 3 | Loss:  18.5033\n","Epoch 28 | Batch: 4 | Loss:  20.5811\n","Epoch 28 | Batch: 5 | Loss:  26.5899\n","Epoch 28 | Batch: 6 | Loss:  14.2264\n","Epoch 28 | Batch: 7 | Loss:  18.7958\n","Epoch 28 | Batch: 8 | Loss:  15.7365\n","Epoch 28 | Batch: 9 | Loss:  14.1233\n","Epoch 28 | Batch: 10 | Loss:  9.4256\n","Epoch 28 | Batch: 11 | Loss:  17.0344\n","Epoch 28 | Batch: 12 | Loss:  16.9532\n","Epoch 28 | Batch: 13 | Loss:  15.6940\n","Epoch 28 | Batch: 14 | Loss:  11.6991\n","Epoch 28 | Batch: 15 | Loss:  17.2262\n","Epoch 28 | Batch: 16 | Loss:  14.0032\n","Epoch 28 | Batch: 17 | Loss:  24.8270\n","Epoch 28 | Batch: 18 | Loss:  18.1734\n","Epoch 28 | Batch: 19 | Loss:  14.6910\n","Epoch 28 | Batch: 20 | Loss:  17.5498\n","Epoch 28 | Batch: 21 | Loss:  17.8304\n","Epoch 28 | Batch: 22 | Loss:  11.6512\n","Epoch 28 | Batch: 23 | Loss:  16.5182\n","Epoch 28 | Batch: 24 | Loss:  10.5388\n","Epoch 29 | Batch: 1 | Loss:  15.2914\n","Epoch 29 | Batch: 2 | Loss:  16.0469\n","Epoch 29 | Batch: 3 | Loss:  16.4388\n","Epoch 29 | Batch: 4 | Loss:  12.9340\n","Epoch 29 | Batch: 5 | Loss:  15.2517\n","Epoch 29 | Batch: 6 | Loss:  17.5774\n","Epoch 29 | Batch: 7 | Loss:  14.4230\n","Epoch 29 | Batch: 8 | Loss:  13.0495\n","Epoch 29 | Batch: 9 | Loss:  16.1828\n","Epoch 29 | Batch: 10 | Loss:  17.5724\n","Epoch 29 | Batch: 11 | Loss:  18.0786\n","Epoch 29 | Batch: 12 | Loss:  15.3647\n","Epoch 29 | Batch: 13 | Loss:  13.0346\n","Epoch 29 | Batch: 14 | Loss:  10.6094\n","Epoch 29 | Batch: 15 | Loss:  17.1923\n","Epoch 29 | Batch: 16 | Loss:  14.2252\n","Epoch 29 | Batch: 17 | Loss:  16.0666\n","Epoch 29 | Batch: 18 | Loss:  16.7095\n","Epoch 29 | Batch: 19 | Loss:  15.6807\n","Epoch 29 | Batch: 20 | Loss:  16.2268\n","Epoch 29 | Batch: 21 | Loss:  19.0108\n","Epoch 29 | Batch: 22 | Loss:  16.7524\n","Epoch 29 | Batch: 23 | Loss:  13.8466\n","Epoch 29 | Batch: 24 | Loss:  11.3282\n","Epoch 30 | Batch: 1 | Loss:  13.6704\n","Epoch 30 | Batch: 2 | Loss:  16.7250\n","Epoch 30 | Batch: 3 | Loss:  20.6618\n","Epoch 30 | Batch: 4 | Loss:  21.2537\n","Epoch 30 | Batch: 5 | Loss:  14.3723\n","Epoch 30 | Batch: 6 | Loss:  14.2786\n","Epoch 30 | Batch: 7 | Loss:  11.4119\n","Epoch 30 | Batch: 8 | Loss:  20.3913\n","Epoch 30 | Batch: 9 | Loss:  14.4469\n","Epoch 30 | Batch: 10 | Loss:  14.0506\n","Epoch 30 | Batch: 11 | Loss:  11.1454\n","Epoch 30 | Batch: 12 | Loss:  18.3968\n","Epoch 30 | Batch: 13 | Loss:  13.5707\n","Epoch 30 | Batch: 14 | Loss:  15.2632\n","Epoch 30 | Batch: 15 | Loss:  17.9417\n","Epoch 30 | Batch: 16 | Loss:  19.7583\n","Epoch 30 | Batch: 17 | Loss:  9.7186\n","Epoch 30 | Batch: 18 | Loss:  10.5166\n","Epoch 30 | Batch: 19 | Loss:  11.5832\n","Epoch 30 | Batch: 20 | Loss:  22.5506\n","Epoch 30 | Batch: 21 | Loss:  14.8369\n","Epoch 30 | Batch: 22 | Loss:  16.6246\n","Epoch 30 | Batch: 23 | Loss:  18.9104\n","Epoch 30 | Batch: 24 | Loss:  10.7497\n","Epoch 31 | Batch: 1 | Loss:  14.8984\n","Epoch 31 | Batch: 2 | Loss:  14.3918\n","Epoch 31 | Batch: 3 | Loss:  17.8958\n","Epoch 31 | Batch: 4 | Loss:  18.5432\n","Epoch 31 | Batch: 5 | Loss:  14.4232\n","Epoch 31 | Batch: 6 | Loss:  16.3915\n","Epoch 31 | Batch: 7 | Loss:  16.0062\n","Epoch 31 | Batch: 8 | Loss:  16.9773\n","Epoch 31 | Batch: 9 | Loss:  17.2896\n","Epoch 31 | Batch: 10 | Loss:  16.4203\n","Epoch 31 | Batch: 11 | Loss:  15.5853\n","Epoch 31 | Batch: 12 | Loss:  12.1124\n","Epoch 31 | Batch: 13 | Loss:  16.5172\n","Epoch 31 | Batch: 14 | Loss:  14.2048\n","Epoch 31 | Batch: 15 | Loss:  13.1291\n","Epoch 31 | Batch: 16 | Loss:  15.4573\n","Epoch 31 | Batch: 17 | Loss:  12.0116\n","Epoch 31 | Batch: 18 | Loss:  16.4847\n","Epoch 31 | Batch: 19 | Loss:  20.2406\n","Epoch 31 | Batch: 20 | Loss:  23.9999\n","Epoch 31 | Batch: 21 | Loss:  22.0561\n","Epoch 31 | Batch: 22 | Loss:  20.3551\n","Epoch 31 | Batch: 23 | Loss:  13.2363\n","Epoch 31 | Batch: 24 | Loss:  10.8052\n","Epoch 32 | Batch: 1 | Loss:  11.4399\n","Epoch 32 | Batch: 2 | Loss:  15.3974\n","Epoch 32 | Batch: 3 | Loss:  15.0874\n","Epoch 32 | Batch: 4 | Loss:  16.0446\n","Epoch 32 | Batch: 5 | Loss:  13.9575\n","Epoch 32 | Batch: 6 | Loss:  14.9117\n","Epoch 32 | Batch: 7 | Loss:  21.5119\n","Epoch 32 | Batch: 8 | Loss:  20.7803\n","Epoch 32 | Batch: 9 | Loss:  15.2248\n","Epoch 32 | Batch: 10 | Loss:  15.7318\n","Epoch 32 | Batch: 11 | Loss:  15.4923\n","Epoch 32 | Batch: 12 | Loss:  15.0220\n","Epoch 32 | Batch: 13 | Loss:  15.1840\n","Epoch 32 | Batch: 14 | Loss:  14.8533\n","Epoch 32 | Batch: 15 | Loss:  22.9372\n","Epoch 32 | Batch: 16 | Loss:  20.4075\n","Epoch 32 | Batch: 17 | Loss:  12.6810\n","Epoch 32 | Batch: 18 | Loss:  14.5991\n","Epoch 32 | Batch: 19 | Loss:  12.8887\n","Epoch 32 | Batch: 20 | Loss:  17.6817\n","Epoch 32 | Batch: 21 | Loss:  12.8963\n","Epoch 32 | Batch: 22 | Loss:  19.1993\n","Epoch 32 | Batch: 23 | Loss:  13.8978\n","Epoch 32 | Batch: 24 | Loss:  9.8884\n","Epoch 33 | Batch: 1 | Loss:  17.6668\n","Epoch 33 | Batch: 2 | Loss:  12.9170\n","Epoch 33 | Batch: 3 | Loss:  16.4853\n","Epoch 33 | Batch: 4 | Loss:  15.2604\n","Epoch 33 | Batch: 5 | Loss:  22.3745\n","Epoch 33 | Batch: 6 | Loss:  11.7570\n","Epoch 33 | Batch: 7 | Loss:  14.3355\n","Epoch 33 | Batch: 8 | Loss:  18.8349\n","Epoch 33 | Batch: 9 | Loss:  15.7990\n","Epoch 33 | Batch: 10 | Loss:  14.8607\n","Epoch 33 | Batch: 11 | Loss:  11.3942\n","Epoch 33 | Batch: 12 | Loss:  14.9310\n","Epoch 33 | Batch: 13 | Loss:  19.6415\n","Epoch 33 | Batch: 14 | Loss:  16.1758\n","Epoch 33 | Batch: 15 | Loss:  14.8741\n","Epoch 33 | Batch: 16 | Loss:  9.2792\n","Epoch 33 | Batch: 17 | Loss:  20.3041\n","Epoch 33 | Batch: 18 | Loss:  16.2306\n","Epoch 33 | Batch: 19 | Loss:  14.9412\n","Epoch 33 | Batch: 20 | Loss:  18.0413\n","Epoch 33 | Batch: 21 | Loss:  17.2989\n","Epoch 33 | Batch: 22 | Loss:  17.1091\n","Epoch 33 | Batch: 23 | Loss:  14.9520\n","Epoch 33 | Batch: 24 | Loss:  14.9910\n","Epoch 34 | Batch: 1 | Loss:  16.7969\n","Epoch 34 | Batch: 2 | Loss:  14.6391\n","Epoch 34 | Batch: 3 | Loss:  18.0207\n","Epoch 34 | Batch: 4 | Loss:  15.3994\n","Epoch 34 | Batch: 5 | Loss:  20.4071\n","Epoch 34 | Batch: 6 | Loss:  18.5915\n","Epoch 34 | Batch: 7 | Loss:  17.8066\n","Epoch 34 | Batch: 8 | Loss:  15.2523\n","Epoch 34 | Batch: 9 | Loss:  12.9830\n","Epoch 34 | Batch: 10 | Loss:  10.7995\n","Epoch 34 | Batch: 11 | Loss:  15.6527\n","Epoch 34 | Batch: 12 | Loss:  15.2999\n","Epoch 34 | Batch: 13 | Loss:  16.5671\n","Epoch 34 | Batch: 14 | Loss:  12.5605\n","Epoch 34 | Batch: 15 | Loss:  12.0720\n","Epoch 34 | Batch: 16 | Loss:  15.0121\n","Epoch 34 | Batch: 17 | Loss:  14.3944\n","Epoch 34 | Batch: 18 | Loss:  18.9214\n","Epoch 34 | Batch: 19 | Loss:  16.5991\n","Epoch 34 | Batch: 20 | Loss:  16.4885\n","Epoch 34 | Batch: 21 | Loss:  15.3021\n","Epoch 34 | Batch: 22 | Loss:  14.5311\n","Epoch 34 | Batch: 23 | Loss:  13.4308\n","Epoch 34 | Batch: 24 | Loss:  9.6477\n","Epoch 35 | Batch: 1 | Loss:  12.1571\n","Epoch 35 | Batch: 2 | Loss:  16.6942\n","Epoch 35 | Batch: 3 | Loss:  16.6444\n","Epoch 35 | Batch: 4 | Loss:  20.0173\n","Epoch 35 | Batch: 5 | Loss:  12.0565\n","Epoch 35 | Batch: 6 | Loss:  17.2611\n","Epoch 35 | Batch: 7 | Loss:  18.3149\n","Epoch 35 | Batch: 8 | Loss:  9.5728\n","Epoch 35 | Batch: 9 | Loss:  17.1433\n","Epoch 35 | Batch: 10 | Loss:  16.2947\n","Epoch 35 | Batch: 11 | Loss:  14.2413\n","Epoch 35 | Batch: 12 | Loss:  11.2094\n","Epoch 35 | Batch: 13 | Loss:  24.2485\n","Epoch 35 | Batch: 14 | Loss:  13.7270\n","Epoch 35 | Batch: 15 | Loss:  13.8176\n","Epoch 35 | Batch: 16 | Loss:  11.6217\n","Epoch 35 | Batch: 17 | Loss:  15.5266\n","Epoch 35 | Batch: 18 | Loss:  22.0015\n","Epoch 35 | Batch: 19 | Loss:  19.2440\n","Epoch 35 | Batch: 20 | Loss:  13.8383\n","Epoch 35 | Batch: 21 | Loss:  12.2374\n","Epoch 35 | Batch: 22 | Loss:  13.6955\n","Epoch 35 | Batch: 23 | Loss:  23.2825\n","Epoch 35 | Batch: 24 | Loss:  13.0858\n","Epoch 36 | Batch: 1 | Loss:  15.7373\n","Epoch 36 | Batch: 2 | Loss:  17.1731\n","Epoch 36 | Batch: 3 | Loss:  14.7541\n","Epoch 36 | Batch: 4 | Loss:  16.4699\n","Epoch 36 | Batch: 5 | Loss:  17.5286\n","Epoch 36 | Batch: 6 | Loss:  13.6326\n","Epoch 36 | Batch: 7 | Loss:  16.2102\n","Epoch 36 | Batch: 8 | Loss:  22.5402\n","Epoch 36 | Batch: 9 | Loss:  14.5282\n","Epoch 36 | Batch: 10 | Loss:  8.2900\n","Epoch 36 | Batch: 11 | Loss:  16.8263\n","Epoch 36 | Batch: 12 | Loss:  13.7183\n","Epoch 36 | Batch: 13 | Loss:  14.7913\n","Epoch 36 | Batch: 14 | Loss:  15.5396\n","Epoch 36 | Batch: 15 | Loss:  17.2478\n","Epoch 36 | Batch: 16 | Loss:  17.8136\n","Epoch 36 | Batch: 17 | Loss:  16.1476\n","Epoch 36 | Batch: 18 | Loss:  15.9854\n","Epoch 36 | Batch: 19 | Loss:  12.5220\n","Epoch 36 | Batch: 20 | Loss:  16.4097\n","Epoch 36 | Batch: 21 | Loss:  17.1190\n","Epoch 36 | Batch: 22 | Loss:  14.4980\n","Epoch 36 | Batch: 23 | Loss:  16.7101\n","Epoch 36 | Batch: 24 | Loss:  9.3361\n","Epoch 37 | Batch: 1 | Loss:  18.2482\n","Epoch 37 | Batch: 2 | Loss:  13.7268\n","Epoch 37 | Batch: 3 | Loss:  15.1610\n","Epoch 37 | Batch: 4 | Loss:  17.8007\n","Epoch 37 | Batch: 5 | Loss:  16.1276\n","Epoch 37 | Batch: 6 | Loss:  11.6150\n","Epoch 37 | Batch: 7 | Loss:  11.6866\n","Epoch 37 | Batch: 8 | Loss:  15.5850\n","Epoch 37 | Batch: 9 | Loss:  13.6921\n","Epoch 37 | Batch: 10 | Loss:  20.3933\n","Epoch 37 | Batch: 11 | Loss:  18.0942\n","Epoch 37 | Batch: 12 | Loss:  15.9449\n","Epoch 37 | Batch: 13 | Loss:  15.1820\n","Epoch 37 | Batch: 14 | Loss:  14.2702\n","Epoch 37 | Batch: 15 | Loss:  15.0228\n","Epoch 37 | Batch: 16 | Loss:  12.8469\n","Epoch 37 | Batch: 17 | Loss:  13.0140\n","Epoch 37 | Batch: 18 | Loss:  14.9699\n","Epoch 37 | Batch: 19 | Loss:  15.5279\n","Epoch 37 | Batch: 20 | Loss:  14.3452\n","Epoch 37 | Batch: 21 | Loss:  16.6217\n","Epoch 37 | Batch: 22 | Loss:  20.3692\n","Epoch 37 | Batch: 23 | Loss:  14.4316\n","Epoch 37 | Batch: 24 | Loss:  13.1444\n","Epoch 38 | Batch: 1 | Loss:  11.3558\n","Epoch 38 | Batch: 2 | Loss:  11.6559\n","Epoch 38 | Batch: 3 | Loss:  17.4046\n","Epoch 38 | Batch: 4 | Loss:  13.4972\n","Epoch 38 | Batch: 5 | Loss:  16.4845\n","Epoch 38 | Batch: 6 | Loss:  14.1456\n","Epoch 38 | Batch: 7 | Loss:  12.3947\n","Epoch 38 | Batch: 8 | Loss:  14.6572\n","Epoch 38 | Batch: 9 | Loss:  14.9900\n","Epoch 38 | Batch: 10 | Loss:  18.5065\n","Epoch 38 | Batch: 11 | Loss:  16.2863\n","Epoch 38 | Batch: 12 | Loss:  11.7876\n","Epoch 38 | Batch: 13 | Loss:  13.5244\n","Epoch 38 | Batch: 14 | Loss:  16.9249\n","Epoch 38 | Batch: 15 | Loss:  16.7837\n","Epoch 38 | Batch: 16 | Loss:  21.3710\n","Epoch 38 | Batch: 17 | Loss:  16.7584\n","Epoch 38 | Batch: 18 | Loss:  14.5392\n","Epoch 38 | Batch: 19 | Loss:  17.4208\n","Epoch 38 | Batch: 20 | Loss:  18.8430\n","Epoch 38 | Batch: 21 | Loss:  15.2148\n","Epoch 38 | Batch: 22 | Loss:  16.2777\n","Epoch 38 | Batch: 23 | Loss:  14.3455\n","Epoch 38 | Batch: 24 | Loss:  13.9773\n","Epoch 39 | Batch: 1 | Loss:  15.1577\n","Epoch 39 | Batch: 2 | Loss:  17.8288\n","Epoch 39 | Batch: 3 | Loss:  14.8859\n","Epoch 39 | Batch: 4 | Loss:  15.7204\n","Epoch 39 | Batch: 5 | Loss:  12.1561\n","Epoch 39 | Batch: 6 | Loss:  22.9065\n","Epoch 39 | Batch: 7 | Loss:  20.8857\n","Epoch 39 | Batch: 8 | Loss:  20.2588\n","Epoch 39 | Batch: 9 | Loss:  15.4765\n","Epoch 39 | Batch: 10 | Loss:  14.9468\n","Epoch 39 | Batch: 11 | Loss:  19.6058\n","Epoch 39 | Batch: 12 | Loss:  10.4849\n","Epoch 39 | Batch: 13 | Loss:  12.2101\n","Epoch 39 | Batch: 14 | Loss:  17.8517\n","Epoch 39 | Batch: 15 | Loss:  15.0959\n","Epoch 39 | Batch: 16 | Loss:  14.4205\n","Epoch 39 | Batch: 17 | Loss:  10.5294\n","Epoch 39 | Batch: 18 | Loss:  17.7901\n","Epoch 39 | Batch: 19 | Loss:  13.9489\n","Epoch 39 | Batch: 20 | Loss:  15.3937\n","Epoch 39 | Batch: 21 | Loss:  21.8516\n","Epoch 39 | Batch: 22 | Loss:  15.5497\n","Epoch 39 | Batch: 23 | Loss:  13.9582\n","Epoch 39 | Batch: 24 | Loss:  9.9027\n","Epoch 40 | Batch: 1 | Loss:  10.3694\n","Epoch 40 | Batch: 2 | Loss:  13.2275\n","Epoch 40 | Batch: 3 | Loss:  13.2817\n","Epoch 40 | Batch: 4 | Loss:  14.3153\n","Epoch 40 | Batch: 5 | Loss:  23.5947\n","Epoch 40 | Batch: 6 | Loss:  12.5878\n","Epoch 40 | Batch: 7 | Loss:  12.9639\n","Epoch 40 | Batch: 8 | Loss:  14.4681\n","Epoch 40 | Batch: 9 | Loss:  17.9810\n","Epoch 40 | Batch: 10 | Loss:  18.3898\n","Epoch 40 | Batch: 11 | Loss:  16.0645\n","Epoch 40 | Batch: 12 | Loss:  14.2493\n","Epoch 40 | Batch: 13 | Loss:  14.5671\n","Epoch 40 | Batch: 14 | Loss:  15.8826\n","Epoch 40 | Batch: 15 | Loss:  20.1869\n","Epoch 40 | Batch: 16 | Loss:  15.4660\n","Epoch 40 | Batch: 17 | Loss:  13.7299\n","Epoch 40 | Batch: 18 | Loss:  6.9775\n","Epoch 40 | Batch: 19 | Loss:  18.2521\n","Epoch 40 | Batch: 20 | Loss:  12.1143\n","Epoch 40 | Batch: 21 | Loss:  13.6712\n","Epoch 40 | Batch: 22 | Loss:  20.5293\n","Epoch 40 | Batch: 23 | Loss:  23.3126\n","Epoch 40 | Batch: 24 | Loss:  9.0853\n","Epoch 41 | Batch: 1 | Loss:  23.7902\n","Epoch 41 | Batch: 2 | Loss:  18.3940\n","Epoch 41 | Batch: 3 | Loss:  13.7066\n","Epoch 41 | Batch: 4 | Loss:  18.6550\n","Epoch 41 | Batch: 5 | Loss:  17.1158\n","Epoch 41 | Batch: 6 | Loss:  13.1458\n","Epoch 41 | Batch: 7 | Loss:  15.2446\n","Epoch 41 | Batch: 8 | Loss:  22.7175\n","Epoch 41 | Batch: 9 | Loss:  15.6601\n","Epoch 41 | Batch: 10 | Loss:  14.5217\n","Epoch 41 | Batch: 11 | Loss:  11.3405\n","Epoch 41 | Batch: 12 | Loss:  15.9866\n","Epoch 41 | Batch: 13 | Loss:  19.6816\n","Epoch 41 | Batch: 14 | Loss:  18.0901\n","Epoch 41 | Batch: 15 | Loss:  13.4440\n","Epoch 41 | Batch: 16 | Loss:  16.6829\n","Epoch 41 | Batch: 17 | Loss:  16.9526\n","Epoch 41 | Batch: 18 | Loss:  12.6957\n","Epoch 41 | Batch: 19 | Loss:  11.9569\n","Epoch 41 | Batch: 20 | Loss:  14.5195\n","Epoch 41 | Batch: 21 | Loss:  14.7257\n","Epoch 41 | Batch: 22 | Loss:  19.4346\n","Epoch 41 | Batch: 23 | Loss:  15.1867\n","Epoch 41 | Batch: 24 | Loss:  7.3583\n","Epoch 42 | Batch: 1 | Loss:  14.7584\n","Epoch 42 | Batch: 2 | Loss:  14.2712\n","Epoch 42 | Batch: 3 | Loss:  12.9862\n","Epoch 42 | Batch: 4 | Loss:  15.7377\n","Epoch 42 | Batch: 5 | Loss:  9.0512\n","Epoch 42 | Batch: 6 | Loss:  28.7873\n","Epoch 42 | Batch: 7 | Loss:  21.3097\n","Epoch 42 | Batch: 8 | Loss:  14.2195\n","Epoch 42 | Batch: 9 | Loss:  14.7461\n","Epoch 42 | Batch: 10 | Loss:  19.2495\n","Epoch 42 | Batch: 11 | Loss:  11.9132\n","Epoch 42 | Batch: 12 | Loss:  23.2097\n","Epoch 42 | Batch: 13 | Loss:  18.4673\n","Epoch 42 | Batch: 14 | Loss:  13.6220\n","Epoch 42 | Batch: 15 | Loss:  16.2653\n","Epoch 42 | Batch: 16 | Loss:  10.7474\n","Epoch 42 | Batch: 17 | Loss:  13.6698\n","Epoch 42 | Batch: 18 | Loss:  17.8500\n","Epoch 42 | Batch: 19 | Loss:  17.2980\n","Epoch 42 | Batch: 20 | Loss:  13.0103\n","Epoch 42 | Batch: 21 | Loss:  21.8271\n","Epoch 42 | Batch: 22 | Loss:  20.7599\n","Epoch 42 | Batch: 23 | Loss:  13.4115\n","Epoch 42 | Batch: 24 | Loss:  5.5302\n","Epoch 43 | Batch: 1 | Loss:  16.8456\n","Epoch 43 | Batch: 2 | Loss:  10.3902\n","Epoch 43 | Batch: 3 | Loss:  14.1584\n","Epoch 43 | Batch: 4 | Loss:  15.1826\n","Epoch 43 | Batch: 5 | Loss:  14.8814\n","Epoch 43 | Batch: 6 | Loss:  10.1966\n","Epoch 43 | Batch: 7 | Loss:  12.3038\n","Epoch 43 | Batch: 8 | Loss:  12.9679\n","Epoch 43 | Batch: 9 | Loss:  28.9055\n","Epoch 43 | Batch: 10 | Loss:  13.5490\n","Epoch 43 | Batch: 11 | Loss:  17.5720\n","Epoch 43 | Batch: 12 | Loss:  17.4086\n","Epoch 43 | Batch: 13 | Loss:  15.8192\n","Epoch 43 | Batch: 14 | Loss:  12.8781\n","Epoch 43 | Batch: 15 | Loss:  9.3998\n","Epoch 43 | Batch: 16 | Loss:  18.6598\n","Epoch 43 | Batch: 17 | Loss:  18.9240\n","Epoch 43 | Batch: 18 | Loss:  16.6244\n","Epoch 43 | Batch: 19 | Loss:  20.6148\n","Epoch 43 | Batch: 20 | Loss:  14.9795\n","Epoch 43 | Batch: 21 | Loss:  11.9253\n","Epoch 43 | Batch: 22 | Loss:  15.8350\n","Epoch 43 | Batch: 23 | Loss:  11.8077\n","Epoch 43 | Batch: 24 | Loss:  12.8879\n","Epoch 44 | Batch: 1 | Loss:  17.1621\n","Epoch 44 | Batch: 2 | Loss:  17.8084\n","Epoch 44 | Batch: 3 | Loss:  14.3036\n","Epoch 44 | Batch: 4 | Loss:  21.9257\n","Epoch 44 | Batch: 5 | Loss:  16.0059\n","Epoch 44 | Batch: 6 | Loss:  11.6684\n","Epoch 44 | Batch: 7 | Loss:  19.8399\n","Epoch 44 | Batch: 8 | Loss:  16.4402\n","Epoch 44 | Batch: 9 | Loss:  17.4917\n","Epoch 44 | Batch: 10 | Loss:  14.9587\n","Epoch 44 | Batch: 11 | Loss:  13.8840\n","Epoch 44 | Batch: 12 | Loss:  11.1334\n","Epoch 44 | Batch: 13 | Loss:  10.4580\n","Epoch 44 | Batch: 14 | Loss:  15.9598\n","Epoch 44 | Batch: 15 | Loss:  14.7374\n","Epoch 44 | Batch: 16 | Loss:  19.3719\n","Epoch 44 | Batch: 17 | Loss:  13.1447\n","Epoch 44 | Batch: 18 | Loss:  15.2353\n","Epoch 44 | Batch: 19 | Loss:  10.6075\n","Epoch 44 | Batch: 20 | Loss:  17.5827\n","Epoch 44 | Batch: 21 | Loss:  12.1468\n","Epoch 44 | Batch: 22 | Loss:  12.8172\n","Epoch 44 | Batch: 23 | Loss:  18.9740\n","Epoch 44 | Batch: 24 | Loss:  13.9071\n","Epoch 45 | Batch: 1 | Loss:  16.1287\n","Epoch 45 | Batch: 2 | Loss:  14.5310\n","Epoch 45 | Batch: 3 | Loss:  14.3926\n","Epoch 45 | Batch: 4 | Loss:  24.4539\n","Epoch 45 | Batch: 5 | Loss:  20.8256\n","Epoch 45 | Batch: 6 | Loss:  11.8129\n","Epoch 45 | Batch: 7 | Loss:  19.4060\n","Epoch 45 | Batch: 8 | Loss:  14.5692\n","Epoch 45 | Batch: 9 | Loss:  15.3438\n","Epoch 45 | Batch: 10 | Loss:  21.0619\n","Epoch 45 | Batch: 11 | Loss:  16.5110\n","Epoch 45 | Batch: 12 | Loss:  10.0349\n","Epoch 45 | Batch: 13 | Loss:  21.0202\n","Epoch 45 | Batch: 14 | Loss:  15.9277\n","Epoch 45 | Batch: 15 | Loss:  15.4720\n","Epoch 45 | Batch: 16 | Loss:  15.7158\n","Epoch 45 | Batch: 17 | Loss:  18.8481\n","Epoch 45 | Batch: 18 | Loss:  16.7819\n","Epoch 45 | Batch: 19 | Loss:  15.5329\n","Epoch 45 | Batch: 20 | Loss:  10.0248\n","Epoch 45 | Batch: 21 | Loss:  15.4786\n","Epoch 45 | Batch: 22 | Loss:  15.8628\n","Epoch 45 | Batch: 23 | Loss:  15.9347\n","Epoch 45 | Batch: 24 | Loss:  10.0268\n","Epoch 46 | Batch: 1 | Loss:  12.5139\n","Epoch 46 | Batch: 2 | Loss:  17.3050\n","Epoch 46 | Batch: 3 | Loss:  15.7005\n","Epoch 46 | Batch: 4 | Loss:  15.4010\n","Epoch 46 | Batch: 5 | Loss:  13.5900\n","Epoch 46 | Batch: 6 | Loss:  13.8159\n","Epoch 46 | Batch: 7 | Loss:  16.6884\n","Epoch 46 | Batch: 8 | Loss:  13.2659\n","Epoch 46 | Batch: 9 | Loss:  17.9592\n","Epoch 46 | Batch: 10 | Loss:  11.6746\n","Epoch 46 | Batch: 11 | Loss:  14.0352\n","Epoch 46 | Batch: 12 | Loss:  16.7048\n","Epoch 46 | Batch: 13 | Loss:  16.0256\n","Epoch 46 | Batch: 14 | Loss:  11.0131\n","Epoch 46 | Batch: 15 | Loss:  19.0550\n","Epoch 46 | Batch: 16 | Loss:  14.3713\n","Epoch 46 | Batch: 17 | Loss:  13.1123\n","Epoch 46 | Batch: 18 | Loss:  12.4285\n","Epoch 46 | Batch: 19 | Loss:  13.2802\n","Epoch 46 | Batch: 20 | Loss:  19.4350\n","Epoch 46 | Batch: 21 | Loss:  19.0948\n","Epoch 46 | Batch: 22 | Loss:  12.3759\n","Epoch 46 | Batch: 23 | Loss:  26.4938\n","Epoch 46 | Batch: 24 | Loss:  9.7341\n","Epoch 47 | Batch: 1 | Loss:  16.1449\n","Epoch 47 | Batch: 2 | Loss:  15.1154\n","Epoch 47 | Batch: 3 | Loss:  15.4277\n","Epoch 47 | Batch: 4 | Loss:  16.9422\n","Epoch 47 | Batch: 5 | Loss:  11.0700\n","Epoch 47 | Batch: 6 | Loss:  12.5245\n","Epoch 47 | Batch: 7 | Loss:  20.4853\n","Epoch 47 | Batch: 8 | Loss:  16.0428\n","Epoch 47 | Batch: 9 | Loss:  18.8477\n","Epoch 47 | Batch: 10 | Loss:  16.5566\n","Epoch 47 | Batch: 11 | Loss:  18.1101\n","Epoch 47 | Batch: 12 | Loss:  11.3729\n","Epoch 47 | Batch: 13 | Loss:  13.8268\n","Epoch 47 | Batch: 14 | Loss:  11.3441\n","Epoch 47 | Batch: 15 | Loss:  12.8511\n","Epoch 47 | Batch: 16 | Loss:  17.0067\n","Epoch 47 | Batch: 17 | Loss:  13.6088\n","Epoch 47 | Batch: 18 | Loss:  17.0649\n","Epoch 47 | Batch: 19 | Loss:  18.9127\n","Epoch 47 | Batch: 20 | Loss:  14.9623\n","Epoch 47 | Batch: 21 | Loss:  17.1299\n","Epoch 47 | Batch: 22 | Loss:  19.3823\n","Epoch 47 | Batch: 23 | Loss:  14.6548\n","Epoch 47 | Batch: 24 | Loss:  5.2942\n","Epoch 48 | Batch: 1 | Loss:  13.0202\n","Epoch 48 | Batch: 2 | Loss:  16.2054\n","Epoch 48 | Batch: 3 | Loss:  18.5716\n","Epoch 48 | Batch: 4 | Loss:  12.8260\n","Epoch 48 | Batch: 5 | Loss:  12.8883\n","Epoch 48 | Batch: 6 | Loss:  15.7939\n","Epoch 48 | Batch: 7 | Loss:  22.5222\n","Epoch 48 | Batch: 8 | Loss:  15.4463\n","Epoch 48 | Batch: 9 | Loss:  11.9619\n","Epoch 48 | Batch: 10 | Loss:  18.0958\n","Epoch 48 | Batch: 11 | Loss:  14.9257\n","Epoch 48 | Batch: 12 | Loss:  14.7622\n","Epoch 48 | Batch: 13 | Loss:  15.7080\n","Epoch 48 | Batch: 14 | Loss:  11.0879\n","Epoch 48 | Batch: 15 | Loss:  11.5011\n","Epoch 48 | Batch: 16 | Loss:  14.6599\n","Epoch 48 | Batch: 17 | Loss:  16.0836\n","Epoch 48 | Batch: 18 | Loss:  15.5356\n","Epoch 48 | Batch: 19 | Loss:  17.8700\n","Epoch 48 | Batch: 20 | Loss:  18.3690\n","Epoch 48 | Batch: 21 | Loss:  16.0446\n","Epoch 48 | Batch: 22 | Loss:  18.5556\n","Epoch 48 | Batch: 23 | Loss:  13.4845\n","Epoch 48 | Batch: 24 | Loss:  9.6262\n","Epoch 49 | Batch: 1 | Loss:  17.9658\n","Epoch 49 | Batch: 2 | Loss:  9.3075\n","Epoch 49 | Batch: 3 | Loss:  13.0095\n","Epoch 49 | Batch: 4 | Loss:  12.3867\n","Epoch 49 | Batch: 5 | Loss:  17.8017\n","Epoch 49 | Batch: 6 | Loss:  17.7970\n","Epoch 49 | Batch: 7 | Loss:  19.1157\n","Epoch 49 | Batch: 8 | Loss:  15.8505\n","Epoch 49 | Batch: 9 | Loss:  13.7439\n","Epoch 49 | Batch: 10 | Loss:  12.4690\n","Epoch 49 | Batch: 11 | Loss:  10.4206\n","Epoch 49 | Batch: 12 | Loss:  22.1699\n","Epoch 49 | Batch: 13 | Loss:  21.7156\n","Epoch 49 | Batch: 14 | Loss:  14.4577\n","Epoch 49 | Batch: 15 | Loss:  17.1119\n","Epoch 49 | Batch: 16 | Loss:  15.8634\n","Epoch 49 | Batch: 17 | Loss:  15.8150\n","Epoch 49 | Batch: 18 | Loss:  19.1123\n","Epoch 49 | Batch: 19 | Loss:  15.9936\n","Epoch 49 | Batch: 20 | Loss:  24.9107\n","Epoch 49 | Batch: 21 | Loss:  14.6219\n","Epoch 49 | Batch: 22 | Loss:  16.9709\n","Epoch 49 | Batch: 23 | Loss:  7.6207\n","Epoch 49 | Batch: 24 | Loss:  9.4180\n","Epoch 50 | Batch: 1 | Loss:  14.4291\n","Epoch 50 | Batch: 2 | Loss:  18.0731\n","Epoch 50 | Batch: 3 | Loss:  18.1011\n","Epoch 50 | Batch: 4 | Loss:  12.6961\n","Epoch 50 | Batch: 5 | Loss:  12.2797\n","Epoch 50 | Batch: 6 | Loss:  16.3522\n","Epoch 50 | Batch: 7 | Loss:  12.9149\n","Epoch 50 | Batch: 8 | Loss:  11.1211\n","Epoch 50 | Batch: 9 | Loss:  10.2677\n","Epoch 50 | Batch: 10 | Loss:  18.4953\n","Epoch 50 | Batch: 11 | Loss:  13.9613\n","Epoch 50 | Batch: 12 | Loss:  12.2109\n","Epoch 50 | Batch: 13 | Loss:  17.7745\n","Epoch 50 | Batch: 14 | Loss:  16.2535\n","Epoch 50 | Batch: 15 | Loss:  13.6538\n","Epoch 50 | Batch: 16 | Loss:  20.3612\n","Epoch 50 | Batch: 17 | Loss:  14.1119\n","Epoch 50 | Batch: 18 | Loss:  13.6892\n","Epoch 50 | Batch: 19 | Loss:  17.2143\n","Epoch 50 | Batch: 20 | Loss:  13.6667\n","Epoch 50 | Batch: 21 | Loss:  17.9423\n","Epoch 50 | Batch: 22 | Loss:  18.7958\n","Epoch 50 | Batch: 23 | Loss:  13.2018\n","Epoch 50 | Batch: 24 | Loss:  7.8814\n","Epoch 51 | Batch: 1 | Loss:  15.8053\n","Epoch 51 | Batch: 2 | Loss:  9.9754\n","Epoch 51 | Batch: 3 | Loss:  10.8849\n","Epoch 51 | Batch: 4 | Loss:  16.9713\n","Epoch 51 | Batch: 5 | Loss:  15.2363\n","Epoch 51 | Batch: 6 | Loss:  16.1370\n","Epoch 51 | Batch: 7 | Loss:  15.3600\n","Epoch 51 | Batch: 8 | Loss:  17.2184\n","Epoch 51 | Batch: 9 | Loss:  21.3231\n","Epoch 51 | Batch: 10 | Loss:  23.6255\n","Epoch 51 | Batch: 11 | Loss:  15.5411\n","Epoch 51 | Batch: 12 | Loss:  18.3813\n","Epoch 51 | Batch: 13 | Loss:  16.6605\n","Epoch 51 | Batch: 14 | Loss:  13.5082\n","Epoch 51 | Batch: 15 | Loss:  16.9477\n","Epoch 51 | Batch: 16 | Loss:  19.8820\n","Epoch 51 | Batch: 17 | Loss:  11.3820\n","Epoch 51 | Batch: 18 | Loss:  14.5078\n","Epoch 51 | Batch: 19 | Loss:  14.9026\n","Epoch 51 | Batch: 20 | Loss:  12.1091\n","Epoch 51 | Batch: 21 | Loss:  13.6189\n","Epoch 51 | Batch: 22 | Loss:  14.9894\n","Epoch 51 | Batch: 23 | Loss:  12.0058\n","Epoch 51 | Batch: 24 | Loss:  10.6848\n","Epoch 52 | Batch: 1 | Loss:  12.6915\n","Epoch 52 | Batch: 2 | Loss:  21.7277\n","Epoch 52 | Batch: 3 | Loss:  13.0485\n","Epoch 52 | Batch: 4 | Loss:  16.9765\n","Epoch 52 | Batch: 5 | Loss:  12.2140\n","Epoch 52 | Batch: 6 | Loss:  10.1363\n","Epoch 52 | Batch: 7 | Loss:  10.6538\n","Epoch 52 | Batch: 8 | Loss:  20.8582\n","Epoch 52 | Batch: 9 | Loss:  14.3742\n","Epoch 52 | Batch: 10 | Loss:  16.8261\n","Epoch 52 | Batch: 11 | Loss:  18.8859\n","Epoch 52 | Batch: 12 | Loss:  16.1143\n","Epoch 52 | Batch: 13 | Loss:  14.2246\n","Epoch 52 | Batch: 14 | Loss:  8.2329\n","Epoch 52 | Batch: 15 | Loss:  12.1086\n","Epoch 52 | Batch: 16 | Loss:  9.2106\n","Epoch 52 | Batch: 17 | Loss:  13.8200\n","Epoch 52 | Batch: 18 | Loss:  15.6189\n","Epoch 52 | Batch: 19 | Loss:  18.8995\n","Epoch 52 | Batch: 20 | Loss:  19.6050\n","Epoch 52 | Batch: 21 | Loss:  16.5090\n","Epoch 52 | Batch: 22 | Loss:  14.7628\n","Epoch 52 | Batch: 23 | Loss:  14.7653\n","Epoch 52 | Batch: 24 | Loss:  15.3340\n","Epoch 53 | Batch: 1 | Loss:  15.2081\n","Epoch 53 | Batch: 2 | Loss:  16.9816\n","Epoch 53 | Batch: 3 | Loss:  19.0909\n","Epoch 53 | Batch: 4 | Loss:  14.8262\n","Epoch 53 | Batch: 5 | Loss:  21.6624\n","Epoch 53 | Batch: 6 | Loss:  15.5219\n","Epoch 53 | Batch: 7 | Loss:  16.3690\n","Epoch 53 | Batch: 8 | Loss:  11.2702\n","Epoch 53 | Batch: 9 | Loss:  13.2097\n","Epoch 53 | Batch: 10 | Loss:  13.8731\n","Epoch 53 | Batch: 11 | Loss:  15.0551\n","Epoch 53 | Batch: 12 | Loss:  14.5679\n","Epoch 53 | Batch: 13 | Loss:  16.5599\n","Epoch 53 | Batch: 14 | Loss:  12.6222\n","Epoch 53 | Batch: 15 | Loss:  15.4206\n","Epoch 53 | Batch: 16 | Loss:  14.5306\n","Epoch 53 | Batch: 17 | Loss:  20.5560\n","Epoch 53 | Batch: 18 | Loss:  13.0500\n","Epoch 53 | Batch: 19 | Loss:  13.0201\n","Epoch 53 | Batch: 20 | Loss:  14.1651\n","Epoch 53 | Batch: 21 | Loss:  13.0264\n","Epoch 53 | Batch: 22 | Loss:  16.0280\n","Epoch 53 | Batch: 23 | Loss:  14.6123\n","Epoch 53 | Batch: 24 | Loss:  13.3277\n","Epoch 54 | Batch: 1 | Loss:  15.4658\n","Epoch 54 | Batch: 2 | Loss:  12.3375\n","Epoch 54 | Batch: 3 | Loss:  23.9278\n","Epoch 54 | Batch: 4 | Loss:  15.8821\n","Epoch 54 | Batch: 5 | Loss:  11.7191\n","Epoch 54 | Batch: 6 | Loss:  10.6804\n","Epoch 54 | Batch: 7 | Loss:  17.2353\n","Epoch 54 | Batch: 8 | Loss:  16.3950\n","Epoch 54 | Batch: 9 | Loss:  11.1056\n","Epoch 54 | Batch: 10 | Loss:  16.7165\n","Epoch 54 | Batch: 11 | Loss:  15.8421\n","Epoch 54 | Batch: 12 | Loss:  20.1495\n","Epoch 54 | Batch: 13 | Loss:  15.0858\n","Epoch 54 | Batch: 14 | Loss:  13.7633\n","Epoch 54 | Batch: 15 | Loss:  19.6912\n","Epoch 54 | Batch: 16 | Loss:  16.7691\n","Epoch 54 | Batch: 17 | Loss:  15.0116\n","Epoch 54 | Batch: 18 | Loss:  10.1026\n","Epoch 54 | Batch: 19 | Loss:  16.8912\n","Epoch 54 | Batch: 20 | Loss:  12.7886\n","Epoch 54 | Batch: 21 | Loss:  15.9748\n","Epoch 54 | Batch: 22 | Loss:  17.4875\n","Epoch 54 | Batch: 23 | Loss:  20.5040\n","Epoch 54 | Batch: 24 | Loss:  8.8548\n","Epoch 55 | Batch: 1 | Loss:  19.2697\n","Epoch 55 | Batch: 2 | Loss:  18.8089\n","Epoch 55 | Batch: 3 | Loss:  19.2219\n","Epoch 55 | Batch: 4 | Loss:  12.1471\n","Epoch 55 | Batch: 5 | Loss:  10.3888\n","Epoch 55 | Batch: 6 | Loss:  14.7512\n","Epoch 55 | Batch: 7 | Loss:  8.1025\n","Epoch 55 | Batch: 8 | Loss:  16.3465\n","Epoch 55 | Batch: 9 | Loss:  20.7161\n","Epoch 55 | Batch: 10 | Loss:  18.9862\n","Epoch 55 | Batch: 11 | Loss:  14.8277\n","Epoch 55 | Batch: 12 | Loss:  13.2589\n","Epoch 55 | Batch: 13 | Loss:  19.2299\n","Epoch 55 | Batch: 14 | Loss:  14.8627\n","Epoch 55 | Batch: 15 | Loss:  14.7021\n","Epoch 55 | Batch: 16 | Loss:  20.1614\n","Epoch 55 | Batch: 17 | Loss:  16.4755\n","Epoch 55 | Batch: 18 | Loss:  13.6403\n","Epoch 55 | Batch: 19 | Loss:  14.1415\n","Epoch 55 | Batch: 20 | Loss:  18.4402\n","Epoch 55 | Batch: 21 | Loss:  14.8601\n","Epoch 55 | Batch: 22 | Loss:  10.4202\n","Epoch 55 | Batch: 23 | Loss:  17.2894\n","Epoch 55 | Batch: 24 | Loss:  10.1983\n","Epoch 56 | Batch: 1 | Loss:  14.1887\n","Epoch 56 | Batch: 2 | Loss:  16.4288\n","Epoch 56 | Batch: 3 | Loss:  15.5302\n","Epoch 56 | Batch: 4 | Loss:  13.2847\n","Epoch 56 | Batch: 5 | Loss:  13.8410\n","Epoch 56 | Batch: 6 | Loss:  10.5063\n","Epoch 56 | Batch: 7 | Loss:  13.3271\n","Epoch 56 | Batch: 8 | Loss:  11.6015\n","Epoch 56 | Batch: 9 | Loss:  11.6923\n","Epoch 56 | Batch: 10 | Loss:  13.4394\n","Epoch 56 | Batch: 11 | Loss:  17.8027\n","Epoch 56 | Batch: 12 | Loss:  16.9269\n","Epoch 56 | Batch: 13 | Loss:  15.2663\n","Epoch 56 | Batch: 14 | Loss:  13.8406\n","Epoch 56 | Batch: 15 | Loss:  16.2079\n","Epoch 56 | Batch: 16 | Loss:  10.1036\n","Epoch 56 | Batch: 17 | Loss:  16.6326\n","Epoch 56 | Batch: 18 | Loss:  14.7341\n","Epoch 56 | Batch: 19 | Loss:  22.1442\n","Epoch 56 | Batch: 20 | Loss:  12.5969\n","Epoch 56 | Batch: 21 | Loss:  19.8110\n","Epoch 56 | Batch: 22 | Loss:  14.9245\n","Epoch 56 | Batch: 23 | Loss:  19.0809\n","Epoch 56 | Batch: 24 | Loss:  16.5210\n","Epoch 57 | Batch: 1 | Loss:  19.6276\n","Epoch 57 | Batch: 2 | Loss:  14.4351\n","Epoch 57 | Batch: 3 | Loss:  13.1464\n","Epoch 57 | Batch: 4 | Loss:  15.1690\n","Epoch 57 | Batch: 5 | Loss:  12.4393\n","Epoch 57 | Batch: 6 | Loss:  12.3166\n","Epoch 57 | Batch: 7 | Loss:  20.1971\n","Epoch 57 | Batch: 8 | Loss:  20.5700\n","Epoch 57 | Batch: 9 | Loss:  12.1620\n","Epoch 57 | Batch: 10 | Loss:  16.2462\n","Epoch 57 | Batch: 11 | Loss:  16.8094\n","Epoch 57 | Batch: 12 | Loss:  16.3401\n","Epoch 57 | Batch: 13 | Loss:  17.5952\n","Epoch 57 | Batch: 14 | Loss:  11.6205\n","Epoch 57 | Batch: 15 | Loss:  11.5199\n","Epoch 57 | Batch: 16 | Loss:  9.5565\n","Epoch 57 | Batch: 17 | Loss:  17.3578\n","Epoch 57 | Batch: 18 | Loss:  19.0734\n","Epoch 57 | Batch: 19 | Loss:  16.8149\n","Epoch 57 | Batch: 20 | Loss:  10.3179\n","Epoch 57 | Batch: 21 | Loss:  18.4974\n","Epoch 57 | Batch: 22 | Loss:  16.3802\n","Epoch 57 | Batch: 23 | Loss:  13.1091\n","Epoch 57 | Batch: 24 | Loss:  12.8985\n","Epoch 58 | Batch: 1 | Loss:  15.0245\n","Epoch 58 | Batch: 2 | Loss:  14.8826\n","Epoch 58 | Batch: 3 | Loss:  12.7717\n","Epoch 58 | Batch: 4 | Loss:  14.6978\n","Epoch 58 | Batch: 5 | Loss:  10.8885\n","Epoch 58 | Batch: 6 | Loss:  15.4691\n","Epoch 58 | Batch: 7 | Loss:  11.7961\n","Epoch 58 | Batch: 8 | Loss:  13.0410\n","Epoch 58 | Batch: 9 | Loss:  11.5037\n","Epoch 58 | Batch: 10 | Loss:  15.0305\n","Epoch 58 | Batch: 11 | Loss:  11.6679\n","Epoch 58 | Batch: 12 | Loss:  17.7535\n","Epoch 58 | Batch: 13 | Loss:  16.9414\n","Epoch 58 | Batch: 14 | Loss:  14.5608\n","Epoch 58 | Batch: 15 | Loss:  23.0780\n","Epoch 58 | Batch: 16 | Loss:  14.8513\n","Epoch 58 | Batch: 17 | Loss:  13.9135\n","Epoch 58 | Batch: 18 | Loss:  16.2260\n","Epoch 58 | Batch: 19 | Loss:  14.1359\n","Epoch 58 | Batch: 20 | Loss:  18.0310\n","Epoch 58 | Batch: 21 | Loss:  16.6074\n","Epoch 58 | Batch: 22 | Loss:  16.7700\n","Epoch 58 | Batch: 23 | Loss:  17.5113\n","Epoch 58 | Batch: 24 | Loss:  13.3097\n","Epoch 59 | Batch: 1 | Loss:  12.0221\n","Epoch 59 | Batch: 2 | Loss:  14.5112\n","Epoch 59 | Batch: 3 | Loss:  15.6482\n","Epoch 59 | Batch: 4 | Loss:  16.0058\n","Epoch 59 | Batch: 5 | Loss:  15.2324\n","Epoch 59 | Batch: 6 | Loss:  13.1470\n","Epoch 59 | Batch: 7 | Loss:  16.4845\n","Epoch 59 | Batch: 8 | Loss:  16.1992\n","Epoch 59 | Batch: 9 | Loss:  15.7108\n","Epoch 59 | Batch: 10 | Loss:  10.1886\n","Epoch 59 | Batch: 11 | Loss:  13.9614\n","Epoch 59 | Batch: 12 | Loss:  23.5334\n","Epoch 59 | Batch: 13 | Loss:  18.2135\n","Epoch 59 | Batch: 14 | Loss:  13.2425\n","Epoch 59 | Batch: 15 | Loss:  17.5228\n","Epoch 59 | Batch: 16 | Loss:  13.6918\n","Epoch 59 | Batch: 17 | Loss:  15.1742\n","Epoch 59 | Batch: 18 | Loss:  18.4296\n","Epoch 59 | Batch: 19 | Loss:  12.1177\n","Epoch 59 | Batch: 20 | Loss:  18.6663\n","Epoch 59 | Batch: 21 | Loss:  13.8587\n","Epoch 59 | Batch: 22 | Loss:  12.9688\n","Epoch 59 | Batch: 23 | Loss:  16.8422\n","Epoch 59 | Batch: 24 | Loss:  11.7774\n","Epoch 60 | Batch: 1 | Loss:  14.1053\n","Epoch 60 | Batch: 2 | Loss:  17.8065\n","Epoch 60 | Batch: 3 | Loss:  18.6496\n","Epoch 60 | Batch: 4 | Loss:  10.0185\n","Epoch 60 | Batch: 5 | Loss:  14.6055\n","Epoch 60 | Batch: 6 | Loss:  11.9984\n","Epoch 60 | Batch: 7 | Loss:  13.9362\n","Epoch 60 | Batch: 8 | Loss:  15.1813\n","Epoch 60 | Batch: 9 | Loss:  13.9290\n","Epoch 60 | Batch: 10 | Loss:  12.1530\n","Epoch 60 | Batch: 11 | Loss:  17.7966\n","Epoch 60 | Batch: 12 | Loss:  16.7021\n","Epoch 60 | Batch: 13 | Loss:  12.2307\n","Epoch 60 | Batch: 14 | Loss:  12.1813\n","Epoch 60 | Batch: 15 | Loss:  11.1002\n","Epoch 60 | Batch: 16 | Loss:  17.6239\n","Epoch 60 | Batch: 17 | Loss:  16.1511\n","Epoch 60 | Batch: 18 | Loss:  13.2887\n","Epoch 60 | Batch: 19 | Loss:  16.1192\n","Epoch 60 | Batch: 20 | Loss:  15.8026\n","Epoch 60 | Batch: 21 | Loss:  17.0470\n","Epoch 60 | Batch: 22 | Loss:  15.9572\n","Epoch 60 | Batch: 23 | Loss:  19.0040\n","Epoch 60 | Batch: 24 | Loss:  8.7176\n","Epoch 61 | Batch: 1 | Loss:  16.9738\n","Epoch 61 | Batch: 2 | Loss:  15.0818\n","Epoch 61 | Batch: 3 | Loss:  16.1676\n","Epoch 61 | Batch: 4 | Loss:  17.1968\n","Epoch 61 | Batch: 5 | Loss:  12.9957\n","Epoch 61 | Batch: 6 | Loss:  18.2236\n","Epoch 61 | Batch: 7 | Loss:  17.3412\n","Epoch 61 | Batch: 8 | Loss:  12.9975\n","Epoch 61 | Batch: 9 | Loss:  15.1929\n","Epoch 61 | Batch: 10 | Loss:  19.3831\n","Epoch 61 | Batch: 11 | Loss:  14.7785\n","Epoch 61 | Batch: 12 | Loss:  16.2343\n","Epoch 61 | Batch: 13 | Loss:  18.9760\n","Epoch 61 | Batch: 14 | Loss:  17.2334\n","Epoch 61 | Batch: 15 | Loss:  16.4404\n","Epoch 61 | Batch: 16 | Loss:  12.7788\n","Epoch 61 | Batch: 17 | Loss:  18.2198\n","Epoch 61 | Batch: 18 | Loss:  16.0578\n","Epoch 61 | Batch: 19 | Loss:  12.3405\n","Epoch 61 | Batch: 20 | Loss:  10.9611\n","Epoch 61 | Batch: 21 | Loss:  13.1609\n","Epoch 61 | Batch: 22 | Loss:  12.5081\n","Epoch 61 | Batch: 23 | Loss:  14.5667\n","Epoch 61 | Batch: 24 | Loss:  10.1832\n","Epoch 62 | Batch: 1 | Loss:  13.3038\n","Epoch 62 | Batch: 2 | Loss:  15.5657\n","Epoch 62 | Batch: 3 | Loss:  11.8574\n","Epoch 62 | Batch: 4 | Loss:  14.4634\n","Epoch 62 | Batch: 5 | Loss:  14.3867\n","Epoch 62 | Batch: 6 | Loss:  17.0847\n","Epoch 62 | Batch: 7 | Loss:  10.4691\n","Epoch 62 | Batch: 8 | Loss:  16.4506\n","Epoch 62 | Batch: 9 | Loss:  18.4643\n","Epoch 62 | Batch: 10 | Loss:  14.5194\n","Epoch 62 | Batch: 11 | Loss:  13.6008\n","Epoch 62 | Batch: 12 | Loss:  15.4114\n","Epoch 62 | Batch: 13 | Loss:  15.8333\n","Epoch 62 | Batch: 14 | Loss:  14.2838\n","Epoch 62 | Batch: 15 | Loss:  15.4207\n","Epoch 62 | Batch: 16 | Loss:  21.4617\n","Epoch 62 | Batch: 17 | Loss:  13.1420\n","Epoch 62 | Batch: 18 | Loss:  19.4650\n","Epoch 62 | Batch: 19 | Loss:  15.5290\n","Epoch 62 | Batch: 20 | Loss:  16.5443\n","Epoch 62 | Batch: 21 | Loss:  11.8432\n","Epoch 62 | Batch: 22 | Loss:  16.5213\n","Epoch 62 | Batch: 23 | Loss:  13.4433\n","Epoch 62 | Batch: 24 | Loss:  13.1966\n","Epoch 63 | Batch: 1 | Loss:  15.7009\n","Epoch 63 | Batch: 2 | Loss:  9.5993\n","Epoch 63 | Batch: 3 | Loss:  15.7889\n","Epoch 63 | Batch: 4 | Loss:  14.6466\n","Epoch 63 | Batch: 5 | Loss:  24.5427\n","Epoch 63 | Batch: 6 | Loss:  13.3446\n","Epoch 63 | Batch: 7 | Loss:  16.1920\n","Epoch 63 | Batch: 8 | Loss:  13.9413\n","Epoch 63 | Batch: 9 | Loss:  13.7591\n","Epoch 63 | Batch: 10 | Loss:  20.7465\n","Epoch 63 | Batch: 11 | Loss:  13.3917\n","Epoch 63 | Batch: 12 | Loss:  19.3020\n","Epoch 63 | Batch: 13 | Loss:  12.5597\n","Epoch 63 | Batch: 14 | Loss:  17.5923\n","Epoch 63 | Batch: 15 | Loss:  9.3461\n","Epoch 63 | Batch: 16 | Loss:  10.9621\n","Epoch 63 | Batch: 17 | Loss:  17.1861\n","Epoch 63 | Batch: 18 | Loss:  15.0959\n","Epoch 63 | Batch: 19 | Loss:  13.2480\n","Epoch 63 | Batch: 20 | Loss:  14.3444\n","Epoch 63 | Batch: 21 | Loss:  15.8261\n","Epoch 63 | Batch: 22 | Loss:  13.4354\n","Epoch 63 | Batch: 23 | Loss:  20.3549\n","Epoch 63 | Batch: 24 | Loss:  17.5010\n","Epoch 64 | Batch: 1 | Loss:  16.3703\n","Epoch 64 | Batch: 2 | Loss:  13.2840\n","Epoch 64 | Batch: 3 | Loss:  12.5059\n","Epoch 64 | Batch: 4 | Loss:  20.9887\n","Epoch 64 | Batch: 5 | Loss:  20.7698\n","Epoch 64 | Batch: 6 | Loss:  14.3546\n","Epoch 64 | Batch: 7 | Loss:  15.5074\n","Epoch 64 | Batch: 8 | Loss:  13.2725\n","Epoch 64 | Batch: 9 | Loss:  18.1625\n","Epoch 64 | Batch: 10 | Loss:  18.2111\n","Epoch 64 | Batch: 11 | Loss:  17.9573\n","Epoch 64 | Batch: 12 | Loss:  13.9019\n","Epoch 64 | Batch: 13 | Loss:  14.4974\n","Epoch 64 | Batch: 14 | Loss:  14.4910\n","Epoch 64 | Batch: 15 | Loss:  11.8124\n","Epoch 64 | Batch: 16 | Loss:  14.8732\n","Epoch 64 | Batch: 17 | Loss:  10.8726\n","Epoch 64 | Batch: 18 | Loss:  9.3536\n","Epoch 64 | Batch: 19 | Loss:  18.3648\n","Epoch 64 | Batch: 20 | Loss:  18.8595\n","Epoch 64 | Batch: 21 | Loss:  13.2719\n","Epoch 64 | Batch: 22 | Loss:  18.1064\n","Epoch 64 | Batch: 23 | Loss:  12.7257\n","Epoch 64 | Batch: 24 | Loss:  10.0102\n","Epoch 65 | Batch: 1 | Loss:  12.6414\n","Epoch 65 | Batch: 2 | Loss:  14.5473\n","Epoch 65 | Batch: 3 | Loss:  14.4855\n","Epoch 65 | Batch: 4 | Loss:  17.1019\n","Epoch 65 | Batch: 5 | Loss:  14.6465\n","Epoch 65 | Batch: 6 | Loss:  22.1590\n","Epoch 65 | Batch: 7 | Loss:  14.5887\n","Epoch 65 | Batch: 8 | Loss:  14.5553\n","Epoch 65 | Batch: 9 | Loss:  14.6234\n","Epoch 65 | Batch: 10 | Loss:  14.3686\n","Epoch 65 | Batch: 11 | Loss:  11.9795\n","Epoch 65 | Batch: 12 | Loss:  10.3971\n","Epoch 65 | Batch: 13 | Loss:  13.7171\n","Epoch 65 | Batch: 14 | Loss:  17.4498\n","Epoch 65 | Batch: 15 | Loss:  17.1424\n","Epoch 65 | Batch: 16 | Loss:  13.7204\n","Epoch 65 | Batch: 17 | Loss:  12.1878\n","Epoch 65 | Batch: 18 | Loss:  13.1256\n","Epoch 65 | Batch: 19 | Loss:  16.5647\n","Epoch 65 | Batch: 20 | Loss:  14.7444\n","Epoch 65 | Batch: 21 | Loss:  19.2195\n","Epoch 65 | Batch: 22 | Loss:  14.1815\n","Epoch 65 | Batch: 23 | Loss:  10.1968\n","Epoch 65 | Batch: 24 | Loss:  13.5423\n","Epoch 66 | Batch: 1 | Loss:  12.9943\n","Epoch 66 | Batch: 2 | Loss:  11.4317\n","Epoch 66 | Batch: 3 | Loss:  22.9456\n","Epoch 66 | Batch: 4 | Loss:  15.8702\n","Epoch 66 | Batch: 5 | Loss:  10.4062\n","Epoch 66 | Batch: 6 | Loss:  24.2199\n","Epoch 66 | Batch: 7 | Loss:  14.1777\n","Epoch 66 | Batch: 8 | Loss:  12.9796\n","Epoch 66 | Batch: 9 | Loss:  14.1772\n","Epoch 66 | Batch: 10 | Loss:  12.0303\n","Epoch 66 | Batch: 11 | Loss:  14.8717\n","Epoch 66 | Batch: 12 | Loss:  17.5882\n","Epoch 66 | Batch: 13 | Loss:  20.8565\n","Epoch 66 | Batch: 14 | Loss:  11.3999\n","Epoch 66 | Batch: 15 | Loss:  20.1772\n","Epoch 66 | Batch: 16 | Loss:  14.7265\n","Epoch 66 | Batch: 17 | Loss:  15.4504\n","Epoch 66 | Batch: 18 | Loss:  13.8442\n","Epoch 66 | Batch: 19 | Loss:  11.2562\n","Epoch 66 | Batch: 20 | Loss:  13.2710\n","Epoch 66 | Batch: 21 | Loss:  14.2328\n","Epoch 66 | Batch: 22 | Loss:  16.6649\n","Epoch 66 | Batch: 23 | Loss:  11.8580\n","Epoch 66 | Batch: 24 | Loss:  11.5337\n","Epoch 67 | Batch: 1 | Loss:  13.2362\n","Epoch 67 | Batch: 2 | Loss:  15.1158\n","Epoch 67 | Batch: 3 | Loss:  14.5480\n","Epoch 67 | Batch: 4 | Loss:  22.8986\n","Epoch 67 | Batch: 5 | Loss:  15.8532\n","Epoch 67 | Batch: 6 | Loss:  18.2447\n","Epoch 67 | Batch: 7 | Loss:  14.7722\n","Epoch 67 | Batch: 8 | Loss:  13.3096\n","Epoch 67 | Batch: 9 | Loss:  19.5478\n","Epoch 67 | Batch: 10 | Loss:  15.2475\n","Epoch 67 | Batch: 11 | Loss:  16.1051\n","Epoch 67 | Batch: 12 | Loss:  15.6722\n","Epoch 67 | Batch: 13 | Loss:  11.9222\n","Epoch 67 | Batch: 14 | Loss:  12.0712\n","Epoch 67 | Batch: 15 | Loss:  19.3333\n","Epoch 67 | Batch: 16 | Loss:  9.9146\n","Epoch 67 | Batch: 17 | Loss:  22.0469\n","Epoch 67 | Batch: 18 | Loss:  15.5551\n","Epoch 67 | Batch: 19 | Loss:  14.1782\n","Epoch 67 | Batch: 20 | Loss:  12.5794\n","Epoch 67 | Batch: 21 | Loss:  18.6466\n","Epoch 67 | Batch: 22 | Loss:  16.6270\n","Epoch 67 | Batch: 23 | Loss:  12.4549\n","Epoch 67 | Batch: 24 | Loss:  8.3902\n","Epoch 68 | Batch: 1 | Loss:  20.7804\n","Epoch 68 | Batch: 2 | Loss:  24.3132\n","Epoch 68 | Batch: 3 | Loss:  13.5819\n","Epoch 68 | Batch: 4 | Loss:  15.5081\n","Epoch 68 | Batch: 5 | Loss:  12.1958\n","Epoch 68 | Batch: 6 | Loss:  16.9215\n","Epoch 68 | Batch: 7 | Loss:  15.4965\n","Epoch 68 | Batch: 8 | Loss:  9.3070\n","Epoch 68 | Batch: 9 | Loss:  17.2636\n","Epoch 68 | Batch: 10 | Loss:  11.5839\n","Epoch 68 | Batch: 11 | Loss:  12.8096\n","Epoch 68 | Batch: 12 | Loss:  14.9972\n","Epoch 68 | Batch: 13 | Loss:  15.6554\n","Epoch 68 | Batch: 14 | Loss:  20.4956\n","Epoch 68 | Batch: 15 | Loss:  19.9276\n","Epoch 68 | Batch: 16 | Loss:  15.3705\n","Epoch 68 | Batch: 17 | Loss:  13.2466\n","Epoch 68 | Batch: 18 | Loss:  9.6101\n","Epoch 68 | Batch: 19 | Loss:  21.1587\n","Epoch 68 | Batch: 20 | Loss:  16.8583\n","Epoch 68 | Batch: 21 | Loss:  16.8850\n","Epoch 68 | Batch: 22 | Loss:  15.9340\n","Epoch 68 | Batch: 23 | Loss:  12.1347\n","Epoch 68 | Batch: 24 | Loss:  7.5372\n","Epoch 69 | Batch: 1 | Loss:  16.3779\n","Epoch 69 | Batch: 2 | Loss:  21.2342\n","Epoch 69 | Batch: 3 | Loss:  14.4736\n","Epoch 69 | Batch: 4 | Loss:  13.3508\n","Epoch 69 | Batch: 5 | Loss:  15.8912\n","Epoch 69 | Batch: 6 | Loss:  14.4545\n","Epoch 69 | Batch: 7 | Loss:  11.6093\n","Epoch 69 | Batch: 8 | Loss:  28.3931\n","Epoch 69 | Batch: 9 | Loss:  20.2159\n","Epoch 69 | Batch: 10 | Loss:  16.3683\n","Epoch 69 | Batch: 11 | Loss:  12.2280\n","Epoch 69 | Batch: 12 | Loss:  11.5299\n","Epoch 69 | Batch: 13 | Loss:  17.0967\n","Epoch 69 | Batch: 14 | Loss:  17.2448\n","Epoch 69 | Batch: 15 | Loss:  13.5782\n","Epoch 69 | Batch: 16 | Loss:  15.9127\n","Epoch 69 | Batch: 17 | Loss:  16.2578\n","Epoch 69 | Batch: 18 | Loss:  11.7824\n","Epoch 69 | Batch: 19 | Loss:  11.3745\n","Epoch 69 | Batch: 20 | Loss:  12.3721\n","Epoch 69 | Batch: 21 | Loss:  15.8561\n","Epoch 69 | Batch: 22 | Loss:  15.7202\n","Epoch 69 | Batch: 23 | Loss:  16.1883\n","Epoch 69 | Batch: 24 | Loss:  11.0905\n","Epoch 70 | Batch: 1 | Loss:  14.7956\n","Epoch 70 | Batch: 2 | Loss:  7.4797\n","Epoch 70 | Batch: 3 | Loss:  15.8650\n","Epoch 70 | Batch: 4 | Loss:  15.6021\n","Epoch 70 | Batch: 5 | Loss:  16.9780\n","Epoch 70 | Batch: 6 | Loss:  14.4023\n","Epoch 70 | Batch: 7 | Loss:  14.1993\n","Epoch 70 | Batch: 8 | Loss:  14.6342\n","Epoch 70 | Batch: 9 | Loss:  11.4989\n","Epoch 70 | Batch: 10 | Loss:  20.3088\n","Epoch 70 | Batch: 11 | Loss:  14.4991\n","Epoch 70 | Batch: 12 | Loss:  15.2953\n","Epoch 70 | Batch: 13 | Loss:  15.7997\n","Epoch 70 | Batch: 14 | Loss:  15.4740\n","Epoch 70 | Batch: 15 | Loss:  18.5350\n","Epoch 70 | Batch: 16 | Loss:  18.1538\n","Epoch 70 | Batch: 17 | Loss:  15.6479\n","Epoch 70 | Batch: 18 | Loss:  7.4682\n","Epoch 70 | Batch: 19 | Loss:  16.6537\n","Epoch 70 | Batch: 20 | Loss:  10.9644\n","Epoch 70 | Batch: 21 | Loss:  16.5409\n","Epoch 70 | Batch: 22 | Loss:  17.3918\n","Epoch 70 | Batch: 23 | Loss:  18.3995\n","Epoch 70 | Batch: 24 | Loss:  9.3622\n","Epoch 71 | Batch: 1 | Loss:  15.8384\n","Epoch 71 | Batch: 2 | Loss:  11.4643\n","Epoch 71 | Batch: 3 | Loss:  18.3932\n","Epoch 71 | Batch: 4 | Loss:  15.9074\n","Epoch 71 | Batch: 5 | Loss:  22.0049\n","Epoch 71 | Batch: 6 | Loss:  20.1471\n","Epoch 71 | Batch: 7 | Loss:  7.8196\n","Epoch 71 | Batch: 8 | Loss:  11.6568\n","Epoch 71 | Batch: 9 | Loss:  8.7808\n","Epoch 71 | Batch: 10 | Loss:  18.8666\n","Epoch 71 | Batch: 11 | Loss:  19.2822\n","Epoch 71 | Batch: 12 | Loss:  19.3759\n","Epoch 71 | Batch: 13 | Loss:  15.2966\n","Epoch 71 | Batch: 14 | Loss:  13.8583\n","Epoch 71 | Batch: 15 | Loss:  18.1949\n","Epoch 71 | Batch: 16 | Loss:  14.1435\n","Epoch 71 | Batch: 17 | Loss:  18.1277\n","Epoch 71 | Batch: 18 | Loss:  11.5101\n","Epoch 71 | Batch: 19 | Loss:  16.0561\n","Epoch 71 | Batch: 20 | Loss:  16.8057\n","Epoch 71 | Batch: 21 | Loss:  13.6191\n","Epoch 71 | Batch: 22 | Loss:  18.4564\n","Epoch 71 | Batch: 23 | Loss:  11.6586\n","Epoch 71 | Batch: 24 | Loss:  11.4412\n","Epoch 72 | Batch: 1 | Loss:  13.6213\n","Epoch 72 | Batch: 2 | Loss:  12.9455\n","Epoch 72 | Batch: 3 | Loss:  12.7575\n","Epoch 72 | Batch: 4 | Loss:  16.6558\n","Epoch 72 | Batch: 5 | Loss:  20.4191\n","Epoch 72 | Batch: 6 | Loss:  14.2163\n","Epoch 72 | Batch: 7 | Loss:  19.1903\n","Epoch 72 | Batch: 8 | Loss:  18.6467\n","Epoch 72 | Batch: 9 | Loss:  13.1386\n","Epoch 72 | Batch: 10 | Loss:  13.2824\n","Epoch 72 | Batch: 11 | Loss:  12.2938\n","Epoch 72 | Batch: 12 | Loss:  12.0591\n","Epoch 72 | Batch: 13 | Loss:  27.2257\n","Epoch 72 | Batch: 14 | Loss:  17.9166\n","Epoch 72 | Batch: 15 | Loss:  14.8124\n","Epoch 72 | Batch: 16 | Loss:  13.2898\n","Epoch 72 | Batch: 17 | Loss:  12.2121\n","Epoch 72 | Batch: 18 | Loss:  9.1213\n","Epoch 72 | Batch: 19 | Loss:  18.5599\n","Epoch 72 | Batch: 20 | Loss:  15.1998\n","Epoch 72 | Batch: 21 | Loss:  15.5718\n","Epoch 72 | Batch: 22 | Loss:  13.1165\n","Epoch 72 | Batch: 23 | Loss:  16.1427\n","Epoch 72 | Batch: 24 | Loss:  9.5251\n","Epoch 73 | Batch: 1 | Loss:  15.7479\n","Epoch 73 | Batch: 2 | Loss:  13.7886\n","Epoch 73 | Batch: 3 | Loss:  11.5682\n","Epoch 73 | Batch: 4 | Loss:  9.7576\n","Epoch 73 | Batch: 5 | Loss:  14.3330\n","Epoch 73 | Batch: 6 | Loss:  12.8067\n","Epoch 73 | Batch: 7 | Loss:  13.6058\n","Epoch 73 | Batch: 8 | Loss:  15.6696\n","Epoch 73 | Batch: 9 | Loss:  17.0495\n","Epoch 73 | Batch: 10 | Loss:  17.2558\n","Epoch 73 | Batch: 11 | Loss:  23.3568\n","Epoch 73 | Batch: 12 | Loss:  24.0990\n","Epoch 73 | Batch: 13 | Loss:  13.4193\n","Epoch 73 | Batch: 14 | Loss:  17.0293\n","Epoch 73 | Batch: 15 | Loss:  13.4215\n","Epoch 73 | Batch: 16 | Loss:  14.5987\n","Epoch 73 | Batch: 17 | Loss:  10.5058\n","Epoch 73 | Batch: 18 | Loss:  20.7187\n","Epoch 73 | Batch: 19 | Loss:  22.0150\n","Epoch 73 | Batch: 20 | Loss:  17.9455\n","Epoch 73 | Batch: 21 | Loss:  14.6356\n","Epoch 73 | Batch: 22 | Loss:  13.2053\n","Epoch 73 | Batch: 23 | Loss:  13.9086\n","Epoch 73 | Batch: 24 | Loss:  10.2414\n","Epoch 74 | Batch: 1 | Loss:  20.3754\n","Epoch 74 | Batch: 2 | Loss:  15.5041\n","Epoch 74 | Batch: 3 | Loss:  13.4391\n","Epoch 74 | Batch: 4 | Loss:  15.7155\n","Epoch 74 | Batch: 5 | Loss:  16.8603\n","Epoch 74 | Batch: 6 | Loss:  16.2727\n","Epoch 74 | Batch: 7 | Loss:  12.8033\n","Epoch 74 | Batch: 8 | Loss:  13.9681\n","Epoch 74 | Batch: 9 | Loss:  17.8673\n","Epoch 74 | Batch: 10 | Loss:  12.3434\n","Epoch 74 | Batch: 11 | Loss:  11.8641\n","Epoch 74 | Batch: 12 | Loss:  18.0595\n","Epoch 74 | Batch: 13 | Loss:  16.0887\n","Epoch 74 | Batch: 14 | Loss:  18.7965\n","Epoch 74 | Batch: 15 | Loss:  14.9896\n","Epoch 74 | Batch: 16 | Loss:  11.9732\n","Epoch 74 | Batch: 17 | Loss:  17.7197\n","Epoch 74 | Batch: 18 | Loss:  11.4249\n","Epoch 74 | Batch: 19 | Loss:  10.5991\n","Epoch 74 | Batch: 20 | Loss:  23.1147\n","Epoch 74 | Batch: 21 | Loss:  15.5238\n","Epoch 74 | Batch: 22 | Loss:  17.2367\n","Epoch 74 | Batch: 23 | Loss:  12.4537\n","Epoch 74 | Batch: 24 | Loss:  8.8504\n","Epoch 75 | Batch: 1 | Loss:  15.3658\n","Epoch 75 | Batch: 2 | Loss:  11.9656\n","Epoch 75 | Batch: 3 | Loss:  11.2278\n","Epoch 75 | Batch: 4 | Loss:  10.4220\n","Epoch 75 | Batch: 5 | Loss:  15.1213\n","Epoch 75 | Batch: 6 | Loss:  15.9437\n","Epoch 75 | Batch: 7 | Loss:  12.6857\n","Epoch 75 | Batch: 8 | Loss:  15.3030\n","Epoch 75 | Batch: 9 | Loss:  14.6517\n","Epoch 75 | Batch: 10 | Loss:  20.1870\n","Epoch 75 | Batch: 11 | Loss:  18.8816\n","Epoch 75 | Batch: 12 | Loss:  18.8763\n","Epoch 75 | Batch: 13 | Loss:  15.3359\n","Epoch 75 | Batch: 14 | Loss:  14.5103\n","Epoch 75 | Batch: 15 | Loss:  12.9078\n","Epoch 75 | Batch: 16 | Loss:  15.8733\n","Epoch 75 | Batch: 17 | Loss:  16.7360\n","Epoch 75 | Batch: 18 | Loss:  11.6850\n","Epoch 75 | Batch: 19 | Loss:  18.7998\n","Epoch 75 | Batch: 20 | Loss:  13.7090\n","Epoch 75 | Batch: 21 | Loss:  9.7638\n","Epoch 75 | Batch: 22 | Loss:  13.1348\n","Epoch 75 | Batch: 23 | Loss:  21.6936\n","Epoch 75 | Batch: 24 | Loss:  11.0287\n","Epoch 76 | Batch: 1 | Loss:  13.3944\n","Epoch 76 | Batch: 2 | Loss:  11.5630\n","Epoch 76 | Batch: 3 | Loss:  15.3277\n","Epoch 76 | Batch: 4 | Loss:  19.1238\n","Epoch 76 | Batch: 5 | Loss:  20.1138\n","Epoch 76 | Batch: 6 | Loss:  18.7048\n","Epoch 76 | Batch: 7 | Loss:  15.0146\n","Epoch 76 | Batch: 8 | Loss:  15.9019\n","Epoch 76 | Batch: 9 | Loss:  17.6984\n","Epoch 76 | Batch: 10 | Loss:  12.1574\n","Epoch 76 | Batch: 11 | Loss:  11.7423\n","Epoch 76 | Batch: 12 | Loss:  15.3788\n","Epoch 76 | Batch: 13 | Loss:  11.8991\n","Epoch 76 | Batch: 14 | Loss:  19.0782\n","Epoch 76 | Batch: 15 | Loss:  10.1820\n","Epoch 76 | Batch: 16 | Loss:  16.5363\n","Epoch 76 | Batch: 17 | Loss:  12.2250\n","Epoch 76 | Batch: 18 | Loss:  11.0268\n","Epoch 76 | Batch: 19 | Loss:  10.1439\n","Epoch 76 | Batch: 20 | Loss:  10.7729\n","Epoch 76 | Batch: 21 | Loss:  17.8404\n","Epoch 76 | Batch: 22 | Loss:  13.3153\n","Epoch 76 | Batch: 23 | Loss:  24.2897\n","Epoch 76 | Batch: 24 | Loss:  10.9157\n","Epoch 77 | Batch: 1 | Loss:  17.6056\n","Epoch 77 | Batch: 2 | Loss:  16.5065\n","Epoch 77 | Batch: 3 | Loss:  17.6430\n","Epoch 77 | Batch: 4 | Loss:  15.2794\n","Epoch 77 | Batch: 5 | Loss:  18.4496\n","Epoch 77 | Batch: 6 | Loss:  12.8837\n","Epoch 77 | Batch: 7 | Loss:  12.7845\n","Epoch 77 | Batch: 8 | Loss:  13.1780\n","Epoch 77 | Batch: 9 | Loss:  15.7143\n","Epoch 77 | Batch: 10 | Loss:  15.1751\n","Epoch 77 | Batch: 11 | Loss:  15.7030\n","Epoch 77 | Batch: 12 | Loss:  15.7452\n","Epoch 77 | Batch: 13 | Loss:  21.0938\n","Epoch 77 | Batch: 14 | Loss:  13.7187\n","Epoch 77 | Batch: 15 | Loss:  11.3274\n","Epoch 77 | Batch: 16 | Loss:  15.9200\n","Epoch 77 | Batch: 17 | Loss:  12.2453\n","Epoch 77 | Batch: 18 | Loss:  14.2346\n","Epoch 77 | Batch: 19 | Loss:  17.3591\n","Epoch 77 | Batch: 20 | Loss:  17.9073\n","Epoch 77 | Batch: 21 | Loss:  10.1570\n","Epoch 77 | Batch: 22 | Loss:  10.6703\n","Epoch 77 | Batch: 23 | Loss:  15.1776\n","Epoch 77 | Batch: 24 | Loss:  12.6418\n","Epoch 78 | Batch: 1 | Loss:  12.3469\n","Epoch 78 | Batch: 2 | Loss:  10.3297\n","Epoch 78 | Batch: 3 | Loss:  12.0986\n","Epoch 78 | Batch: 4 | Loss:  14.2724\n","Epoch 78 | Batch: 5 | Loss:  10.7184\n","Epoch 78 | Batch: 6 | Loss:  13.8111\n","Epoch 78 | Batch: 7 | Loss:  15.9048\n","Epoch 78 | Batch: 8 | Loss:  13.7810\n","Epoch 78 | Batch: 9 | Loss:  15.5867\n","Epoch 78 | Batch: 10 | Loss:  10.0530\n","Epoch 78 | Batch: 11 | Loss:  16.7205\n","Epoch 78 | Batch: 12 | Loss:  14.7639\n","Epoch 78 | Batch: 13 | Loss:  17.4797\n","Epoch 78 | Batch: 14 | Loss:  13.1121\n","Epoch 78 | Batch: 15 | Loss:  17.1042\n","Epoch 78 | Batch: 16 | Loss:  20.5049\n","Epoch 78 | Batch: 17 | Loss:  16.5739\n","Epoch 78 | Batch: 18 | Loss:  18.0987\n","Epoch 78 | Batch: 19 | Loss:  14.4357\n","Epoch 78 | Batch: 20 | Loss:  16.7861\n","Epoch 78 | Batch: 21 | Loss:  18.9471\n","Epoch 78 | Batch: 22 | Loss:  15.9947\n","Epoch 78 | Batch: 23 | Loss:  19.9758\n","Epoch 78 | Batch: 24 | Loss:  5.6865\n","Epoch 79 | Batch: 1 | Loss:  16.5312\n","Epoch 79 | Batch: 2 | Loss:  16.1645\n","Epoch 79 | Batch: 3 | Loss:  14.0284\n","Epoch 79 | Batch: 4 | Loss:  12.3506\n","Epoch 79 | Batch: 5 | Loss:  14.8132\n","Epoch 79 | Batch: 6 | Loss:  17.3343\n","Epoch 79 | Batch: 7 | Loss:  11.8855\n","Epoch 79 | Batch: 8 | Loss:  22.4045\n","Epoch 79 | Batch: 9 | Loss:  18.8474\n","Epoch 79 | Batch: 10 | Loss:  18.6625\n","Epoch 79 | Batch: 11 | Loss:  11.1605\n","Epoch 79 | Batch: 12 | Loss:  14.6061\n","Epoch 79 | Batch: 13 | Loss:  21.0372\n","Epoch 79 | Batch: 14 | Loss:  15.1298\n","Epoch 79 | Batch: 15 | Loss:  14.3811\n","Epoch 79 | Batch: 16 | Loss:  15.3963\n","Epoch 79 | Batch: 17 | Loss:  16.6068\n","Epoch 79 | Batch: 18 | Loss:  14.0380\n","Epoch 79 | Batch: 19 | Loss:  16.1796\n","Epoch 79 | Batch: 20 | Loss:  13.6364\n","Epoch 79 | Batch: 21 | Loss:  9.9278\n","Epoch 79 | Batch: 22 | Loss:  15.1383\n","Epoch 79 | Batch: 23 | Loss:  11.7687\n","Epoch 79 | Batch: 24 | Loss:  11.3902\n","Epoch 80 | Batch: 1 | Loss:  15.2347\n","Epoch 80 | Batch: 2 | Loss:  14.7258\n","Epoch 80 | Batch: 3 | Loss:  12.4353\n","Epoch 80 | Batch: 4 | Loss:  16.4592\n","Epoch 80 | Batch: 5 | Loss:  21.3335\n","Epoch 80 | Batch: 6 | Loss:  13.0914\n","Epoch 80 | Batch: 7 | Loss:  13.0890\n","Epoch 80 | Batch: 8 | Loss:  15.5373\n","Epoch 80 | Batch: 9 | Loss:  16.8011\n","Epoch 80 | Batch: 10 | Loss:  14.3109\n","Epoch 80 | Batch: 11 | Loss:  16.7057\n","Epoch 80 | Batch: 12 | Loss:  14.0040\n","Epoch 80 | Batch: 13 | Loss:  13.6466\n","Epoch 80 | Batch: 14 | Loss:  9.8037\n","Epoch 80 | Batch: 15 | Loss:  19.9097\n","Epoch 80 | Batch: 16 | Loss:  12.1067\n","Epoch 80 | Batch: 17 | Loss:  18.3054\n","Epoch 80 | Batch: 18 | Loss:  16.7421\n","Epoch 80 | Batch: 19 | Loss:  16.1794\n","Epoch 80 | Batch: 20 | Loss:  16.8944\n","Epoch 80 | Batch: 21 | Loss:  13.4866\n","Epoch 80 | Batch: 22 | Loss:  10.3282\n","Epoch 80 | Batch: 23 | Loss:  12.6420\n","Epoch 80 | Batch: 24 | Loss:  14.4575\n","Epoch 81 | Batch: 1 | Loss:  11.3856\n","Epoch 81 | Batch: 2 | Loss:  15.0924\n","Epoch 81 | Batch: 3 | Loss:  12.2265\n","Epoch 81 | Batch: 4 | Loss:  10.9705\n","Epoch 81 | Batch: 5 | Loss:  18.3280\n","Epoch 81 | Batch: 6 | Loss:  14.3385\n","Epoch 81 | Batch: 7 | Loss:  13.2478\n","Epoch 81 | Batch: 8 | Loss:  17.6369\n","Epoch 81 | Batch: 9 | Loss:  18.7937\n","Epoch 81 | Batch: 10 | Loss:  16.2965\n","Epoch 81 | Batch: 11 | Loss:  15.6376\n","Epoch 81 | Batch: 12 | Loss:  11.2261\n","Epoch 81 | Batch: 13 | Loss:  13.4567\n","Epoch 81 | Batch: 14 | Loss:  17.2081\n","Epoch 81 | Batch: 15 | Loss:  19.2488\n","Epoch 81 | Batch: 16 | Loss:  17.9617\n","Epoch 81 | Batch: 17 | Loss:  11.6288\n","Epoch 81 | Batch: 18 | Loss:  15.2139\n","Epoch 81 | Batch: 19 | Loss:  20.1543\n","Epoch 81 | Batch: 20 | Loss:  15.9741\n","Epoch 81 | Batch: 21 | Loss:  23.1880\n","Epoch 81 | Batch: 22 | Loss:  13.6966\n","Epoch 81 | Batch: 23 | Loss:  13.9360\n","Epoch 81 | Batch: 24 | Loss:  8.9487\n","Epoch 82 | Batch: 1 | Loss:  15.2565\n","Epoch 82 | Batch: 2 | Loss:  10.6045\n","Epoch 82 | Batch: 3 | Loss:  13.7700\n","Epoch 82 | Batch: 4 | Loss:  18.0779\n","Epoch 82 | Batch: 5 | Loss:  16.0901\n","Epoch 82 | Batch: 6 | Loss:  10.1675\n","Epoch 82 | Batch: 7 | Loss:  15.9422\n","Epoch 82 | Batch: 8 | Loss:  10.5651\n","Epoch 82 | Batch: 9 | Loss:  11.2428\n","Epoch 82 | Batch: 10 | Loss:  22.7125\n","Epoch 82 | Batch: 11 | Loss:  16.5690\n","Epoch 82 | Batch: 12 | Loss:  19.9500\n","Epoch 82 | Batch: 13 | Loss:  14.6068\n","Epoch 82 | Batch: 14 | Loss:  11.8423\n","Epoch 82 | Batch: 15 | Loss:  18.7166\n","Epoch 82 | Batch: 16 | Loss:  14.7896\n","Epoch 82 | Batch: 17 | Loss:  20.9789\n","Epoch 82 | Batch: 18 | Loss:  11.0282\n","Epoch 82 | Batch: 19 | Loss:  13.3405\n","Epoch 82 | Batch: 20 | Loss:  12.0170\n","Epoch 82 | Batch: 21 | Loss:  19.1884\n","Epoch 82 | Batch: 22 | Loss:  9.5052\n","Epoch 82 | Batch: 23 | Loss:  21.2886\n","Epoch 82 | Batch: 24 | Loss:  13.1434\n","Epoch 83 | Batch: 1 | Loss:  14.4140\n","Epoch 83 | Batch: 2 | Loss:  12.1193\n","Epoch 83 | Batch: 3 | Loss:  11.2282\n","Epoch 83 | Batch: 4 | Loss:  11.7423\n","Epoch 83 | Batch: 5 | Loss:  12.0455\n","Epoch 83 | Batch: 6 | Loss:  18.6910\n","Epoch 83 | Batch: 7 | Loss:  14.6282\n","Epoch 83 | Batch: 8 | Loss:  17.0567\n","Epoch 83 | Batch: 9 | Loss:  19.9095\n","Epoch 83 | Batch: 10 | Loss:  16.2763\n","Epoch 83 | Batch: 11 | Loss:  14.9005\n","Epoch 83 | Batch: 12 | Loss:  21.3631\n","Epoch 83 | Batch: 13 | Loss:  16.2927\n","Epoch 83 | Batch: 14 | Loss:  12.4015\n","Epoch 83 | Batch: 15 | Loss:  19.8786\n","Epoch 83 | Batch: 16 | Loss:  14.4396\n","Epoch 83 | Batch: 17 | Loss:  18.4196\n","Epoch 83 | Batch: 18 | Loss:  18.4223\n","Epoch 83 | Batch: 19 | Loss:  15.6458\n","Epoch 83 | Batch: 20 | Loss:  16.0952\n","Epoch 83 | Batch: 21 | Loss:  17.9883\n","Epoch 83 | Batch: 22 | Loss:  9.9069\n","Epoch 83 | Batch: 23 | Loss:  17.4562\n","Epoch 83 | Batch: 24 | Loss:  9.6960\n","Epoch 84 | Batch: 1 | Loss:  14.2189\n","Epoch 84 | Batch: 2 | Loss:  14.5779\n","Epoch 84 | Batch: 3 | Loss:  19.1493\n","Epoch 84 | Batch: 4 | Loss:  11.9236\n","Epoch 84 | Batch: 5 | Loss:  17.2730\n","Epoch 84 | Batch: 6 | Loss:  17.0366\n","Epoch 84 | Batch: 7 | Loss:  13.6973\n","Epoch 84 | Batch: 8 | Loss:  13.0630\n","Epoch 84 | Batch: 9 | Loss:  13.8393\n","Epoch 84 | Batch: 10 | Loss:  12.5942\n","Epoch 84 | Batch: 11 | Loss:  16.7698\n","Epoch 84 | Batch: 12 | Loss:  18.6573\n","Epoch 84 | Batch: 13 | Loss:  12.3566\n","Epoch 84 | Batch: 14 | Loss:  15.6998\n","Epoch 84 | Batch: 15 | Loss:  18.7377\n","Epoch 84 | Batch: 16 | Loss:  15.6457\n","Epoch 84 | Batch: 17 | Loss:  17.2316\n","Epoch 84 | Batch: 18 | Loss:  14.2833\n","Epoch 84 | Batch: 19 | Loss:  12.2102\n","Epoch 84 | Batch: 20 | Loss:  11.3626\n","Epoch 84 | Batch: 21 | Loss:  14.2378\n","Epoch 84 | Batch: 22 | Loss:  11.4143\n","Epoch 84 | Batch: 23 | Loss:  12.2417\n","Epoch 84 | Batch: 24 | Loss:  13.2653\n","Epoch 85 | Batch: 1 | Loss:  16.8160\n","Epoch 85 | Batch: 2 | Loss:  12.8958\n","Epoch 85 | Batch: 3 | Loss:  12.8205\n","Epoch 85 | Batch: 4 | Loss:  14.5774\n","Epoch 85 | Batch: 5 | Loss:  10.5034\n","Epoch 85 | Batch: 6 | Loss:  11.4763\n","Epoch 85 | Batch: 7 | Loss:  15.1840\n","Epoch 85 | Batch: 8 | Loss:  18.6115\n","Epoch 85 | Batch: 9 | Loss:  11.0852\n","Epoch 85 | Batch: 10 | Loss:  14.4616\n","Epoch 85 | Batch: 11 | Loss:  15.7432\n","Epoch 85 | Batch: 12 | Loss:  16.8577\n","Epoch 85 | Batch: 13 | Loss:  15.8044\n","Epoch 85 | Batch: 14 | Loss:  19.5893\n","Epoch 85 | Batch: 15 | Loss:  15.0815\n","Epoch 85 | Batch: 16 | Loss:  15.7833\n","Epoch 85 | Batch: 17 | Loss:  17.5678\n","Epoch 85 | Batch: 18 | Loss:  19.0398\n","Epoch 85 | Batch: 19 | Loss:  9.5344\n","Epoch 85 | Batch: 20 | Loss:  16.3385\n","Epoch 85 | Batch: 21 | Loss:  17.3000\n","Epoch 85 | Batch: 22 | Loss:  13.9082\n","Epoch 85 | Batch: 23 | Loss:  14.8488\n","Epoch 85 | Batch: 24 | Loss:  8.8941\n","Epoch 86 | Batch: 1 | Loss:  12.2013\n","Epoch 86 | Batch: 2 | Loss:  12.9358\n","Epoch 86 | Batch: 3 | Loss:  14.5067\n","Epoch 86 | Batch: 4 | Loss:  15.7866\n","Epoch 86 | Batch: 5 | Loss:  18.3096\n","Epoch 86 | Batch: 6 | Loss:  15.8384\n","Epoch 86 | Batch: 7 | Loss:  14.0929\n","Epoch 86 | Batch: 8 | Loss:  11.3628\n","Epoch 86 | Batch: 9 | Loss:  15.3428\n","Epoch 86 | Batch: 10 | Loss:  16.5228\n","Epoch 86 | Batch: 11 | Loss:  15.0139\n","Epoch 86 | Batch: 12 | Loss:  11.1743\n","Epoch 86 | Batch: 13 | Loss:  11.2642\n","Epoch 86 | Batch: 14 | Loss:  13.9852\n","Epoch 86 | Batch: 15 | Loss:  11.1032\n","Epoch 86 | Batch: 16 | Loss:  19.4123\n","Epoch 86 | Batch: 17 | Loss:  19.4048\n","Epoch 86 | Batch: 18 | Loss:  21.9932\n","Epoch 86 | Batch: 19 | Loss:  18.0223\n","Epoch 86 | Batch: 20 | Loss:  16.7999\n","Epoch 86 | Batch: 21 | Loss:  12.6176\n","Epoch 86 | Batch: 22 | Loss:  15.1867\n","Epoch 86 | Batch: 23 | Loss:  11.2999\n","Epoch 86 | Batch: 24 | Loss:  7.6510\n","Epoch 87 | Batch: 1 | Loss:  11.6981\n","Epoch 87 | Batch: 2 | Loss:  13.5161\n","Epoch 87 | Batch: 3 | Loss:  11.3193\n","Epoch 87 | Batch: 4 | Loss:  17.7391\n","Epoch 87 | Batch: 5 | Loss:  16.1353\n","Epoch 87 | Batch: 6 | Loss:  19.3976\n","Epoch 87 | Batch: 7 | Loss:  10.9947\n","Epoch 87 | Batch: 8 | Loss:  16.5839\n","Epoch 87 | Batch: 9 | Loss:  16.2971\n","Epoch 87 | Batch: 10 | Loss:  15.0810\n","Epoch 87 | Batch: 11 | Loss:  17.6213\n","Epoch 87 | Batch: 12 | Loss:  13.6389\n","Epoch 87 | Batch: 13 | Loss:  15.4963\n","Epoch 87 | Batch: 14 | Loss:  19.5816\n","Epoch 87 | Batch: 15 | Loss:  11.7775\n","Epoch 87 | Batch: 16 | Loss:  10.9541\n","Epoch 87 | Batch: 17 | Loss:  13.1122\n","Epoch 87 | Batch: 18 | Loss:  12.5468\n","Epoch 87 | Batch: 19 | Loss:  20.5175\n","Epoch 87 | Batch: 20 | Loss:  19.8750\n","Epoch 87 | Batch: 21 | Loss:  12.9390\n","Epoch 87 | Batch: 22 | Loss:  13.5545\n","Epoch 87 | Batch: 23 | Loss:  11.0109\n","Epoch 87 | Batch: 24 | Loss:  15.2404\n","Epoch 88 | Batch: 1 | Loss:  15.6114\n","Epoch 88 | Batch: 2 | Loss:  12.9063\n","Epoch 88 | Batch: 3 | Loss:  14.3331\n","Epoch 88 | Batch: 4 | Loss:  12.3367\n","Epoch 88 | Batch: 5 | Loss:  16.8680\n","Epoch 88 | Batch: 6 | Loss:  17.7881\n","Epoch 88 | Batch: 7 | Loss:  16.9737\n","Epoch 88 | Batch: 8 | Loss:  12.6811\n","Epoch 88 | Batch: 9 | Loss:  16.5459\n","Epoch 88 | Batch: 10 | Loss:  15.0945\n","Epoch 88 | Batch: 11 | Loss:  19.0357\n","Epoch 88 | Batch: 12 | Loss:  11.7624\n","Epoch 88 | Batch: 13 | Loss:  11.2624\n","Epoch 88 | Batch: 14 | Loss:  16.1594\n","Epoch 88 | Batch: 15 | Loss:  11.8820\n","Epoch 88 | Batch: 16 | Loss:  12.9422\n","Epoch 88 | Batch: 17 | Loss:  13.1081\n","Epoch 88 | Batch: 18 | Loss:  14.5874\n","Epoch 88 | Batch: 19 | Loss:  16.5681\n","Epoch 88 | Batch: 20 | Loss:  20.4740\n","Epoch 88 | Batch: 21 | Loss:  20.5246\n","Epoch 88 | Batch: 22 | Loss:  14.5940\n","Epoch 88 | Batch: 23 | Loss:  10.9415\n","Epoch 88 | Batch: 24 | Loss:  9.1515\n","Epoch 89 | Batch: 1 | Loss:  15.3762\n","Epoch 89 | Batch: 2 | Loss:  14.1845\n","Epoch 89 | Batch: 3 | Loss:  13.7769\n","Epoch 89 | Batch: 4 | Loss:  12.2245\n","Epoch 89 | Batch: 5 | Loss:  11.8491\n","Epoch 89 | Batch: 6 | Loss:  10.2239\n","Epoch 89 | Batch: 7 | Loss:  12.5472\n","Epoch 89 | Batch: 8 | Loss:  15.4787\n","Epoch 89 | Batch: 9 | Loss:  18.1553\n","Epoch 89 | Batch: 10 | Loss:  19.0077\n","Epoch 89 | Batch: 11 | Loss:  9.4697\n","Epoch 89 | Batch: 12 | Loss:  11.2980\n","Epoch 89 | Batch: 13 | Loss:  22.8636\n","Epoch 89 | Batch: 14 | Loss:  14.6356\n","Epoch 89 | Batch: 15 | Loss:  14.0565\n","Epoch 89 | Batch: 16 | Loss:  16.4063\n","Epoch 89 | Batch: 17 | Loss:  12.3809\n","Epoch 89 | Batch: 18 | Loss:  15.0521\n","Epoch 89 | Batch: 19 | Loss:  25.8663\n","Epoch 89 | Batch: 20 | Loss:  13.0292\n","Epoch 89 | Batch: 21 | Loss:  13.6992\n","Epoch 89 | Batch: 22 | Loss:  17.7162\n","Epoch 89 | Batch: 23 | Loss:  19.8228\n","Epoch 89 | Batch: 24 | Loss:  12.3248\n","Epoch 90 | Batch: 1 | Loss:  14.8929\n","Epoch 90 | Batch: 2 | Loss:  12.5926\n","Epoch 90 | Batch: 3 | Loss:  11.5851\n","Epoch 90 | Batch: 4 | Loss:  15.0138\n","Epoch 90 | Batch: 5 | Loss:  10.2195\n","Epoch 90 | Batch: 6 | Loss:  16.4421\n","Epoch 90 | Batch: 7 | Loss:  19.1934\n","Epoch 90 | Batch: 8 | Loss:  15.8356\n","Epoch 90 | Batch: 9 | Loss:  12.3361\n","Epoch 90 | Batch: 10 | Loss:  17.5171\n","Epoch 90 | Batch: 11 | Loss:  13.1720\n","Epoch 90 | Batch: 12 | Loss:  11.3610\n","Epoch 90 | Batch: 13 | Loss:  13.7632\n","Epoch 90 | Batch: 14 | Loss:  18.2685\n","Epoch 90 | Batch: 15 | Loss:  13.6492\n","Epoch 90 | Batch: 16 | Loss:  13.3283\n","Epoch 90 | Batch: 17 | Loss:  21.8097\n","Epoch 90 | Batch: 18 | Loss:  12.0486\n","Epoch 90 | Batch: 19 | Loss:  17.2745\n","Epoch 90 | Batch: 20 | Loss:  17.6188\n","Epoch 90 | Batch: 21 | Loss:  16.2296\n","Epoch 90 | Batch: 22 | Loss:  16.3896\n","Epoch 90 | Batch: 23 | Loss:  12.2280\n","Epoch 90 | Batch: 24 | Loss:  11.1178\n","Epoch 91 | Batch: 1 | Loss:  11.5788\n","Epoch 91 | Batch: 2 | Loss:  19.3494\n","Epoch 91 | Batch: 3 | Loss:  15.3515\n","Epoch 91 | Batch: 4 | Loss:  19.4010\n","Epoch 91 | Batch: 5 | Loss:  11.2197\n","Epoch 91 | Batch: 6 | Loss:  12.3963\n","Epoch 91 | Batch: 7 | Loss:  13.7240\n","Epoch 91 | Batch: 8 | Loss:  10.8817\n","Epoch 91 | Batch: 9 | Loss:  20.8446\n","Epoch 91 | Batch: 10 | Loss:  14.2201\n","Epoch 91 | Batch: 11 | Loss:  17.2363\n","Epoch 91 | Batch: 12 | Loss:  18.5531\n","Epoch 91 | Batch: 13 | Loss:  12.4753\n","Epoch 91 | Batch: 14 | Loss:  13.6548\n","Epoch 91 | Batch: 15 | Loss:  10.9644\n","Epoch 91 | Batch: 16 | Loss:  15.7709\n","Epoch 91 | Batch: 17 | Loss:  15.1518\n","Epoch 91 | Batch: 18 | Loss:  8.4141\n","Epoch 91 | Batch: 19 | Loss:  16.6594\n","Epoch 91 | Batch: 20 | Loss:  13.2640\n","Epoch 91 | Batch: 21 | Loss:  14.1188\n","Epoch 91 | Batch: 22 | Loss:  17.3110\n","Epoch 91 | Batch: 23 | Loss:  10.6278\n","Epoch 91 | Batch: 24 | Loss:  11.9425\n","Epoch 92 | Batch: 1 | Loss:  14.6036\n","Epoch 92 | Batch: 2 | Loss:  13.4644\n","Epoch 92 | Batch: 3 | Loss:  19.0655\n","Epoch 92 | Batch: 4 | Loss:  9.3450\n","Epoch 92 | Batch: 5 | Loss:  17.5894\n","Epoch 92 | Batch: 6 | Loss:  15.6281\n","Epoch 92 | Batch: 7 | Loss:  19.1035\n","Epoch 92 | Batch: 8 | Loss:  14.3203\n","Epoch 92 | Batch: 9 | Loss:  16.0071\n","Epoch 92 | Batch: 10 | Loss:  16.4416\n","Epoch 92 | Batch: 11 | Loss:  17.6550\n","Epoch 92 | Batch: 12 | Loss:  11.1806\n","Epoch 92 | Batch: 13 | Loss:  13.7334\n","Epoch 92 | Batch: 14 | Loss:  15.6403\n","Epoch 92 | Batch: 15 | Loss:  13.2462\n","Epoch 92 | Batch: 16 | Loss:  8.2098\n","Epoch 92 | Batch: 17 | Loss:  18.5026\n","Epoch 92 | Batch: 18 | Loss:  18.3171\n","Epoch 92 | Batch: 19 | Loss:  12.6902\n","Epoch 92 | Batch: 20 | Loss:  12.2904\n","Epoch 92 | Batch: 21 | Loss:  20.9578\n","Epoch 92 | Batch: 22 | Loss:  10.3518\n","Epoch 92 | Batch: 23 | Loss:  21.0060\n","Epoch 92 | Batch: 24 | Loss:  9.0929\n","Epoch 93 | Batch: 1 | Loss:  17.1813\n","Epoch 93 | Batch: 2 | Loss:  13.6220\n","Epoch 93 | Batch: 3 | Loss:  15.8747\n","Epoch 93 | Batch: 4 | Loss:  17.4652\n","Epoch 93 | Batch: 5 | Loss:  18.1408\n","Epoch 93 | Batch: 6 | Loss:  15.4741\n","Epoch 93 | Batch: 7 | Loss:  12.1417\n","Epoch 93 | Batch: 8 | Loss:  14.8314\n","Epoch 93 | Batch: 9 | Loss:  15.6157\n","Epoch 93 | Batch: 10 | Loss:  22.5660\n","Epoch 93 | Batch: 11 | Loss:  18.0195\n","Epoch 93 | Batch: 12 | Loss:  18.2604\n","Epoch 93 | Batch: 13 | Loss:  13.7992\n","Epoch 93 | Batch: 14 | Loss:  15.3262\n","Epoch 93 | Batch: 15 | Loss:  17.0033\n","Epoch 93 | Batch: 16 | Loss:  8.9595\n","Epoch 93 | Batch: 17 | Loss:  8.8803\n","Epoch 93 | Batch: 18 | Loss:  11.5620\n","Epoch 93 | Batch: 19 | Loss:  16.0214\n","Epoch 93 | Batch: 20 | Loss:  10.5459\n","Epoch 93 | Batch: 21 | Loss:  17.5507\n","Epoch 93 | Batch: 22 | Loss:  11.4153\n","Epoch 93 | Batch: 23 | Loss:  13.8037\n","Epoch 93 | Batch: 24 | Loss:  9.1273\n","Epoch 94 | Batch: 1 | Loss:  8.6473\n","Epoch 94 | Batch: 2 | Loss:  15.9988\n","Epoch 94 | Batch: 3 | Loss:  13.5453\n","Epoch 94 | Batch: 4 | Loss:  16.5810\n","Epoch 94 | Batch: 5 | Loss:  13.2085\n","Epoch 94 | Batch: 6 | Loss:  21.9531\n","Epoch 94 | Batch: 7 | Loss:  17.1382\n","Epoch 94 | Batch: 8 | Loss:  17.0517\n","Epoch 94 | Batch: 9 | Loss:  14.9430\n","Epoch 94 | Batch: 10 | Loss:  11.7156\n","Epoch 94 | Batch: 11 | Loss:  17.6632\n","Epoch 94 | Batch: 12 | Loss:  13.3519\n","Epoch 94 | Batch: 13 | Loss:  17.1504\n","Epoch 94 | Batch: 14 | Loss:  15.1612\n","Epoch 94 | Batch: 15 | Loss:  13.9434\n","Epoch 94 | Batch: 16 | Loss:  13.7919\n","Epoch 94 | Batch: 17 | Loss:  14.4733\n","Epoch 94 | Batch: 18 | Loss:  14.0386\n","Epoch 94 | Batch: 19 | Loss:  12.3049\n","Epoch 94 | Batch: 20 | Loss:  18.8575\n","Epoch 94 | Batch: 21 | Loss:  12.1304\n","Epoch 94 | Batch: 22 | Loss:  8.6995\n","Epoch 94 | Batch: 23 | Loss:  15.9117\n","Epoch 94 | Batch: 24 | Loss:  9.1744\n","Epoch 95 | Batch: 1 | Loss:  13.7929\n","Epoch 95 | Batch: 2 | Loss:  21.3443\n","Epoch 95 | Batch: 3 | Loss:  16.2373\n","Epoch 95 | Batch: 4 | Loss:  14.2048\n","Epoch 95 | Batch: 5 | Loss:  9.5790\n","Epoch 95 | Batch: 6 | Loss:  16.1934\n","Epoch 95 | Batch: 7 | Loss:  11.9342\n","Epoch 95 | Batch: 8 | Loss:  19.8748\n","Epoch 95 | Batch: 9 | Loss:  12.2563\n","Epoch 95 | Batch: 10 | Loss:  18.7809\n","Epoch 95 | Batch: 11 | Loss:  12.2639\n","Epoch 95 | Batch: 12 | Loss:  8.7404\n","Epoch 95 | Batch: 13 | Loss:  12.6082\n","Epoch 95 | Batch: 14 | Loss:  14.3844\n","Epoch 95 | Batch: 15 | Loss:  18.3851\n","Epoch 95 | Batch: 16 | Loss:  15.1698\n","Epoch 95 | Batch: 17 | Loss:  18.0248\n","Epoch 95 | Batch: 18 | Loss:  15.8047\n","Epoch 95 | Batch: 19 | Loss:  14.3330\n","Epoch 95 | Batch: 20 | Loss:  12.7736\n","Epoch 95 | Batch: 21 | Loss:  13.1126\n","Epoch 95 | Batch: 22 | Loss:  15.8086\n","Epoch 95 | Batch: 23 | Loss:  14.6945\n","Epoch 95 | Batch: 24 | Loss:  12.3807\n","Epoch 96 | Batch: 1 | Loss:  14.8337\n","Epoch 96 | Batch: 2 | Loss:  15.4624\n","Epoch 96 | Batch: 3 | Loss:  14.8639\n","Epoch 96 | Batch: 4 | Loss:  11.4112\n","Epoch 96 | Batch: 5 | Loss:  17.2476\n","Epoch 96 | Batch: 6 | Loss:  13.1473\n","Epoch 96 | Batch: 7 | Loss:  16.5388\n","Epoch 96 | Batch: 8 | Loss:  8.1810\n","Epoch 96 | Batch: 9 | Loss:  12.2375\n","Epoch 96 | Batch: 10 | Loss:  19.6378\n","Epoch 96 | Batch: 11 | Loss:  20.4076\n","Epoch 96 | Batch: 12 | Loss:  12.8344\n","Epoch 96 | Batch: 13 | Loss:  16.7492\n","Epoch 96 | Batch: 14 | Loss:  10.1915\n","Epoch 96 | Batch: 15 | Loss:  12.9173\n","Epoch 96 | Batch: 16 | Loss:  16.0783\n","Epoch 96 | Batch: 17 | Loss:  16.6439\n","Epoch 96 | Batch: 18 | Loss:  17.3907\n","Epoch 96 | Batch: 19 | Loss:  12.6121\n","Epoch 96 | Batch: 20 | Loss:  17.6374\n","Epoch 96 | Batch: 21 | Loss:  16.4199\n","Epoch 96 | Batch: 22 | Loss:  11.6472\n","Epoch 96 | Batch: 23 | Loss:  13.7320\n","Epoch 96 | Batch: 24 | Loss:  20.8510\n","Epoch 97 | Batch: 1 | Loss:  17.5614\n","Epoch 97 | Batch: 2 | Loss:  17.0358\n","Epoch 97 | Batch: 3 | Loss:  22.5568\n","Epoch 97 | Batch: 4 | Loss:  14.8031\n","Epoch 97 | Batch: 5 | Loss:  10.9434\n","Epoch 97 | Batch: 6 | Loss:  19.5617\n","Epoch 97 | Batch: 7 | Loss:  16.2501\n","Epoch 97 | Batch: 8 | Loss:  17.2511\n","Epoch 97 | Batch: 9 | Loss:  10.8455\n","Epoch 97 | Batch: 10 | Loss:  12.1631\n","Epoch 97 | Batch: 11 | Loss:  14.1827\n","Epoch 97 | Batch: 12 | Loss:  18.8168\n","Epoch 97 | Batch: 13 | Loss:  14.2882\n","Epoch 97 | Batch: 14 | Loss:  17.0592\n","Epoch 97 | Batch: 15 | Loss:  14.4358\n","Epoch 97 | Batch: 16 | Loss:  16.7706\n","Epoch 97 | Batch: 17 | Loss:  10.7767\n","Epoch 97 | Batch: 18 | Loss:  15.3130\n","Epoch 97 | Batch: 19 | Loss:  8.2695\n","Epoch 97 | Batch: 20 | Loss:  10.8401\n","Epoch 97 | Batch: 21 | Loss:  14.1187\n","Epoch 97 | Batch: 22 | Loss:  14.6329\n","Epoch 97 | Batch: 23 | Loss:  17.3865\n","Epoch 97 | Batch: 24 | Loss:  10.3926\n","Epoch 98 | Batch: 1 | Loss:  12.3712\n","Epoch 98 | Batch: 2 | Loss:  15.5454\n","Epoch 98 | Batch: 3 | Loss:  12.2972\n","Epoch 98 | Batch: 4 | Loss:  11.7257\n","Epoch 98 | Batch: 5 | Loss:  17.4460\n","Epoch 98 | Batch: 6 | Loss:  15.1477\n","Epoch 98 | Batch: 7 | Loss:  22.2926\n","Epoch 98 | Batch: 8 | Loss:  15.3303\n","Epoch 98 | Batch: 9 | Loss:  13.5664\n","Epoch 98 | Batch: 10 | Loss:  11.1812\n","Epoch 98 | Batch: 11 | Loss:  12.1137\n","Epoch 98 | Batch: 12 | Loss:  20.4326\n","Epoch 98 | Batch: 13 | Loss:  16.7684\n","Epoch 98 | Batch: 14 | Loss:  9.9247\n","Epoch 98 | Batch: 15 | Loss:  19.1194\n","Epoch 98 | Batch: 16 | Loss:  11.1442\n","Epoch 98 | Batch: 17 | Loss:  9.5881\n","Epoch 98 | Batch: 18 | Loss:  11.6593\n","Epoch 98 | Batch: 19 | Loss:  16.6267\n","Epoch 98 | Batch: 20 | Loss:  17.8723\n","Epoch 98 | Batch: 21 | Loss:  17.0549\n","Epoch 98 | Batch: 22 | Loss:  12.9086\n","Epoch 98 | Batch: 23 | Loss:  21.2587\n","Epoch 98 | Batch: 24 | Loss:  10.7998\n","Epoch 99 | Batch: 1 | Loss:  19.1248\n","Epoch 99 | Batch: 2 | Loss:  12.8948\n","Epoch 99 | Batch: 3 | Loss:  19.4204\n","Epoch 99 | Batch: 4 | Loss:  17.1906\n","Epoch 99 | Batch: 5 | Loss:  15.3498\n","Epoch 99 | Batch: 6 | Loss:  12.6451\n","Epoch 99 | Batch: 7 | Loss:  13.9566\n","Epoch 99 | Batch: 8 | Loss:  15.6567\n","Epoch 99 | Batch: 9 | Loss:  15.1782\n","Epoch 99 | Batch: 10 | Loss:  13.3323\n","Epoch 99 | Batch: 11 | Loss:  12.5541\n","Epoch 99 | Batch: 12 | Loss:  17.8441\n","Epoch 99 | Batch: 13 | Loss:  14.7895\n","Epoch 99 | Batch: 14 | Loss:  16.0102\n","Epoch 99 | Batch: 15 | Loss:  12.2596\n","Epoch 99 | Batch: 16 | Loss:  13.1995\n","Epoch 99 | Batch: 17 | Loss:  18.6124\n","Epoch 99 | Batch: 18 | Loss:  16.1246\n","Epoch 99 | Batch: 19 | Loss:  17.0446\n","Epoch 99 | Batch: 20 | Loss:  12.2784\n","Epoch 99 | Batch: 21 | Loss:  10.3927\n","Epoch 99 | Batch: 22 | Loss:  12.0016\n","Epoch 99 | Batch: 23 | Loss:  15.1557\n","Epoch 99 | Batch: 24 | Loss:  11.5936\n","Epoch 100 | Batch: 1 | Loss:  15.5974\n","Epoch 100 | Batch: 2 | Loss:  18.4898\n","Epoch 100 | Batch: 3 | Loss:  14.9786\n","Epoch 100 | Batch: 4 | Loss:  9.6806\n","Epoch 100 | Batch: 5 | Loss:  18.9045\n","Epoch 100 | Batch: 6 | Loss:  13.1821\n","Epoch 100 | Batch: 7 | Loss:  15.8760\n","Epoch 100 | Batch: 8 | Loss:  13.5877\n","Epoch 100 | Batch: 9 | Loss:  13.3359\n","Epoch 100 | Batch: 10 | Loss:  12.8894\n","Epoch 100 | Batch: 11 | Loss:  12.1434\n","Epoch 100 | Batch: 12 | Loss:  20.6029\n","Epoch 100 | Batch: 13 | Loss:  14.4167\n","Epoch 100 | Batch: 14 | Loss:  17.9895\n","Epoch 100 | Batch: 15 | Loss:  13.5662\n","Epoch 100 | Batch: 16 | Loss:  13.3115\n","Epoch 100 | Batch: 17 | Loss:  14.8964\n","Epoch 100 | Batch: 18 | Loss:  10.2527\n","Epoch 100 | Batch: 19 | Loss:  16.3500\n","Epoch 100 | Batch: 20 | Loss:  15.1853\n","Epoch 100 | Batch: 21 | Loss:  18.8956\n","Epoch 100 | Batch: 22 | Loss:  13.8026\n","Epoch 100 | Batch: 23 | Loss:  14.4138\n","Epoch 100 | Batch: 24 | Loss:  10.2608\n"]}]},{"cell_type":"markdown","metadata":{"id":"3xcT7Lgt1jXq"},"source":["## PytorchZeroToAll. Lec09"]},{"cell_type":"code","metadata":{"id":"oEK1vLta1j_-","executionInfo":{"status":"ok","timestamp":1632757253110,"user_tz":-540,"elapsed":41,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["from __future__ import print_function\n","from torch import nn, optim, cuda\n","from torch.utils import data\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","import time"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AA-OGSjj3tww","executionInfo":{"status":"ok","timestamp":1632757253111,"user_tz":-540,"elapsed":40,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"faa7eef7-53d4-4158-9f5d-1adaee0f6c70"},"source":["batch_size = 64\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n","\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./mnist_data/',\n","                               train=True,\n","                               transform=transforms.ToTensor(),\n","                               download=True)\n","\n","test_dataset = datasets.MNIST(root='./mnist_data/',\n","                              train=False,\n","                              transform=transforms.ToTensor())\n","\n","# Data Loader (Input Pipeline)\n","train_loader = data.DataLoader(dataset=train_dataset, \n","                               batch_size=batch_size, \n","                               shuffle=True)\n","\n","test_loader = data.DataLoader(dataset=test_dataset, \n","                              batch_size=batch_size, \n","                              shuffle=False)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Training MNIST Model on cuda\n","============================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"]}]},{"cell_type":"code","metadata":{"id":"plj19HCk4sJi","executionInfo":{"status":"ok","timestamp":1632757255241,"user_tz":-540,"elapsed":2138,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["class Net(nn.Module):\n","\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.l1 = nn.Linear(784, 520)\n","    self.l2 = nn.Linear(520, 320)\n","    self.l3 = nn.Linear(320, 240)\n","    self.l4 = nn.Linear(240, 120)\n","    self.l5 = nn.Linear(120, 10)\n","\n","  def forward(self, x):\n","    # Flatten the data (n, 1, 28, 28) -> (n, 784)\n","    x = x.view(-1, 784)\n","    x = F.relu(self.l1(x))\n","    x = F.relu(self.l2(x))\n","    x = F.relu(self.l3(x))\n","    x = F.relu(self.l4(x))\n","    return self.l5(x) # No need activation\n","\n","model = Net()\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4BuU61247H0","executionInfo":{"status":"ok","timestamp":1632757255242,"user_tz":-540,"elapsed":13,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["def train(epoch):\n","  model.train()\n","  for batch_idx, (data, target) in enumerate(train_loader):\n","    data, target = data.to(device), target.to(device)\n","    optimizer.zero_grad()\n","    output = model(data)\n","    loss = criterion(output, target) \n","    loss.backward()\n","    optimizer.step()\n","    if batch_idx % 10 == 0:\n","      print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"GotQAdaF5Kff","executionInfo":{"status":"ok","timestamp":1632757255243,"user_tz":-540,"elapsed":12,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["def test():\n","  model.eval()\n","  test_loss = 0\n","  correct = 0\n","  for data, target in test_loader:\n","    data, target = data.to(device), target.to(device)\n","    output = model(data)\n","    # sum up batch loss\n","    test_loss += criterion(output, target).item()\n","    # get the index of the max\n","    pred = output.data.max(1, keepdim=True)[1]\n","    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","  test_loss /= len(test_loader.dataset)\n","  print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n","          f'({100. * correct / len(test_loader.dataset):.0f}%)')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ufot97yw5Oqs","executionInfo":{"status":"error","timestamp":1632757270534,"user_tz":-540,"elapsed":15301,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"837161b1-22b2-45be-c4e4-138d333769d2"},"source":["if __name__ == '__main__':\n","    since = time.time()\n","    for epoch in range(1,10):\n","      epoch_start = time.time()\n","      train(epoch)\n","      m, s = divmod(time.time() - epoch_start, 60)\n","      print(f'Training time: {m:.0f}m {s:.0f}s')\n","      test()\n","      m, s = divmod(time.time() - epoch_start, 60)\n","      print(f'Testing time: {m:.0f}m {s:.0f}s')\n","\n","    m, s = divmod(time.time() - since, 60)\n","    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.308764\n","Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.304667\n","Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.303483\n","Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.298565\n","Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.304631\n","Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.307826\n","Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.305692\n","Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.296519\n","Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.313386\n","Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.303475\n","Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.303573\n","Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.303077\n","Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.297778\n","Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.287597\n","Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.288209\n","Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.303378\n","Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.286193\n","Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.305990\n","Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.291062\n","Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.304578\n","Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.294863\n","Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.301984\n","Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.288330\n","Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.282544\n","Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.287009\n","Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.294093\n","Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.287493\n","Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.293876\n","Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.292469\n","Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.282327\n","Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.287408\n","Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.292064\n","Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.289453\n","Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.284397\n","Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.297941\n","Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.277152\n","Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.285858\n","Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.280701\n","Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.286843\n","Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.285439\n","Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.282830\n","Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.285285\n","Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.277468\n","Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.275673\n","Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.270580\n","Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.271829\n","Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.262055\n","Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.275879\n","Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.287445\n","Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.269004\n","Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.258859\n","Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.277206\n","Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.258178\n","Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.260376\n","Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.261195\n","Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.250823\n","Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.259248\n","Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.246943\n","Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.240851\n","Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.240818\n","Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.258685\n","Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.235485\n","Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.224584\n","Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.214392\n","Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.217319\n","Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.210095\n","Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.199297\n","Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.192276\n","Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.159553\n","Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.143708\n","Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.120531\n","Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.113513\n","Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.127715\n","Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.063063\n","Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.021540\n","Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.062465\n","Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 1.926399\n","Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 1.950133\n","Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 1.918302\n","Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 1.801779\n","Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 1.889760\n","Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 1.821749\n","Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 1.747348\n","Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 1.778443\n","Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 1.641566\n","Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 1.745024\n","Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 1.689838\n","Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 1.557748\n","Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 1.613911\n","Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 1.474704\n","Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 1.579227\n","Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 1.384276\n","Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 1.475827\n","Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 1.341682\n","Training time: 0m 10s\n","===========================\n","Test set: Average loss: 0.0210, Accuracy: 5422/10000 (54%)\n","Testing time: 0m 11s\n","Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.479406\n","Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 1.368841\n","Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 1.454959\n","Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 1.182219\n","Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 1.272762\n","Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 1.017446\n","Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 1.133179\n","Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 1.118798\n","Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 1.243198\n","Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 1.260277\n","Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 1.106446\n","Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 1.217410\n","Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 0.996334\n","Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 0.868106\n","Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 1.121514\n","Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 0.972590\n","Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 1.121693\n","Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 0.837251\n","Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.984896\n","Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 0.971202\n","Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.829733\n","Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 0.990449\n","Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.780640\n","Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 1.003142\n","Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.927081\n","Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.938775\n","Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.893715\n","Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.778658\n","Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.681469\n","Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.856384\n","Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.715804\n","Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.672335\n","Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.482904\n","Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.573508\n","Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.671550\n","Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.680749\n","Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.617035\n","Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.518365\n","Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.805306\n","Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.589142\n","Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.750200\n","Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.768400\n","Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.751389\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-08b1232192b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training time: {m:.0f}m {s:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-c63ca40ac3e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"aNZ6cBrR5m52","executionInfo":{"status":"aborted","timestamp":1632757270532,"user_tz":-540,"elapsed":14,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":[""],"execution_count":null,"outputs":[]}]}
