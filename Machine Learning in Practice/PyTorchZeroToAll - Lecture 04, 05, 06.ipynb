{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture06.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN63F+LeOy1VmTpDgWHRoQw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iRUyLsLzkHJ2"},"source":["## PytorchZeroToAll. Lec04"]},{"cell_type":"code","metadata":{"id":"x2J2B5-Vlnrs","executionInfo":{"status":"ok","timestamp":1632754541213,"user_tz":-540,"elapsed":873,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","x_data = [1.0,2.0,3.0]\n","y_data = [2.0,4.0,6.0]\n","\n","w = torch.tensor([1.0], requires_grad=True)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"k97HSBH4lrPB","executionInfo":{"status":"ok","timestamp":1632754541215,"user_tz":-540,"elapsed":21,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["# our model forward pass\n","def forward(x):\n","  return x * w\n","\n","# loss function\n","def loss(y_pred, y_val):\n","    return (y_pred - y_val) ** 2"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJIQxV7skDLG","executionInfo":{"status":"ok","timestamp":1632754541219,"user_tz":-540,"elapsed":24,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"a1865207-a80c-4cb6-edf1-814b95077244"},"source":["# before training\n","print('predict (before training)', 4, forward(4).item())\n","\n","for epoch in range(10):\n","  for x_val, y_val in zip(x_data, y_data):\n","    y_pred = forward(x_val) # 1) Forward pass\n","    l = loss(y_pred, y_val) # 2) Compute loss\n","    l.backward() # 3) Back Propagation to update weights \n","    print('\\tgrad: ', x_val, y_val, w.grad.item())\n","    w.data = w.data - 0.01 * w.grad.item()\n","\n","    # Manually zero the gradients after updating weights\n","    w.grad.data.zero_()\n","\n","  print(f'Epoch: {epoch} | Loss: {l.item()}')\n","\n","# After training\n","print('Prediction (after training)', 4, forward(4).item())"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["predict (before training) 4 4.0\n","\tgrad:  1.0 2.0 -2.0\n","\tgrad:  2.0 4.0 -7.840000152587891\n","\tgrad:  3.0 6.0 -16.228801727294922\n","Epoch: 0 | Loss: 7.315943717956543\n","\tgrad:  1.0 2.0 -1.478623867034912\n","\tgrad:  2.0 4.0 -5.796205520629883\n","\tgrad:  3.0 6.0 -11.998146057128906\n","Epoch: 1 | Loss: 3.9987640380859375\n","\tgrad:  1.0 2.0 -1.0931644439697266\n","\tgrad:  2.0 4.0 -4.285204887390137\n","\tgrad:  3.0 6.0 -8.870372772216797\n","Epoch: 2 | Loss: 2.1856532096862793\n","\tgrad:  1.0 2.0 -0.8081896305084229\n","\tgrad:  2.0 4.0 -3.1681032180786133\n","\tgrad:  3.0 6.0 -6.557973861694336\n","Epoch: 3 | Loss: 1.1946394443511963\n","\tgrad:  1.0 2.0 -0.5975041389465332\n","\tgrad:  2.0 4.0 -2.3422164916992188\n","\tgrad:  3.0 6.0 -4.848389625549316\n","Epoch: 4 | Loss: 0.6529689431190491\n","\tgrad:  1.0 2.0 -0.4417421817779541\n","\tgrad:  2.0 4.0 -1.7316293716430664\n","\tgrad:  3.0 6.0 -3.58447265625\n","Epoch: 5 | Loss: 0.35690122842788696\n","\tgrad:  1.0 2.0 -0.3265852928161621\n","\tgrad:  2.0 4.0 -1.2802143096923828\n","\tgrad:  3.0 6.0 -2.650045394897461\n","Epoch: 6 | Loss: 0.195076122879982\n","\tgrad:  1.0 2.0 -0.24144840240478516\n","\tgrad:  2.0 4.0 -0.9464778900146484\n","\tgrad:  3.0 6.0 -1.9592113494873047\n","Epoch: 7 | Loss: 0.10662525147199631\n","\tgrad:  1.0 2.0 -0.17850565910339355\n","\tgrad:  2.0 4.0 -0.699742317199707\n","\tgrad:  3.0 6.0 -1.4484672546386719\n","Epoch: 8 | Loss: 0.0582793727517128\n","\tgrad:  1.0 2.0 -0.1319713592529297\n","\tgrad:  2.0 4.0 -0.5173273086547852\n","\tgrad:  3.0 6.0 -1.070866584777832\n","Epoch: 9 | Loss: 0.03185431286692619\n","Prediction (after training) 4 7.804864406585693\n"]}]},{"cell_type":"markdown","metadata":{"id":"wFem4yiPltiw"},"source":["## PytorchZeroToAll. Lec05"]},{"cell_type":"code","metadata":{"id":"3xQfG_ktlvhm","executionInfo":{"status":"ok","timestamp":1632754541220,"user_tz":-540,"elapsed":16,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["from torch import nn\n","import torch\n","from torch import tensor\n","\n","x_data = tensor([[1.0],[2.0],[3.0]]) # torch.tensor -> torch\n","y_data = tensor([[2.0],[4.0],[6.0]])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntgGE-NSl2mL","executionInfo":{"status":"ok","timestamp":1632754541220,"user_tz":-540,"elapsed":15,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    \"\"\"\n","    In the constructor we instantiate nn.Linear module\n","    \"\"\"\n","    super(Model, self).__init__()\n","    self.linear = torch.nn.Linear(1,1) # One in and one out\n","\n","  def forward(self, x):\n","    \"\"\"\n","    In the forward function we accept a Variable of input data and we must return\n","    a Variable of output data. We can use Modules defined in the constructor as \n","    well as arbitary operators on Variables.\n","    \"\"\"\n","    y_pred = self.linear(x)\n","    return y_pred\n","\n","# our model\n","model = Model()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"XW5IT8BUo77u","executionInfo":{"status":"ok","timestamp":1632754541221,"user_tz":-540,"elapsed":15,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["# Construct our loss function and an Optimizer. The call to model.parameters()\n","# in the SGD constructor will contaion the learnable parameters of the\n","# nn.Linear modules which are members of the model.\n","\n","criterion = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pcc-2WyrsIW5","executionInfo":{"status":"ok","timestamp":1632754541925,"user_tz":-540,"elapsed":718,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"5a618ce2-cd1e-47a4-c127-7749e26935d0"},"source":["# Training loop\n","for epoch in range(500):\n","  # 1) Forward pass: Compute predicted y by passing x to the model\n","  y_pred = model(x_data)\n","\n","  # 2) Compute and print loss\n","  loss = criterion(y_pred, y_data)\n","  print(f'Epoch: {epoch} | Loss: {loss.item()}')\n","\n","  # Zero gradients, perform a backward pass, and update the weights.\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Loss: 100.76493835449219\n","Epoch: 1 | Loss: 45.47621154785156\n","Epoch: 2 | Loss: 20.854352951049805\n","Epoch: 3 | Loss: 9.884639739990234\n","Epoch: 4 | Loss: 4.992602348327637\n","Epoch: 5 | Loss: 2.806293249130249\n","Epoch: 6 | Loss: 1.8246212005615234\n","Epoch: 7 | Loss: 1.3793411254882812\n","Epoch: 8 | Loss: 1.172965168952942\n","Epoch: 9 | Loss: 1.0730602741241455\n","Epoch: 10 | Loss: 1.020667552947998\n","Epoch: 11 | Loss: 0.989540696144104\n","Epoch: 12 | Loss: 0.9679931998252869\n","Epoch: 13 | Loss: 0.950819730758667\n","Epoch: 14 | Loss: 0.9357025623321533\n","Epoch: 15 | Loss: 0.9216087460517883\n","Epoch: 16 | Loss: 0.9080759286880493\n","Epoch: 17 | Loss: 0.8948972225189209\n","Epoch: 18 | Loss: 0.8819793462753296\n","Epoch: 19 | Loss: 0.8692789673805237\n","Epoch: 20 | Loss: 0.8567740321159363\n","Epoch: 21 | Loss: 0.8444567322731018\n","Epoch: 22 | Loss: 0.8323177099227905\n","Epoch: 23 | Loss: 0.8203551769256592\n","Epoch: 24 | Loss: 0.8085649013519287\n","Epoch: 25 | Loss: 0.7969444990158081\n","Epoch: 26 | Loss: 0.7854909896850586\n","Epoch: 27 | Loss: 0.7742024064064026\n","Epoch: 28 | Loss: 0.7630761861801147\n","Epoch: 29 | Loss: 0.7521090507507324\n","Epoch: 30 | Loss: 0.7412998676300049\n","Epoch: 31 | Loss: 0.7306463718414307\n","Epoch: 32 | Loss: 0.7201458811759949\n","Epoch: 33 | Loss: 0.7097961902618408\n","Epoch: 34 | Loss: 0.6995954513549805\n","Epoch: 35 | Loss: 0.6895411610603333\n","Epoch: 36 | Loss: 0.6796316504478455\n","Epoch: 37 | Loss: 0.6698640584945679\n","Epoch: 38 | Loss: 0.6602368354797363\n","Epoch: 39 | Loss: 0.6507481336593628\n","Epoch: 40 | Loss: 0.6413955688476562\n","Epoch: 41 | Loss: 0.6321783065795898\n","Epoch: 42 | Loss: 0.6230922341346741\n","Epoch: 43 | Loss: 0.6141378879547119\n","Epoch: 44 | Loss: 0.6053117513656616\n","Epoch: 45 | Loss: 0.5966120958328247\n","Epoch: 46 | Loss: 0.5880382061004639\n","Epoch: 47 | Loss: 0.5795871615409851\n","Epoch: 48 | Loss: 0.5712575912475586\n","Epoch: 49 | Loss: 0.563047468662262\n","Epoch: 50 | Loss: 0.5549556016921997\n","Epoch: 51 | Loss: 0.5469801425933838\n","Epoch: 52 | Loss: 0.5391189455986023\n","Epoch: 53 | Loss: 0.531370997428894\n","Epoch: 54 | Loss: 0.5237343907356262\n","Epoch: 55 | Loss: 0.516207754611969\n","Epoch: 56 | Loss: 0.5087885856628418\n","Epoch: 57 | Loss: 0.5014770030975342\n","Epoch: 58 | Loss: 0.49426984786987305\n","Epoch: 59 | Loss: 0.48716622591018677\n","Epoch: 60 | Loss: 0.480165034532547\n","Epoch: 61 | Loss: 0.4732646048069\n","Epoch: 62 | Loss: 0.46646273136138916\n","Epoch: 63 | Loss: 0.45975908637046814\n","Epoch: 64 | Loss: 0.45315173268318176\n","Epoch: 65 | Loss: 0.4466387629508972\n","Epoch: 66 | Loss: 0.440219908952713\n","Epoch: 67 | Loss: 0.4338932931423187\n","Epoch: 68 | Loss: 0.42765796184539795\n","Epoch: 69 | Loss: 0.42151153087615967\n","Epoch: 70 | Loss: 0.41545379161834717\n","Epoch: 71 | Loss: 0.4094829857349396\n","Epoch: 72 | Loss: 0.4035979211330414\n","Epoch: 73 | Loss: 0.39779794216156006\n","Epoch: 74 | Loss: 0.3920806646347046\n","Epoch: 75 | Loss: 0.38644564151763916\n","Epoch: 76 | Loss: 0.3808920979499817\n","Epoch: 77 | Loss: 0.37541794776916504\n","Epoch: 78 | Loss: 0.3700227737426758\n","Epoch: 79 | Loss: 0.3647049069404602\n","Epoch: 80 | Loss: 0.3594636917114258\n","Epoch: 81 | Loss: 0.3542971909046173\n","Epoch: 82 | Loss: 0.34920576214790344\n","Epoch: 83 | Loss: 0.3441869914531708\n","Epoch: 84 | Loss: 0.33924078941345215\n","Epoch: 85 | Loss: 0.3343651592731476\n","Epoch: 86 | Loss: 0.3295595943927765\n","Epoch: 87 | Loss: 0.3248234987258911\n","Epoch: 88 | Loss: 0.32015544176101685\n","Epoch: 89 | Loss: 0.31555402278900146\n","Epoch: 90 | Loss: 0.3110188841819763\n","Epoch: 91 | Loss: 0.3065491020679474\n","Epoch: 92 | Loss: 0.3021436929702759\n","Epoch: 93 | Loss: 0.2978014349937439\n","Epoch: 94 | Loss: 0.2935214042663574\n","Epoch: 95 | Loss: 0.2893030345439911\n","Epoch: 96 | Loss: 0.28514552116394043\n","Epoch: 97 | Loss: 0.28104734420776367\n","Epoch: 98 | Loss: 0.2770083546638489\n","Epoch: 99 | Loss: 0.27302730083465576\n","Epoch: 100 | Loss: 0.2691035866737366\n","Epoch: 101 | Loss: 0.265235960483551\n","Epoch: 102 | Loss: 0.26142415404319763\n","Epoch: 103 | Loss: 0.2576669454574585\n","Epoch: 104 | Loss: 0.25396397709846497\n","Epoch: 105 | Loss: 0.2503140866756439\n","Epoch: 106 | Loss: 0.24671676754951477\n","Epoch: 107 | Loss: 0.24317103624343872\n","Epoch: 108 | Loss: 0.23967640101909637\n","Epoch: 109 | Loss: 0.23623177409172058\n","Epoch: 110 | Loss: 0.23283691704273224\n","Epoch: 111 | Loss: 0.22949042916297913\n","Epoch: 112 | Loss: 0.22619223594665527\n","Epoch: 113 | Loss: 0.22294144332408905\n","Epoch: 114 | Loss: 0.219737708568573\n","Epoch: 115 | Loss: 0.21657972037792206\n","Epoch: 116 | Loss: 0.2134670615196228\n","Epoch: 117 | Loss: 0.2103990763425827\n","Epoch: 118 | Loss: 0.20737530291080475\n","Epoch: 119 | Loss: 0.20439490675926208\n","Epoch: 120 | Loss: 0.2014576494693756\n","Epoch: 121 | Loss: 0.19856233894824982\n","Epoch: 122 | Loss: 0.19570890069007874\n","Epoch: 123 | Loss: 0.1928960233926773\n","Epoch: 124 | Loss: 0.1901235580444336\n","Epoch: 125 | Loss: 0.18739140033721924\n","Epoch: 126 | Loss: 0.18469837307929993\n","Epoch: 127 | Loss: 0.18204385042190552\n","Epoch: 128 | Loss: 0.17942747473716736\n","Epoch: 129 | Loss: 0.17684897780418396\n","Epoch: 130 | Loss: 0.17430739104747772\n","Epoch: 131 | Loss: 0.17180213332176208\n","Epoch: 132 | Loss: 0.16933336853981018\n","Epoch: 133 | Loss: 0.16689959168434143\n","Epoch: 134 | Loss: 0.16450092196464539\n","Epoch: 135 | Loss: 0.16213692724704742\n","Epoch: 136 | Loss: 0.15980678796768188\n","Epoch: 137 | Loss: 0.15750998258590698\n","Epoch: 138 | Loss: 0.1552465856075287\n","Epoch: 139 | Loss: 0.15301546454429626\n","Epoch: 140 | Loss: 0.15081626176834106\n","Epoch: 141 | Loss: 0.14864873886108398\n","Epoch: 142 | Loss: 0.14651250839233398\n","Epoch: 143 | Loss: 0.14440660178661346\n","Epoch: 144 | Loss: 0.14233145117759705\n","Epoch: 145 | Loss: 0.14028584957122803\n","Epoch: 146 | Loss: 0.1382698118686676\n","Epoch: 147 | Loss: 0.13628262281417847\n","Epoch: 148 | Loss: 0.13432399928569794\n","Epoch: 149 | Loss: 0.13239359855651855\n","Epoch: 150 | Loss: 0.13049085438251495\n","Epoch: 151 | Loss: 0.1286155730485916\n","Epoch: 152 | Loss: 0.12676700949668884\n","Epoch: 153 | Loss: 0.12494534999132156\n","Epoch: 154 | Loss: 0.1231495589017868\n","Epoch: 155 | Loss: 0.12137964367866516\n","Epoch: 156 | Loss: 0.11963523179292679\n","Epoch: 157 | Loss: 0.11791584640741348\n","Epoch: 158 | Loss: 0.11622142046689987\n","Epoch: 159 | Loss: 0.11455094814300537\n","Epoch: 160 | Loss: 0.11290472000837326\n","Epoch: 161 | Loss: 0.11128214001655579\n","Epoch: 162 | Loss: 0.1096828356385231\n","Epoch: 163 | Loss: 0.10810649394989014\n","Epoch: 164 | Loss: 0.10655280947685242\n","Epoch: 165 | Loss: 0.10502150654792786\n","Epoch: 166 | Loss: 0.103512242436409\n","Epoch: 167 | Loss: 0.10202454030513763\n","Epoch: 168 | Loss: 0.1005583256483078\n","Epoch: 169 | Loss: 0.0991130843758583\n","Epoch: 170 | Loss: 0.09768876433372498\n","Epoch: 171 | Loss: 0.09628476202487946\n","Epoch: 172 | Loss: 0.09490112960338593\n","Epoch: 173 | Loss: 0.09353714436292648\n","Epoch: 174 | Loss: 0.09219297021627426\n","Epoch: 175 | Loss: 0.09086790680885315\n","Epoch: 176 | Loss: 0.08956195414066315\n","Epoch: 177 | Loss: 0.08827477693557739\n","Epoch: 178 | Loss: 0.08700620383024216\n","Epoch: 179 | Loss: 0.08575577288866043\n","Epoch: 180 | Loss: 0.08452335745096207\n","Epoch: 181 | Loss: 0.08330848813056946\n","Epoch: 182 | Loss: 0.08211126923561096\n","Epoch: 183 | Loss: 0.08093123883008957\n","Epoch: 184 | Loss: 0.07976824045181274\n","Epoch: 185 | Loss: 0.07862166315317154\n","Epoch: 186 | Loss: 0.0774918720126152\n","Epoch: 187 | Loss: 0.07637806981801987\n","Epoch: 188 | Loss: 0.07528053969144821\n","Epoch: 189 | Loss: 0.07419849932193756\n","Epoch: 190 | Loss: 0.07313218712806702\n","Epoch: 191 | Loss: 0.07208119332790375\n","Epoch: 192 | Loss: 0.07104520499706268\n","Epoch: 193 | Loss: 0.07002410292625427\n","Epoch: 194 | Loss: 0.06901776045560837\n","Epoch: 195 | Loss: 0.06802597641944885\n","Epoch: 196 | Loss: 0.06704838573932648\n","Epoch: 197 | Loss: 0.06608473509550095\n","Epoch: 198 | Loss: 0.06513508409261703\n","Epoch: 199 | Loss: 0.06419897824525833\n","Epoch: 200 | Loss: 0.06327623128890991\n","Epoch: 201 | Loss: 0.062366824597120285\n","Epoch: 202 | Loss: 0.0614706426858902\n","Epoch: 203 | Loss: 0.060587141662836075\n","Epoch: 204 | Loss: 0.05971637740731239\n","Epoch: 205 | Loss: 0.05885817110538483\n","Epoch: 206 | Loss: 0.05801225081086159\n","Epoch: 207 | Loss: 0.0571785643696785\n","Epoch: 208 | Loss: 0.05635692924261093\n","Epoch: 209 | Loss: 0.055546943098306656\n","Epoch: 210 | Loss: 0.05474863201379776\n","Epoch: 211 | Loss: 0.05396180599927902\n","Epoch: 212 | Loss: 0.05318624526262283\n","Epoch: 213 | Loss: 0.05242181941866875\n","Epoch: 214 | Loss: 0.05166849493980408\n","Epoch: 215 | Loss: 0.050925906747579575\n","Epoch: 216 | Loss: 0.050194066017866135\n","Epoch: 217 | Loss: 0.04947280511260033\n","Epoch: 218 | Loss: 0.048761673271656036\n","Epoch: 219 | Loss: 0.04806092381477356\n","Epoch: 220 | Loss: 0.047370247542858124\n","Epoch: 221 | Loss: 0.04668937623500824\n","Epoch: 222 | Loss: 0.046018488705158234\n","Epoch: 223 | Loss: 0.045357026159763336\n","Epoch: 224 | Loss: 0.04470525681972504\n","Epoch: 225 | Loss: 0.04406275972723961\n","Epoch: 226 | Loss: 0.0434294193983078\n","Epoch: 227 | Loss: 0.04280538111925125\n","Epoch: 228 | Loss: 0.04219011962413788\n","Epoch: 229 | Loss: 0.04158380627632141\n","Epoch: 230 | Loss: 0.04098612442612648\n","Epoch: 231 | Loss: 0.04039716720581055\n","Epoch: 232 | Loss: 0.03981657326221466\n","Epoch: 233 | Loss: 0.03924437239766121\n","Epoch: 234 | Loss: 0.03868035227060318\n","Epoch: 235 | Loss: 0.038124438375234604\n","Epoch: 236 | Loss: 0.037576496601104736\n","Epoch: 237 | Loss: 0.037036459892988205\n","Epoch: 238 | Loss: 0.036504246294498444\n","Epoch: 239 | Loss: 0.03597967326641083\n","Epoch: 240 | Loss: 0.03546254336833954\n","Epoch: 241 | Loss: 0.034952834248542786\n","Epoch: 242 | Loss: 0.03445052355527878\n","Epoch: 243 | Loss: 0.03395547345280647\n","Epoch: 244 | Loss: 0.03346747159957886\n","Epoch: 245 | Loss: 0.03298640251159668\n","Epoch: 246 | Loss: 0.03251231834292412\n","Epoch: 247 | Loss: 0.03204512968659401\n","Epoch: 248 | Loss: 0.03158463165163994\n","Epoch: 249 | Loss: 0.031130695715546608\n","Epoch: 250 | Loss: 0.03068322315812111\n","Epoch: 251 | Loss: 0.030242308974266052\n","Epoch: 252 | Loss: 0.029807766899466515\n","Epoch: 253 | Loss: 0.029379308223724365\n","Epoch: 254 | Loss: 0.028957078233361244\n","Epoch: 255 | Loss: 0.028540996834635735\n","Epoch: 256 | Loss: 0.02813073806464672\n","Epoch: 257 | Loss: 0.02772649936378002\n","Epoch: 258 | Loss: 0.027328001335263252\n","Epoch: 259 | Loss: 0.026935230940580368\n","Epoch: 260 | Loss: 0.02654813602566719\n","Epoch: 261 | Loss: 0.026166589930653572\n","Epoch: 262 | Loss: 0.025790607556700706\n","Epoch: 263 | Loss: 0.02541995979845524\n","Epoch: 264 | Loss: 0.025054533034563065\n","Epoch: 265 | Loss: 0.02469448745250702\n","Epoch: 266 | Loss: 0.024339638650417328\n","Epoch: 267 | Loss: 0.02398984506726265\n","Epoch: 268 | Loss: 0.023645024746656418\n","Epoch: 269 | Loss: 0.023305246606469154\n","Epoch: 270 | Loss: 0.022970229387283325\n","Epoch: 271 | Loss: 0.02264012023806572\n","Epoch: 272 | Loss: 0.022314738482236862\n","Epoch: 273 | Loss: 0.021994030103087425\n","Epoch: 274 | Loss: 0.02167794667184353\n","Epoch: 275 | Loss: 0.021366503089666367\n","Epoch: 276 | Loss: 0.021059351041913033\n","Epoch: 277 | Loss: 0.02075669914484024\n","Epoch: 278 | Loss: 0.020458439365029335\n","Epoch: 279 | Loss: 0.02016434818506241\n","Epoch: 280 | Loss: 0.01987462118268013\n","Epoch: 281 | Loss: 0.019588911905884743\n","Epoch: 282 | Loss: 0.01930747739970684\n","Epoch: 283 | Loss: 0.019029971212148666\n","Epoch: 284 | Loss: 0.01875648833811283\n","Epoch: 285 | Loss: 0.018486900255084038\n","Epoch: 286 | Loss: 0.018221240490674973\n","Epoch: 287 | Loss: 0.01795937307178974\n","Epoch: 288 | Loss: 0.01770125702023506\n","Epoch: 289 | Loss: 0.01744683086872101\n","Epoch: 290 | Loss: 0.01719614304602146\n","Epoch: 291 | Loss: 0.016949020326137543\n","Epoch: 292 | Loss: 0.016705412417650223\n","Epoch: 293 | Loss: 0.016465291380882263\n","Epoch: 294 | Loss: 0.01622864417731762\n","Epoch: 295 | Loss: 0.015995407477021217\n","Epoch: 296 | Loss: 0.015765544027090073\n","Epoch: 297 | Loss: 0.01553899236023426\n","Epoch: 298 | Loss: 0.015315661206841469\n","Epoch: 299 | Loss: 0.015095619484782219\n","Epoch: 300 | Loss: 0.014878609217703342\n","Epoch: 301 | Loss: 0.014664732851088047\n","Epoch: 302 | Loss: 0.014453970827162266\n","Epoch: 303 | Loss: 0.014246352016925812\n","Epoch: 304 | Loss: 0.014041563495993614\n","Epoch: 305 | Loss: 0.013839783146977425\n","Epoch: 306 | Loss: 0.013640863820910454\n","Epoch: 307 | Loss: 0.013444819487631321\n","Epoch: 308 | Loss: 0.01325160637497902\n","Epoch: 309 | Loss: 0.013061126694083214\n","Epoch: 310 | Loss: 0.01287340559065342\n","Epoch: 311 | Loss: 0.012688424438238144\n","Epoch: 312 | Loss: 0.012506089173257351\n","Epoch: 313 | Loss: 0.012326372787356377\n","Epoch: 314 | Loss: 0.01214924268424511\n","Epoch: 315 | Loss: 0.011974598281085491\n","Epoch: 316 | Loss: 0.011802488006651402\n","Epoch: 317 | Loss: 0.011632883921265602\n","Epoch: 318 | Loss: 0.01146572083234787\n","Epoch: 319 | Loss: 0.011300912126898766\n","Epoch: 320 | Loss: 0.01113849226385355\n","Epoch: 321 | Loss: 0.010978400707244873\n","Epoch: 322 | Loss: 0.010820670053362846\n","Epoch: 323 | Loss: 0.010665119625627995\n","Epoch: 324 | Loss: 0.010511888191103935\n","Epoch: 325 | Loss: 0.010360813699662685\n","Epoch: 326 | Loss: 0.010211865417659283\n","Epoch: 327 | Loss: 0.01006515696644783\n","Epoch: 328 | Loss: 0.009920495562255383\n","Epoch: 329 | Loss: 0.009777892380952835\n","Epoch: 330 | Loss: 0.009637393988668919\n","Epoch: 331 | Loss: 0.009498870931565762\n","Epoch: 332 | Loss: 0.009362328797578812\n","Epoch: 333 | Loss: 0.009227820672094822\n","Epoch: 334 | Loss: 0.009095200337469578\n","Epoch: 335 | Loss: 0.008964486420154572\n","Epoch: 336 | Loss: 0.008835659362375736\n","Epoch: 337 | Loss: 0.008708685636520386\n","Epoch: 338 | Loss: 0.00858352705836296\n","Epoch: 339 | Loss: 0.008460188284516335\n","Epoch: 340 | Loss: 0.008338574320077896\n","Epoch: 341 | Loss: 0.008218706585466862\n","Epoch: 342 | Loss: 0.008100612089037895\n","Epoch: 343 | Loss: 0.007984228432178497\n","Epoch: 344 | Loss: 0.007869446650147438\n","Epoch: 345 | Loss: 0.007756339851766825\n","Epoch: 346 | Loss: 0.007644890807569027\n","Epoch: 347 | Loss: 0.007535005919635296\n","Epoch: 348 | Loss: 0.00742673221975565\n","Epoch: 349 | Loss: 0.007319964002817869\n","Epoch: 350 | Loss: 0.007214758545160294\n","Epoch: 351 | Loss: 0.007111099548637867\n","Epoch: 352 | Loss: 0.007008872460573912\n","Epoch: 353 | Loss: 0.006908148527145386\n","Epoch: 354 | Loss: 0.0068088737316429615\n","Epoch: 355 | Loss: 0.006711026653647423\n","Epoch: 356 | Loss: 0.0066145919263362885\n","Epoch: 357 | Loss: 0.0065195318311452866\n","Epoch: 358 | Loss: 0.006425807718187571\n","Epoch: 359 | Loss: 0.0063334982842206955\n","Epoch: 360 | Loss: 0.006242476403713226\n","Epoch: 361 | Loss: 0.006152719259262085\n","Epoch: 362 | Loss: 0.006064350716769695\n","Epoch: 363 | Loss: 0.005977174267172813\n","Epoch: 364 | Loss: 0.005891244858503342\n","Epoch: 365 | Loss: 0.005806620232760906\n","Epoch: 366 | Loss: 0.005723143927752972\n","Epoch: 367 | Loss: 0.00564091419801116\n","Epoch: 368 | Loss: 0.005559821147471666\n","Epoch: 369 | Loss: 0.005479918327182531\n","Epoch: 370 | Loss: 0.005401144735515118\n","Epoch: 371 | Loss: 0.005323539953678846\n","Epoch: 372 | Loss: 0.005247035529464483\n","Epoch: 373 | Loss: 0.005171611905097961\n","Epoch: 374 | Loss: 0.005097302608191967\n","Epoch: 375 | Loss: 0.005024046637117863\n","Epoch: 376 | Loss: 0.004951811861246824\n","Epoch: 377 | Loss: 0.004880668595433235\n","Epoch: 378 | Loss: 0.00481050368398428\n","Epoch: 379 | Loss: 0.004741383716464043\n","Epoch: 380 | Loss: 0.004673264920711517\n","Epoch: 381 | Loss: 0.00460610818117857\n","Epoch: 382 | Loss: 0.004539887886494398\n","Epoch: 383 | Loss: 0.0044746603816747665\n","Epoch: 384 | Loss: 0.004410334397107363\n","Epoch: 385 | Loss: 0.004346937406808138\n","Epoch: 386 | Loss: 0.0042844898998737335\n","Epoch: 387 | Loss: 0.004222906660288572\n","Epoch: 388 | Loss: 0.00416222307831049\n","Epoch: 389 | Loss: 0.004102421458810568\n","Epoch: 390 | Loss: 0.004043444991111755\n","Epoch: 391 | Loss: 0.003985361196100712\n","Epoch: 392 | Loss: 0.003928076941519976\n","Epoch: 393 | Loss: 0.0038716343697160482\n","Epoch: 394 | Loss: 0.0038159554824233055\n","Epoch: 395 | Loss: 0.0037611329462379217\n","Epoch: 396 | Loss: 0.003707075258716941\n","Epoch: 397 | Loss: 0.0036537745036184788\n","Epoch: 398 | Loss: 0.0036012851633131504\n","Epoch: 399 | Loss: 0.0035495255142450333\n","Epoch: 400 | Loss: 0.003498505102470517\n","Epoch: 401 | Loss: 0.0034482397604733706\n","Epoch: 402 | Loss: 0.003398690838366747\n","Epoch: 403 | Loss: 0.003349857171997428\n","Epoch: 404 | Loss: 0.003301690798252821\n","Epoch: 405 | Loss: 0.0032542282715439796\n","Epoch: 406 | Loss: 0.003207478439435363\n","Epoch: 407 | Loss: 0.0031614063773304224\n","Epoch: 408 | Loss: 0.0031159664504230022\n","Epoch: 409 | Loss: 0.003071183804422617\n","Epoch: 410 | Loss: 0.0030270328279584646\n","Epoch: 411 | Loss: 0.0029835442546755075\n","Epoch: 412 | Loss: 0.0029406417161226273\n","Epoch: 413 | Loss: 0.0028983845841139555\n","Epoch: 414 | Loss: 0.0028567349072545767\n","Epoch: 415 | Loss: 0.0028156889602541924\n","Epoch: 416 | Loss: 0.002775209490209818\n","Epoch: 417 | Loss: 0.0027353183832019567\n","Epoch: 418 | Loss: 0.002696011448279023\n","Epoch: 419 | Loss: 0.0026572593487799168\n","Epoch: 420 | Loss: 0.0026190965436398983\n","Epoch: 421 | Loss: 0.0025814524851739407\n","Epoch: 422 | Loss: 0.002544347895309329\n","Epoch: 423 | Loss: 0.002507791155949235\n","Epoch: 424 | Loss: 0.0024717331398278475\n","Epoch: 425 | Loss: 0.0024362034164369106\n","Epoch: 426 | Loss: 0.0024011784698814154\n","Epoch: 427 | Loss: 0.002366703934967518\n","Epoch: 428 | Loss: 0.002332689706236124\n","Epoch: 429 | Loss: 0.0022991474252194166\n","Epoch: 430 | Loss: 0.002266122028231621\n","Epoch: 431 | Loss: 0.002233558101579547\n","Epoch: 432 | Loss: 0.0022014561109244823\n","Epoch: 433 | Loss: 0.00216980604454875\n","Epoch: 434 | Loss: 0.0021386086009442806\n","Epoch: 435 | Loss: 0.0021078770514577627\n","Epoch: 436 | Loss: 0.002077608834952116\n","Epoch: 437 | Loss: 0.0020477466750890017\n","Epoch: 438 | Loss: 0.002018304541707039\n","Epoch: 439 | Loss: 0.0019893059507012367\n","Epoch: 440 | Loss: 0.001960708061233163\n","Epoch: 441 | Loss: 0.0019325446337461472\n","Epoch: 442 | Loss: 0.0019047573441639543\n","Epoch: 443 | Loss: 0.0018773655174300075\n","Epoch: 444 | Loss: 0.001850404660217464\n","Epoch: 445 | Loss: 0.0018238124903291464\n","Epoch: 446 | Loss: 0.0017975952941924334\n","Epoch: 447 | Loss: 0.0017717734444886446\n","Epoch: 448 | Loss: 0.0017462964169681072\n","Epoch: 449 | Loss: 0.0017212207894772291\n","Epoch: 450 | Loss: 0.001696467981673777\n","Epoch: 451 | Loss: 0.0016720941057428718\n","Epoch: 452 | Loss: 0.0016480549238622189\n","Epoch: 453 | Loss: 0.001624376978725195\n","Epoch: 454 | Loss: 0.0016010472318157554\n","Epoch: 455 | Loss: 0.001578029477968812\n","Epoch: 456 | Loss: 0.0015553459525108337\n","Epoch: 457 | Loss: 0.0015329744201153517\n","Epoch: 458 | Loss: 0.0015109593514353037\n","Epoch: 459 | Loss: 0.001489249523729086\n","Epoch: 460 | Loss: 0.001467845868319273\n","Epoch: 461 | Loss: 0.0014467497821897268\n","Epoch: 462 | Loss: 0.0014259535819292068\n","Epoch: 463 | Loss: 0.0014054493512958288\n","Epoch: 464 | Loss: 0.001385258394293487\n","Epoch: 465 | Loss: 0.001365359523333609\n","Epoch: 466 | Loss: 0.00134573457762599\n","Epoch: 467 | Loss: 0.0013263873988762498\n","Epoch: 468 | Loss: 0.0013073254376649857\n","Epoch: 469 | Loss: 0.0012885431060567498\n","Epoch: 470 | Loss: 0.001270015025511384\n","Epoch: 471 | Loss: 0.0012517670402303338\n","Epoch: 472 | Loss: 0.0012337695807218552\n","Epoch: 473 | Loss: 0.0012160344049334526\n","Epoch: 474 | Loss: 0.001198566285893321\n","Epoch: 475 | Loss: 0.0011813344899564981\n","Epoch: 476 | Loss: 0.001164366491138935\n","Epoch: 477 | Loss: 0.0011476175859570503\n","Epoch: 478 | Loss: 0.0011311328271403909\n","Epoch: 479 | Loss: 0.0011148799676448107\n","Epoch: 480 | Loss: 0.0010988661088049412\n","Epoch: 481 | Loss: 0.0010830771643668413\n","Epoch: 482 | Loss: 0.001067505101673305\n","Epoch: 483 | Loss: 0.001052170293405652\n","Epoch: 484 | Loss: 0.0010370363015681505\n","Epoch: 485 | Loss: 0.0010221288539469242\n","Epoch: 486 | Loss: 0.0010074606398120522\n","Epoch: 487 | Loss: 0.0009929770603775978\n","Epoch: 488 | Loss: 0.000978689407929778\n","Epoch: 489 | Loss: 0.0009646351682022214\n","Epoch: 490 | Loss: 0.0009507579961791635\n","Epoch: 491 | Loss: 0.0009370988700538874\n","Epoch: 492 | Loss: 0.0009236338082700968\n","Epoch: 493 | Loss: 0.0009103716583922505\n","Epoch: 494 | Loss: 0.0008972854120656848\n","Epoch: 495 | Loss: 0.0008843959658406675\n","Epoch: 496 | Loss: 0.0008716718293726444\n","Epoch: 497 | Loss: 0.0008591455989517272\n","Epoch: 498 | Loss: 0.0008467996958643198\n","Epoch: 499 | Loss: 0.0008346321992576122\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zsB-wkivthy5","executionInfo":{"status":"ok","timestamp":1632754541926,"user_tz":-540,"elapsed":21,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"14875621-9fff-42b2-ce60-7c5b86d6beed"},"source":["# After training\n","hour_var = tensor([[4.0]])\n","y_pred = model(hour_var)\n","print('Prediction (after training)', 4, model(hour_var).item())"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction (after training) 4 7.966789722442627\n"]}]},{"cell_type":"markdown","metadata":{"id":"DHQRY4rBuJEr"},"source":["## PytorchZeroToAll. Lec06"]},{"cell_type":"code","metadata":{"id":"GdPSeM8-uJeq","executionInfo":{"status":"ok","timestamp":1632754541927,"user_tz":-540,"elapsed":10,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["from torch import tensor\n","from torch import nn\n","from torch import sigmoid\n","import torch.optim as optim\n","\n","# Training data and ground truth\n","x_data = tensor([[1.0],[2.0],[3.0],[4.0]])\n","y_data = tensor([[0.],[0.],[1.],[1.]])"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"1e5gyfBDuex_","executionInfo":{"status":"ok","timestamp":1632754541928,"user_tz":-540,"elapsed":10,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    \"\"\"\n","    In the constructor we instantiate nn.Linear module\n","    \"\"\"\n","    super(Model, self).__init__()\n","    self.linear = torch.nn.Linear(1,1) # One in and one out\n","\n","  def forward(self, x):\n","    \"\"\"\n","    In the forward function we accept a Variable of input data \n","    and we must return a Variable of output data.\n","    \"\"\"\n","    y_pred = sigmoid(self.linear(x))\n","    return y_pred\n","\n","# our model\n","model = Model()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"I2yy9H9eu0h-","executionInfo":{"status":"ok","timestamp":1632754541929,"user_tz":-540,"elapsed":10,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}}},"source":["# Construct our loss function and an Optimizer. The call to model.parameters()\n","# in the SGD constructor will contaion the learnable parameters of the\n","# nn.Linear modules which are members of the model.\n","\n","criterion = torch.nn.BCELoss(reduction='mean')\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qgxa4620vKna","executionInfo":{"status":"ok","timestamp":1632754543833,"user_tz":-540,"elapsed":1913,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"202e7fd5-3b09-498b-ce69-e5ae9d8019d9"},"source":["# Training loop\n","for epoch in range(1000):\n","  # Forward pass: Compute predicted y by passing x to the model\n","  y_pred = model(x_data)\n","\n","  # Compute and print loss\n","  loss = criterion(y_pred, y_data)\n","  print(f'Epoch: {epoch} | Loss: {loss.item()}')\n","\n","  # Zero gradients, perform a backward pass, and update the weights.\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Loss: 0.6912441849708557\n","Epoch: 1 | Loss: 0.6903122663497925\n","Epoch: 2 | Loss: 0.6893983483314514\n","Epoch: 3 | Loss: 0.6885021328926086\n","Epoch: 4 | Loss: 0.6876230835914612\n","Epoch: 5 | Loss: 0.6867609024047852\n","Epoch: 6 | Loss: 0.6859150528907776\n","Epoch: 7 | Loss: 0.6850853562355042\n","Epoch: 8 | Loss: 0.6842710971832275\n","Epoch: 9 | Loss: 0.683472216129303\n","Epoch: 10 | Loss: 0.682688295841217\n","Epoch: 11 | Loss: 0.6819187998771667\n","Epoch: 12 | Loss: 0.6811635494232178\n","Epoch: 13 | Loss: 0.6804222464561462\n","Epoch: 14 | Loss: 0.6796943545341492\n","Epoch: 15 | Loss: 0.6789796352386475\n","Epoch: 16 | Loss: 0.6782774329185486\n","Epoch: 17 | Loss: 0.6775880455970764\n","Epoch: 18 | Loss: 0.6769107580184937\n","Epoch: 19 | Loss: 0.6762453317642212\n","Epoch: 20 | Loss: 0.6755914688110352\n","Epoch: 21 | Loss: 0.6749488115310669\n","Epoch: 22 | Loss: 0.6743171811103821\n","Epoch: 23 | Loss: 0.6736961007118225\n","Epoch: 24 | Loss: 0.6730854511260986\n","Epoch: 25 | Loss: 0.6724850535392761\n","Epoch: 26 | Loss: 0.671894371509552\n","Epoch: 27 | Loss: 0.6713132858276367\n","Epoch: 28 | Loss: 0.6707414984703064\n","Epoch: 29 | Loss: 0.6701787710189819\n","Epoch: 30 | Loss: 0.669624924659729\n","Epoch: 31 | Loss: 0.6690797209739685\n","Epoch: 32 | Loss: 0.668542742729187\n","Epoch: 33 | Loss: 0.6680138111114502\n","Epoch: 34 | Loss: 0.6674929857254028\n","Epoch: 35 | Loss: 0.6669796705245972\n","Epoch: 36 | Loss: 0.6664738059043884\n","Epoch: 37 | Loss: 0.6659752726554871\n","Epoch: 38 | Loss: 0.6654837131500244\n","Epoch: 39 | Loss: 0.6649990677833557\n","Epoch: 40 | Loss: 0.6645211577415466\n","Epoch: 41 | Loss: 0.6640495657920837\n","Epoch: 42 | Loss: 0.6635844111442566\n","Epoch: 43 | Loss: 0.6631251573562622\n","Epoch: 44 | Loss: 0.6626718640327454\n","Epoch: 45 | Loss: 0.6622244715690613\n","Epoch: 46 | Loss: 0.6617825031280518\n","Epoch: 47 | Loss: 0.6613461971282959\n","Epoch: 48 | Loss: 0.6609150171279907\n","Epoch: 49 | Loss: 0.6604889631271362\n","Epoch: 50 | Loss: 0.6600679159164429\n","Epoch: 51 | Loss: 0.6596516966819763\n","Epoch: 52 | Loss: 0.659240186214447\n","Epoch: 53 | Loss: 0.6588331460952759\n","Epoch: 54 | Loss: 0.6584305763244629\n","Epoch: 55 | Loss: 0.6580322980880737\n","Epoch: 56 | Loss: 0.6576381921768188\n","Epoch: 57 | Loss: 0.6572481989860535\n","Epoch: 58 | Loss: 0.6568620204925537\n","Epoch: 59 | Loss: 0.6564797759056091\n","Epoch: 60 | Loss: 0.6561011075973511\n","Epoch: 61 | Loss: 0.6557260751724243\n","Epoch: 62 | Loss: 0.6553544998168945\n","Epoch: 63 | Loss: 0.6549863219261169\n","Epoch: 64 | Loss: 0.654621422290802\n","Epoch: 65 | Loss: 0.6542596817016602\n","Epoch: 66 | Loss: 0.6539011597633362\n","Epoch: 67 | Loss: 0.6535454988479614\n","Epoch: 68 | Loss: 0.6531928181648254\n","Epoch: 69 | Loss: 0.6528428792953491\n","Epoch: 70 | Loss: 0.6524957418441772\n","Epoch: 71 | Loss: 0.6521512866020203\n","Epoch: 72 | Loss: 0.6518093347549438\n","Epoch: 73 | Loss: 0.6514699459075928\n","Epoch: 74 | Loss: 0.6511330604553223\n","Epoch: 75 | Loss: 0.6507983803749084\n","Epoch: 76 | Loss: 0.6504660844802856\n","Epoch: 77 | Loss: 0.65013587474823\n","Epoch: 78 | Loss: 0.6498080492019653\n","Epoch: 79 | Loss: 0.6494821310043335\n","Epoch: 80 | Loss: 0.6491583585739136\n","Epoch: 81 | Loss: 0.6488364934921265\n","Epoch: 82 | Loss: 0.6485165357589722\n","Epoch: 83 | Loss: 0.6481984853744507\n","Epoch: 84 | Loss: 0.6478821635246277\n","Epoch: 85 | Loss: 0.647567629814148\n","Epoch: 86 | Loss: 0.6472547054290771\n","Epoch: 87 | Loss: 0.6469436287879944\n","Epoch: 88 | Loss: 0.646634042263031\n","Epoch: 89 | Loss: 0.6463260054588318\n","Epoch: 90 | Loss: 0.646019458770752\n","Epoch: 91 | Loss: 0.6457144618034363\n","Epoch: 92 | Loss: 0.6454108357429504\n","Epoch: 93 | Loss: 0.6451085805892944\n","Epoch: 94 | Loss: 0.6448076963424683\n","Epoch: 95 | Loss: 0.6445081233978271\n","Epoch: 96 | Loss: 0.6442098021507263\n","Epoch: 97 | Loss: 0.643912672996521\n","Epoch: 98 | Loss: 0.6436168551445007\n","Epoch: 99 | Loss: 0.6433221697807312\n","Epoch: 100 | Loss: 0.6430284976959229\n","Epoch: 101 | Loss: 0.6427360773086548\n","Epoch: 102 | Loss: 0.6424447298049927\n","Epoch: 103 | Loss: 0.642154335975647\n","Epoch: 104 | Loss: 0.6418649554252625\n","Epoch: 105 | Loss: 0.6415765881538391\n","Epoch: 106 | Loss: 0.641289234161377\n","Epoch: 107 | Loss: 0.6410025954246521\n","Epoch: 108 | Loss: 0.640717089176178\n","Epoch: 109 | Loss: 0.6404324173927307\n","Epoch: 110 | Loss: 0.6401485800743103\n","Epoch: 111 | Loss: 0.6398655772209167\n","Epoch: 112 | Loss: 0.63958340883255\n","Epoch: 113 | Loss: 0.6393019556999207\n","Epoch: 114 | Loss: 0.6390213966369629\n","Epoch: 115 | Loss: 0.6387416124343872\n","Epoch: 116 | Loss: 0.6384623646736145\n","Epoch: 117 | Loss: 0.6381840109825134\n","Epoch: 118 | Loss: 0.6379062533378601\n","Epoch: 119 | Loss: 0.6376292705535889\n","Epoch: 120 | Loss: 0.6373528838157654\n","Epoch: 121 | Loss: 0.6370770931243896\n","Epoch: 122 | Loss: 0.636802077293396\n","Epoch: 123 | Loss: 0.6365276575088501\n","Epoch: 124 | Loss: 0.6362537145614624\n","Epoch: 125 | Loss: 0.6359804272651672\n","Epoch: 126 | Loss: 0.6357077360153198\n","Epoch: 127 | Loss: 0.6354355812072754\n","Epoch: 128 | Loss: 0.6351640224456787\n","Epoch: 129 | Loss: 0.6348928213119507\n","Epoch: 130 | Loss: 0.63462233543396\n","Epoch: 131 | Loss: 0.6343522667884827\n","Epoch: 132 | Loss: 0.6340827941894531\n","Epoch: 133 | Loss: 0.6338136792182922\n","Epoch: 134 | Loss: 0.6335451006889343\n","Epoch: 135 | Loss: 0.6332770586013794\n","Epoch: 136 | Loss: 0.6330094337463379\n","Epoch: 137 | Loss: 0.6327422261238098\n","Epoch: 138 | Loss: 0.6324753761291504\n","Epoch: 139 | Loss: 0.6322091817855835\n","Epoch: 140 | Loss: 0.6319431662559509\n","Epoch: 141 | Loss: 0.6316777467727661\n","Epoch: 142 | Loss: 0.6314125657081604\n","Epoch: 143 | Loss: 0.6311478614807129\n","Epoch: 144 | Loss: 0.6308836340904236\n","Epoch: 145 | Loss: 0.6306196451187134\n","Epoch: 146 | Loss: 0.6303561329841614\n","Epoch: 147 | Loss: 0.630092978477478\n","Epoch: 148 | Loss: 0.6298301219940186\n","Epoch: 149 | Loss: 0.6295676827430725\n","Epoch: 150 | Loss: 0.6293056011199951\n","Epoch: 151 | Loss: 0.6290438175201416\n","Epoch: 152 | Loss: 0.628782331943512\n","Epoch: 153 | Loss: 0.6285213232040405\n","Epoch: 154 | Loss: 0.6282604932785034\n","Epoch: 155 | Loss: 0.6280001401901245\n","Epoch: 156 | Loss: 0.6277399659156799\n","Epoch: 157 | Loss: 0.6274800896644592\n","Epoch: 158 | Loss: 0.627220630645752\n","Epoch: 159 | Loss: 0.6269614100456238\n","Epoch: 160 | Loss: 0.6267025470733643\n","Epoch: 161 | Loss: 0.6264439821243286\n","Epoch: 162 | Loss: 0.6261855959892273\n","Epoch: 163 | Loss: 0.6259276270866394\n","Epoch: 164 | Loss: 0.6256698369979858\n","Epoch: 165 | Loss: 0.6254123449325562\n","Epoch: 166 | Loss: 0.6251550912857056\n","Epoch: 167 | Loss: 0.6248982548713684\n","Epoch: 168 | Loss: 0.6246415972709656\n","Epoch: 169 | Loss: 0.6243851780891418\n","Epoch: 170 | Loss: 0.6241291165351868\n","Epoch: 171 | Loss: 0.6238731741905212\n","Epoch: 172 | Loss: 0.6236175298690796\n","Epoch: 173 | Loss: 0.6233622431755066\n","Epoch: 174 | Loss: 0.6231071352958679\n","Epoch: 175 | Loss: 0.6228523254394531\n","Epoch: 176 | Loss: 0.6225976943969727\n","Epoch: 177 | Loss: 0.6223433017730713\n","Epoch: 178 | Loss: 0.622089147567749\n","Epoch: 179 | Loss: 0.6218352913856506\n","Epoch: 180 | Loss: 0.6215816140174866\n","Epoch: 181 | Loss: 0.6213281750679016\n","Epoch: 182 | Loss: 0.621074914932251\n","Epoch: 183 | Loss: 0.6208220720291138\n","Epoch: 184 | Loss: 0.6205692887306213\n","Epoch: 185 | Loss: 0.6203168034553528\n","Epoch: 186 | Loss: 0.6200644969940186\n","Epoch: 187 | Loss: 0.6198124885559082\n","Epoch: 188 | Loss: 0.6195605397224426\n","Epoch: 189 | Loss: 0.6193090081214905\n","Epoch: 190 | Loss: 0.6190575957298279\n","Epoch: 191 | Loss: 0.6188064217567444\n","Epoch: 192 | Loss: 0.6185554265975952\n","Epoch: 193 | Loss: 0.6183046698570251\n","Epoch: 194 | Loss: 0.6180541515350342\n","Epoch: 195 | Loss: 0.6178037524223328\n","Epoch: 196 | Loss: 0.6175536513328552\n","Epoch: 197 | Loss: 0.6173036694526672\n","Epoch: 198 | Loss: 0.6170540452003479\n","Epoch: 199 | Loss: 0.6168044805526733\n","Epoch: 200 | Loss: 0.6165552139282227\n","Epoch: 201 | Loss: 0.6163060069084167\n","Epoch: 202 | Loss: 0.6160571575164795\n","Epoch: 203 | Loss: 0.6158084869384766\n","Epoch: 204 | Loss: 0.6155599355697632\n","Epoch: 205 | Loss: 0.6153116226196289\n","Epoch: 206 | Loss: 0.6150636076927185\n","Epoch: 207 | Loss: 0.6148155927658081\n","Epoch: 208 | Loss: 0.6145679354667664\n","Epoch: 209 | Loss: 0.6143204569816589\n","Epoch: 210 | Loss: 0.6140730381011963\n","Epoch: 211 | Loss: 0.6138259172439575\n","Epoch: 212 | Loss: 0.6135789752006531\n","Epoch: 213 | Loss: 0.6133322715759277\n","Epoch: 214 | Loss: 0.6130856275558472\n","Epoch: 215 | Loss: 0.6128392815589905\n","Epoch: 216 | Loss: 0.6125930547714233\n","Epoch: 217 | Loss: 0.6123471260070801\n","Epoch: 218 | Loss: 0.6121012568473816\n","Epoch: 219 | Loss: 0.611855685710907\n","Epoch: 220 | Loss: 0.6116102933883667\n","Epoch: 221 | Loss: 0.6113649010658264\n","Epoch: 222 | Loss: 0.6111199855804443\n","Epoch: 223 | Loss: 0.6108750700950623\n","Epoch: 224 | Loss: 0.6106303334236145\n","Epoch: 225 | Loss: 0.6103857755661011\n","Epoch: 226 | Loss: 0.6101415157318115\n","Epoch: 227 | Loss: 0.6098974347114563\n","Epoch: 228 | Loss: 0.6096534132957458\n","Epoch: 229 | Loss: 0.6094096302986145\n","Epoch: 230 | Loss: 0.6091660261154175\n","Epoch: 231 | Loss: 0.6089226007461548\n","Epoch: 232 | Loss: 0.6086793541908264\n","Epoch: 233 | Loss: 0.6084362864494324\n","Epoch: 234 | Loss: 0.6081933975219727\n","Epoch: 235 | Loss: 0.6079506874084473\n","Epoch: 236 | Loss: 0.607708215713501\n","Epoch: 237 | Loss: 0.6074658632278442\n","Epoch: 238 | Loss: 0.607223629951477\n","Epoch: 239 | Loss: 0.606981635093689\n","Epoch: 240 | Loss: 0.6067398190498352\n","Epoch: 241 | Loss: 0.6064982414245605\n","Epoch: 242 | Loss: 0.6062567830085754\n","Epoch: 243 | Loss: 0.6060154438018799\n","Epoch: 244 | Loss: 0.6057743430137634\n","Epoch: 245 | Loss: 0.6055333614349365\n","Epoch: 246 | Loss: 0.6052926182746887\n","Epoch: 247 | Loss: 0.6050520539283752\n","Epoch: 248 | Loss: 0.6048115491867065\n","Epoch: 249 | Loss: 0.6045714020729065\n","Epoch: 250 | Loss: 0.6043313145637512\n","Epoch: 251 | Loss: 0.6040914058685303\n","Epoch: 252 | Loss: 0.6038516759872437\n","Epoch: 253 | Loss: 0.6036121845245361\n","Epoch: 254 | Loss: 0.6033727526664734\n","Epoch: 255 | Loss: 0.6031335592269897\n","Epoch: 256 | Loss: 0.6028945446014404\n","Epoch: 257 | Loss: 0.6026556491851807\n","Epoch: 258 | Loss: 0.6024168729782104\n","Epoch: 259 | Loss: 0.6021783947944641\n","Epoch: 260 | Loss: 0.6019400358200073\n","Epoch: 261 | Loss: 0.6017018556594849\n","Epoch: 262 | Loss: 0.6014639139175415\n","Epoch: 263 | Loss: 0.6012259721755981\n","Epoch: 264 | Loss: 0.6009883284568787\n","Epoch: 265 | Loss: 0.6007509231567383\n","Epoch: 266 | Loss: 0.6005135178565979\n","Epoch: 267 | Loss: 0.6002763509750366\n","Epoch: 268 | Loss: 0.6000394225120544\n","Epoch: 269 | Loss: 0.5998024940490723\n","Epoch: 270 | Loss: 0.599565863609314\n","Epoch: 271 | Loss: 0.5993292927742004\n","Epoch: 272 | Loss: 0.5990930199623108\n","Epoch: 273 | Loss: 0.5988569259643555\n","Epoch: 274 | Loss: 0.5986208319664001\n","Epoch: 275 | Loss: 0.5983850955963135\n","Epoch: 276 | Loss: 0.5981494188308716\n","Epoch: 277 | Loss: 0.597913920879364\n","Epoch: 278 | Loss: 0.5976786613464355\n","Epoch: 279 | Loss: 0.5974434614181519\n","Epoch: 280 | Loss: 0.5972083806991577\n","Epoch: 281 | Loss: 0.5969735980033875\n","Epoch: 282 | Loss: 0.5967390537261963\n","Epoch: 283 | Loss: 0.5965045094490051\n","Epoch: 284 | Loss: 0.5962702035903931\n","Epoch: 285 | Loss: 0.5960360765457153\n","Epoch: 286 | Loss: 0.5958020687103271\n","Epoch: 287 | Loss: 0.5955681800842285\n","Epoch: 288 | Loss: 0.5953344702720642\n","Epoch: 289 | Loss: 0.595100998878479\n","Epoch: 290 | Loss: 0.5948677062988281\n","Epoch: 291 | Loss: 0.5946345329284668\n","Epoch: 292 | Loss: 0.5944015383720398\n","Epoch: 293 | Loss: 0.5941686630249023\n","Epoch: 294 | Loss: 0.593936026096344\n","Epoch: 295 | Loss: 0.5937035083770752\n","Epoch: 296 | Loss: 0.5934711694717407\n","Epoch: 297 | Loss: 0.5932390093803406\n","Epoch: 298 | Loss: 0.5930070281028748\n","Epoch: 299 | Loss: 0.5927751064300537\n","Epoch: 300 | Loss: 0.5925434827804565\n","Epoch: 301 | Loss: 0.5923119187355042\n","Epoch: 302 | Loss: 0.5920805931091309\n","Epoch: 303 | Loss: 0.5918493270874023\n","Epoch: 304 | Loss: 0.5916183590888977\n","Epoch: 305 | Loss: 0.5913873910903931\n","Epoch: 306 | Loss: 0.5911567211151123\n","Epoch: 307 | Loss: 0.5909261703491211\n","Epoch: 308 | Loss: 0.5906957983970642\n","Epoch: 309 | Loss: 0.5904655456542969\n","Epoch: 310 | Loss: 0.5902354717254639\n","Epoch: 311 | Loss: 0.5900055766105652\n","Epoch: 312 | Loss: 0.589775800704956\n","Epoch: 313 | Loss: 0.589546263217926\n","Epoch: 314 | Loss: 0.5893168449401855\n","Epoch: 315 | Loss: 0.5890875458717346\n","Epoch: 316 | Loss: 0.5888584852218628\n","Epoch: 317 | Loss: 0.5886294841766357\n","Epoch: 318 | Loss: 0.5884007811546326\n","Epoch: 319 | Loss: 0.5881721377372742\n","Epoch: 320 | Loss: 0.5879436731338501\n","Epoch: 321 | Loss: 0.5877153873443604\n","Epoch: 322 | Loss: 0.5874872207641602\n","Epoch: 323 | Loss: 0.5872592926025391\n","Epoch: 324 | Loss: 0.5870314240455627\n","Epoch: 325 | Loss: 0.5868037939071655\n","Epoch: 326 | Loss: 0.5865762829780579\n","Epoch: 327 | Loss: 0.5863489508628845\n","Epoch: 328 | Loss: 0.5861217379570007\n","Epoch: 329 | Loss: 0.5858947038650513\n","Epoch: 330 | Loss: 0.5856679081916809\n","Epoch: 331 | Loss: 0.5854411721229553\n","Epoch: 332 | Loss: 0.5852146148681641\n","Epoch: 333 | Loss: 0.5849882364273071\n","Epoch: 334 | Loss: 0.5847620368003845\n","Epoch: 335 | Loss: 0.5845360159873962\n","Epoch: 336 | Loss: 0.5843101143836975\n","Epoch: 337 | Loss: 0.5840842723846436\n","Epoch: 338 | Loss: 0.5838587284088135\n","Epoch: 339 | Loss: 0.5836333632469177\n","Epoch: 340 | Loss: 0.583407998085022\n","Epoch: 341 | Loss: 0.5831829309463501\n","Epoch: 342 | Loss: 0.582957923412323\n","Epoch: 343 | Loss: 0.5827330946922302\n","Epoch: 344 | Loss: 0.5825085639953613\n","Epoch: 345 | Loss: 0.5822840929031372\n","Epoch: 346 | Loss: 0.5820596814155579\n","Epoch: 347 | Loss: 0.5818355679512024\n","Epoch: 348 | Loss: 0.5816115140914917\n","Epoch: 349 | Loss: 0.5813876986503601\n","Epoch: 350 | Loss: 0.5811639428138733\n","Epoch: 351 | Loss: 0.5809404253959656\n","Epoch: 352 | Loss: 0.5807169675827026\n","Epoch: 353 | Loss: 0.5804938077926636\n","Epoch: 354 | Loss: 0.5802706480026245\n","Epoch: 355 | Loss: 0.5800477862358093\n","Epoch: 356 | Loss: 0.5798250436782837\n","Epoch: 357 | Loss: 0.5796024799346924\n","Epoch: 358 | Loss: 0.5793800354003906\n","Epoch: 359 | Loss: 0.5791577100753784\n","Epoch: 360 | Loss: 0.5789355635643005\n","Epoch: 361 | Loss: 0.578713595867157\n","Epoch: 362 | Loss: 0.5784918069839478\n","Epoch: 363 | Loss: 0.5782700777053833\n","Epoch: 364 | Loss: 0.578048586845398\n","Epoch: 365 | Loss: 0.5778271555900574\n","Epoch: 366 | Loss: 0.5776059627532959\n","Epoch: 367 | Loss: 0.5773849487304688\n","Epoch: 368 | Loss: 0.5771639943122864\n","Epoch: 369 | Loss: 0.5769433379173279\n","Epoch: 370 | Loss: 0.5767227411270142\n","Epoch: 371 | Loss: 0.57650226354599\n","Epoch: 372 | Loss: 0.5762819647789001\n","Epoch: 373 | Loss: 0.5760618448257446\n","Epoch: 374 | Loss: 0.5758419036865234\n","Epoch: 375 | Loss: 0.5756220817565918\n","Epoch: 376 | Loss: 0.5754023790359497\n","Epoch: 377 | Loss: 0.5751828551292419\n","Epoch: 378 | Loss: 0.5749634504318237\n","Epoch: 379 | Loss: 0.5747443437576294\n","Epoch: 380 | Loss: 0.5745252370834351\n","Epoch: 381 | Loss: 0.5743063688278198\n","Epoch: 382 | Loss: 0.5740876197814941\n","Epoch: 383 | Loss: 0.5738690495491028\n","Epoch: 384 | Loss: 0.573650598526001\n","Epoch: 385 | Loss: 0.5734322667121887\n","Epoch: 386 | Loss: 0.5732141733169556\n","Epoch: 387 | Loss: 0.5729961395263672\n","Epoch: 388 | Loss: 0.5727783441543579\n","Epoch: 389 | Loss: 0.5725606679916382\n","Epoch: 390 | Loss: 0.5723431706428528\n","Epoch: 391 | Loss: 0.5721257925033569\n","Epoch: 392 | Loss: 0.5719085335731506\n","Epoch: 393 | Loss: 0.5716914534568787\n","Epoch: 394 | Loss: 0.5714746117591858\n","Epoch: 395 | Loss: 0.5712578892707825\n","Epoch: 396 | Loss: 0.5710412263870239\n","Epoch: 397 | Loss: 0.5708247423171997\n","Epoch: 398 | Loss: 0.570608377456665\n","Epoch: 399 | Loss: 0.5703921914100647\n","Epoch: 400 | Loss: 0.5701761841773987\n","Epoch: 401 | Loss: 0.569960355758667\n","Epoch: 402 | Loss: 0.5697446465492249\n","Epoch: 403 | Loss: 0.5695290565490723\n","Epoch: 404 | Loss: 0.569313645362854\n","Epoch: 405 | Loss: 0.5690984129905701\n","Epoch: 406 | Loss: 0.5688832998275757\n","Epoch: 407 | Loss: 0.5686683654785156\n","Epoch: 408 | Loss: 0.5684536099433899\n","Epoch: 409 | Loss: 0.5682389140129089\n","Epoch: 410 | Loss: 0.5680244565010071\n","Epoch: 411 | Loss: 0.5678099989891052\n","Epoch: 412 | Loss: 0.5675958395004272\n","Epoch: 413 | Loss: 0.567381739616394\n","Epoch: 414 | Loss: 0.5671678781509399\n","Epoch: 415 | Loss: 0.5669541358947754\n","Epoch: 416 | Loss: 0.5667405128479004\n","Epoch: 417 | Loss: 0.5665270686149597\n","Epoch: 418 | Loss: 0.5663137435913086\n","Epoch: 419 | Loss: 0.5661005973815918\n","Epoch: 420 | Loss: 0.5658875703811646\n","Epoch: 421 | Loss: 0.5656746625900269\n","Epoch: 422 | Loss: 0.5654619932174683\n","Epoch: 423 | Loss: 0.5652494430541992\n","Epoch: 424 | Loss: 0.5650370121002197\n","Epoch: 425 | Loss: 0.5648247003555298\n","Epoch: 426 | Loss: 0.564612627029419\n","Epoch: 427 | Loss: 0.5644006729125977\n","Epoch: 428 | Loss: 0.5641887784004211\n","Epoch: 429 | Loss: 0.5639771223068237\n","Epoch: 430 | Loss: 0.5637655258178711\n","Epoch: 431 | Loss: 0.5635541677474976\n","Epoch: 432 | Loss: 0.5633429288864136\n","Epoch: 433 | Loss: 0.5631319284439087\n","Epoch: 434 | Loss: 0.562920868396759\n","Epoch: 435 | Loss: 0.5627101063728333\n","Epoch: 436 | Loss: 0.562499463558197\n","Epoch: 437 | Loss: 0.5622889995574951\n","Epoch: 438 | Loss: 0.562078595161438\n","Epoch: 439 | Loss: 0.5618683695793152\n","Epoch: 440 | Loss: 0.5616583228111267\n","Epoch: 441 | Loss: 0.5614483952522278\n","Epoch: 442 | Loss: 0.561238706111908\n","Epoch: 443 | Loss: 0.5610290765762329\n","Epoch: 444 | Loss: 0.5608195662498474\n","Epoch: 445 | Loss: 0.5606102347373962\n","Epoch: 446 | Loss: 0.5604010820388794\n","Epoch: 447 | Loss: 0.5601919889450073\n","Epoch: 448 | Loss: 0.5599830150604248\n","Epoch: 449 | Loss: 0.5597743391990662\n","Epoch: 450 | Loss: 0.5595657229423523\n","Epoch: 451 | Loss: 0.5593572854995728\n","Epoch: 452 | Loss: 0.5591490268707275\n","Epoch: 453 | Loss: 0.5589407682418823\n","Epoch: 454 | Loss: 0.5587326884269714\n","Epoch: 455 | Loss: 0.5585248470306396\n","Epoch: 456 | Loss: 0.5583171248435974\n","Epoch: 457 | Loss: 0.5581095814704895\n","Epoch: 458 | Loss: 0.5579021573066711\n","Epoch: 459 | Loss: 0.5576947927474976\n","Epoch: 460 | Loss: 0.5574876666069031\n","Epoch: 461 | Loss: 0.5572806596755981\n","Epoch: 462 | Loss: 0.5570738315582275\n","Epoch: 463 | Loss: 0.5568670630455017\n","Epoch: 464 | Loss: 0.556660532951355\n","Epoch: 465 | Loss: 0.5564540028572083\n","Epoch: 466 | Loss: 0.5562477111816406\n","Epoch: 467 | Loss: 0.5560416579246521\n","Epoch: 468 | Loss: 0.5558356046676636\n","Epoch: 469 | Loss: 0.5556297898292542\n","Epoch: 470 | Loss: 0.5554240345954895\n","Epoch: 471 | Loss: 0.5552184581756592\n","Epoch: 472 | Loss: 0.5550130605697632\n","Epoch: 473 | Loss: 0.554807722568512\n","Epoch: 474 | Loss: 0.5546025633811951\n","Epoch: 475 | Loss: 0.5543975830078125\n","Epoch: 476 | Loss: 0.5541927814483643\n","Epoch: 477 | Loss: 0.5539880394935608\n","Epoch: 478 | Loss: 0.5537834167480469\n","Epoch: 479 | Loss: 0.5535790324211121\n","Epoch: 480 | Loss: 0.553374707698822\n","Epoch: 481 | Loss: 0.5531705021858215\n","Epoch: 482 | Loss: 0.5529665946960449\n","Epoch: 483 | Loss: 0.5527626276016235\n","Epoch: 484 | Loss: 0.552558958530426\n","Epoch: 485 | Loss: 0.5523554086685181\n","Epoch: 486 | Loss: 0.5521519780158997\n","Epoch: 487 | Loss: 0.551948606967926\n","Epoch: 488 | Loss: 0.5517455339431763\n","Epoch: 489 | Loss: 0.5515424609184265\n","Epoch: 490 | Loss: 0.5513396263122559\n","Epoch: 491 | Loss: 0.55113685131073\n","Epoch: 492 | Loss: 0.5509342551231384\n","Epoch: 493 | Loss: 0.5507318377494812\n","Epoch: 494 | Loss: 0.5505294799804688\n","Epoch: 495 | Loss: 0.5503273010253906\n","Epoch: 496 | Loss: 0.5501253008842468\n","Epoch: 497 | Loss: 0.5499234199523926\n","Epoch: 498 | Loss: 0.5497215986251831\n","Epoch: 499 | Loss: 0.5495200157165527\n","Epoch: 500 | Loss: 0.5493185520172119\n","Epoch: 501 | Loss: 0.5491172671318054\n","Epoch: 502 | Loss: 0.5489160418510437\n","Epoch: 503 | Loss: 0.5487149953842163\n","Epoch: 504 | Loss: 0.5485141277313232\n","Epoch: 505 | Loss: 0.548313319683075\n","Epoch: 506 | Loss: 0.548112690448761\n","Epoch: 507 | Loss: 0.5479121804237366\n","Epoch: 508 | Loss: 0.5477117896080017\n","Epoch: 509 | Loss: 0.547511637210846\n","Epoch: 510 | Loss: 0.5473114848136902\n","Epoch: 511 | Loss: 0.5471116304397583\n","Epoch: 512 | Loss: 0.5469118356704712\n","Epoch: 513 | Loss: 0.5467121601104736\n","Epoch: 514 | Loss: 0.5465126037597656\n","Epoch: 515 | Loss: 0.5463132262229919\n","Epoch: 516 | Loss: 0.5461140275001526\n","Epoch: 517 | Loss: 0.545914888381958\n","Epoch: 518 | Loss: 0.5457159280776978\n","Epoch: 519 | Loss: 0.5455170273780823\n","Epoch: 520 | Loss: 0.5453184247016907\n","Epoch: 521 | Loss: 0.5451198220252991\n","Epoch: 522 | Loss: 0.5449214577674866\n","Epoch: 523 | Loss: 0.5447231531143188\n","Epoch: 524 | Loss: 0.5445249676704407\n","Epoch: 525 | Loss: 0.5443270206451416\n","Epoch: 526 | Loss: 0.5441291332244873\n","Epoch: 527 | Loss: 0.5439314246177673\n","Epoch: 528 | Loss: 0.5437338352203369\n","Epoch: 529 | Loss: 0.543536365032196\n","Epoch: 530 | Loss: 0.5433389544487\n","Epoch: 531 | Loss: 0.5431419014930725\n","Epoch: 532 | Loss: 0.5429447889328003\n","Epoch: 533 | Loss: 0.5427479147911072\n","Epoch: 534 | Loss: 0.5425511002540588\n","Epoch: 535 | Loss: 0.5423545241355896\n","Epoch: 536 | Loss: 0.5421579480171204\n","Epoch: 537 | Loss: 0.5419616103172302\n","Epoch: 538 | Loss: 0.5417653918266296\n","Epoch: 539 | Loss: 0.5415692925453186\n","Epoch: 540 | Loss: 0.5413733720779419\n","Epoch: 541 | Loss: 0.54117751121521\n","Epoch: 542 | Loss: 0.5409817695617676\n","Epoch: 543 | Loss: 0.5407862663269043\n","Epoch: 544 | Loss: 0.5405908226966858\n","Epoch: 545 | Loss: 0.5403954982757568\n","Epoch: 546 | Loss: 0.5402004718780518\n","Epoch: 547 | Loss: 0.5400054454803467\n","Epoch: 548 | Loss: 0.5398105382919312\n","Epoch: 549 | Loss: 0.53961580991745\n","Epoch: 550 | Loss: 0.5394212007522583\n","Epoch: 551 | Loss: 0.539226770401001\n","Epoch: 552 | Loss: 0.5390323996543884\n","Epoch: 553 | Loss: 0.5388382077217102\n","Epoch: 554 | Loss: 0.5386440753936768\n","Epoch: 555 | Loss: 0.5384502410888672\n","Epoch: 556 | Loss: 0.5382564067840576\n","Epoch: 557 | Loss: 0.5380626916885376\n","Epoch: 558 | Loss: 0.5378692150115967\n","Epoch: 559 | Loss: 0.5376757979393005\n","Epoch: 560 | Loss: 0.537482500076294\n","Epoch: 561 | Loss: 0.5372894406318665\n","Epoch: 562 | Loss: 0.5370964407920837\n","Epoch: 563 | Loss: 0.5369035005569458\n","Epoch: 564 | Loss: 0.5367108583450317\n","Epoch: 565 | Loss: 0.5365182161331177\n","Epoch: 566 | Loss: 0.5363256931304932\n","Epoch: 567 | Loss: 0.5361334085464478\n","Epoch: 568 | Loss: 0.5359412431716919\n","Epoch: 569 | Loss: 0.5357491970062256\n","Epoch: 570 | Loss: 0.5355572700500488\n","Epoch: 571 | Loss: 0.5353654623031616\n","Epoch: 572 | Loss: 0.5351737141609192\n","Epoch: 573 | Loss: 0.5349822640419006\n","Epoch: 574 | Loss: 0.5347908139228821\n","Epoch: 575 | Loss: 0.5345996022224426\n","Epoch: 576 | Loss: 0.534408450126648\n","Epoch: 577 | Loss: 0.5342174172401428\n","Epoch: 578 | Loss: 0.534026563167572\n","Epoch: 579 | Loss: 0.533835768699646\n","Epoch: 580 | Loss: 0.5336451530456543\n","Epoch: 581 | Loss: 0.5334547162055969\n","Epoch: 582 | Loss: 0.5332642793655396\n","Epoch: 583 | Loss: 0.5330740809440613\n","Epoch: 584 | Loss: 0.5328840613365173\n","Epoch: 585 | Loss: 0.5326941013336182\n","Epoch: 586 | Loss: 0.5325043201446533\n","Epoch: 587 | Loss: 0.532314658164978\n","Epoch: 588 | Loss: 0.5321249961853027\n","Epoch: 589 | Loss: 0.5319356322288513\n","Epoch: 590 | Loss: 0.5317463278770447\n","Epoch: 591 | Loss: 0.5315570831298828\n","Epoch: 592 | Loss: 0.5313680171966553\n","Epoch: 593 | Loss: 0.5311791896820068\n","Epoch: 594 | Loss: 0.5309903621673584\n","Epoch: 595 | Loss: 0.5308017134666443\n","Epoch: 596 | Loss: 0.5306131839752197\n","Epoch: 597 | Loss: 0.5304248332977295\n","Epoch: 598 | Loss: 0.5302366018295288\n","Epoch: 599 | Loss: 0.5300484299659729\n","Epoch: 600 | Loss: 0.5298604369163513\n","Epoch: 601 | Loss: 0.5296725630760193\n","Epoch: 602 | Loss: 0.5294848084449768\n","Epoch: 603 | Loss: 0.5292972326278687\n","Epoch: 604 | Loss: 0.5291097164154053\n","Epoch: 605 | Loss: 0.5289223194122314\n","Epoch: 606 | Loss: 0.5287351608276367\n","Epoch: 607 | Loss: 0.528548002243042\n","Epoch: 608 | Loss: 0.5283610820770264\n","Epoch: 609 | Loss: 0.5281742215156555\n","Epoch: 610 | Loss: 0.527987539768219\n","Epoch: 611 | Loss: 0.527800977230072\n","Epoch: 612 | Loss: 0.5276145339012146\n","Epoch: 613 | Loss: 0.527428150177002\n","Epoch: 614 | Loss: 0.5272420048713684\n","Epoch: 615 | Loss: 0.5270559191703796\n","Epoch: 616 | Loss: 0.5268699526786804\n","Epoch: 617 | Loss: 0.5266841650009155\n","Epoch: 618 | Loss: 0.5264984369277954\n","Epoch: 619 | Loss: 0.5263128876686096\n","Epoch: 620 | Loss: 0.5261275172233582\n","Epoch: 621 | Loss: 0.5259422063827515\n","Epoch: 622 | Loss: 0.5257570147514343\n","Epoch: 623 | Loss: 0.5255719423294067\n","Epoch: 624 | Loss: 0.5253869891166687\n","Epoch: 625 | Loss: 0.5252022743225098\n","Epoch: 626 | Loss: 0.5250175595283508\n","Epoch: 627 | Loss: 0.5248330235481262\n","Epoch: 628 | Loss: 0.5246486067771912\n","Epoch: 629 | Loss: 0.5244643688201904\n","Epoch: 630 | Loss: 0.5242801904678345\n","Epoch: 631 | Loss: 0.5240961313247681\n","Epoch: 632 | Loss: 0.5239121913909912\n","Epoch: 633 | Loss: 0.5237284302711487\n","Epoch: 634 | Loss: 0.5235447287559509\n","Epoch: 635 | Loss: 0.5233612060546875\n","Epoch: 636 | Loss: 0.5231778621673584\n","Epoch: 637 | Loss: 0.5229945778846741\n","Epoch: 638 | Loss: 0.5228114128112793\n","Epoch: 639 | Loss: 0.5226283669471741\n","Epoch: 640 | Loss: 0.5224454402923584\n","Epoch: 641 | Loss: 0.5222627520561218\n","Epoch: 642 | Loss: 0.5220800638198853\n","Epoch: 643 | Loss: 0.521897554397583\n","Epoch: 644 | Loss: 0.5217151641845703\n","Epoch: 645 | Loss: 0.5215328335762024\n","Epoch: 646 | Loss: 0.5213507413864136\n","Epoch: 647 | Loss: 0.5211686491966248\n","Epoch: 648 | Loss: 0.520986795425415\n","Epoch: 649 | Loss: 0.5208050012588501\n","Epoch: 650 | Loss: 0.5206233859062195\n","Epoch: 651 | Loss: 0.5204418897628784\n","Epoch: 652 | Loss: 0.5202604532241821\n","Epoch: 653 | Loss: 0.5200791358947754\n","Epoch: 654 | Loss: 0.519897997379303\n","Epoch: 655 | Loss: 0.5197169780731201\n","Epoch: 656 | Loss: 0.5195361375808716\n","Epoch: 657 | Loss: 0.519355297088623\n","Epoch: 658 | Loss: 0.5191746354103088\n","Epoch: 659 | Loss: 0.5189940929412842\n","Epoch: 660 | Loss: 0.5188137292861938\n","Epoch: 661 | Loss: 0.5186334252357483\n","Epoch: 662 | Loss: 0.5184533596038818\n","Epoch: 663 | Loss: 0.5182731747627258\n","Epoch: 664 | Loss: 0.5180933475494385\n","Epoch: 665 | Loss: 0.5179134607315063\n","Epoch: 666 | Loss: 0.5177338719367981\n","Epoch: 667 | Loss: 0.5175542831420898\n","Epoch: 668 | Loss: 0.5173748731613159\n","Epoch: 669 | Loss: 0.5171955823898315\n","Epoch: 670 | Loss: 0.5170164108276367\n","Epoch: 671 | Loss: 0.5168373584747314\n","Epoch: 672 | Loss: 0.5166584253311157\n","Epoch: 673 | Loss: 0.5164796113967896\n","Epoch: 674 | Loss: 0.5163009166717529\n","Epoch: 675 | Loss: 0.5161223411560059\n","Epoch: 676 | Loss: 0.5159438848495483\n","Epoch: 677 | Loss: 0.5157655477523804\n","Epoch: 678 | Loss: 0.5155873894691467\n","Epoch: 679 | Loss: 0.5154092907905579\n","Epoch: 680 | Loss: 0.5152313113212585\n","Epoch: 681 | Loss: 0.5150534510612488\n","Epoch: 682 | Loss: 0.5148758292198181\n","Epoch: 683 | Loss: 0.5146982073783875\n","Epoch: 684 | Loss: 0.5145207047462463\n","Epoch: 685 | Loss: 0.5143433213233948\n","Epoch: 686 | Loss: 0.5141661167144775\n","Epoch: 687 | Loss: 0.5139889717102051\n","Epoch: 688 | Loss: 0.5138120651245117\n","Epoch: 689 | Loss: 0.5136350989341736\n","Epoch: 690 | Loss: 0.5134583711624146\n","Epoch: 691 | Loss: 0.5132817625999451\n","Epoch: 692 | Loss: 0.5131053328514099\n","Epoch: 693 | Loss: 0.5129289031028748\n","Epoch: 694 | Loss: 0.5127526521682739\n","Epoch: 695 | Loss: 0.5125764608383179\n","Epoch: 696 | Loss: 0.5124005079269409\n","Epoch: 697 | Loss: 0.512224555015564\n","Epoch: 698 | Loss: 0.5120488405227661\n","Epoch: 699 | Loss: 0.5118731260299683\n","Epoch: 700 | Loss: 0.51169753074646\n","Epoch: 701 | Loss: 0.511522114276886\n","Epoch: 702 | Loss: 0.5113468170166016\n","Epoch: 703 | Loss: 0.5111716389656067\n","Epoch: 704 | Loss: 0.5109965205192566\n","Epoch: 705 | Loss: 0.5108216404914856\n","Epoch: 706 | Loss: 0.5106467604637146\n","Epoch: 707 | Loss: 0.5104721188545227\n","Epoch: 708 | Loss: 0.5102974772453308\n","Epoch: 709 | Loss: 0.5101230144500732\n","Epoch: 710 | Loss: 0.5099486708641052\n","Epoch: 711 | Loss: 0.5097744464874268\n","Epoch: 712 | Loss: 0.5096003413200378\n","Epoch: 713 | Loss: 0.5094263553619385\n","Epoch: 714 | Loss: 0.5092524290084839\n","Epoch: 715 | Loss: 0.5090786814689636\n","Epoch: 716 | Loss: 0.5089050531387329\n","Epoch: 717 | Loss: 0.508731484413147\n","Epoch: 718 | Loss: 0.5085581541061401\n","Epoch: 719 | Loss: 0.5083848237991333\n","Epoch: 720 | Loss: 0.5082117319107056\n","Epoch: 721 | Loss: 0.5080385804176331\n","Epoch: 722 | Loss: 0.5078656673431396\n","Epoch: 723 | Loss: 0.5076928734779358\n","Epoch: 724 | Loss: 0.5075201392173767\n","Epoch: 725 | Loss: 0.507347583770752\n","Epoch: 726 | Loss: 0.507175087928772\n","Epoch: 727 | Loss: 0.5070027709007263\n","Epoch: 728 | Loss: 0.5068305134773254\n","Epoch: 729 | Loss: 0.5066583752632141\n","Epoch: 730 | Loss: 0.5064864158630371\n","Epoch: 731 | Loss: 0.5063145756721497\n","Epoch: 732 | Loss: 0.5061427354812622\n","Epoch: 733 | Loss: 0.5059711933135986\n","Epoch: 734 | Loss: 0.5057995915412903\n","Epoch: 735 | Loss: 0.5056281089782715\n","Epoch: 736 | Loss: 0.5054568648338318\n","Epoch: 737 | Loss: 0.5052857398986816\n","Epoch: 738 | Loss: 0.5051146149635315\n","Epoch: 739 | Loss: 0.5049436688423157\n","Epoch: 740 | Loss: 0.5047728419303894\n","Epoch: 741 | Loss: 0.5046021342277527\n","Epoch: 742 | Loss: 0.5044315457344055\n","Epoch: 743 | Loss: 0.5042610168457031\n","Epoch: 744 | Loss: 0.5040906667709351\n","Epoch: 745 | Loss: 0.5039204359054565\n","Epoch: 746 | Loss: 0.5037502646446228\n","Epoch: 747 | Loss: 0.5035802721977234\n","Epoch: 748 | Loss: 0.5034103393554688\n","Epoch: 749 | Loss: 0.5032405257225037\n","Epoch: 750 | Loss: 0.5030708909034729\n","Epoch: 751 | Loss: 0.5029013156890869\n","Epoch: 752 | Loss: 0.5027318596839905\n","Epoch: 753 | Loss: 0.5025625228881836\n","Epoch: 754 | Loss: 0.5023933053016663\n","Epoch: 755 | Loss: 0.5022242069244385\n","Epoch: 756 | Loss: 0.5020552277565002\n","Epoch: 757 | Loss: 0.5018863677978516\n","Epoch: 758 | Loss: 0.5017175674438477\n","Epoch: 759 | Loss: 0.5015489459037781\n","Epoch: 760 | Loss: 0.501380443572998\n","Epoch: 761 | Loss: 0.5012120008468628\n","Epoch: 762 | Loss: 0.5010436773300171\n","Epoch: 763 | Loss: 0.5008754134178162\n","Epoch: 764 | Loss: 0.5007073879241943\n","Epoch: 765 | Loss: 0.5005394220352173\n","Epoch: 766 | Loss: 0.5003715753555298\n","Epoch: 767 | Loss: 0.5002038478851318\n","Epoch: 768 | Loss: 0.5000362396240234\n","Epoch: 769 | Loss: 0.4998687207698822\n","Epoch: 770 | Loss: 0.4997013211250305\n","Epoch: 771 | Loss: 0.4995340406894684\n","Epoch: 772 | Loss: 0.499366819858551\n","Epoch: 773 | Loss: 0.4991998076438904\n","Epoch: 774 | Loss: 0.49903279542922974\n","Epoch: 775 | Loss: 0.49886608123779297\n","Epoch: 776 | Loss: 0.4986993074417114\n","Epoch: 777 | Loss: 0.4985327422618866\n","Epoch: 778 | Loss: 0.49836617708206177\n","Epoch: 779 | Loss: 0.49819982051849365\n","Epoch: 780 | Loss: 0.4980335235595703\n","Epoch: 781 | Loss: 0.4978673458099365\n","Epoch: 782 | Loss: 0.4977013170719147\n","Epoch: 783 | Loss: 0.4975353479385376\n","Epoch: 784 | Loss: 0.49736955761909485\n","Epoch: 785 | Loss: 0.49720385670661926\n","Epoch: 786 | Loss: 0.49703824520111084\n","Epoch: 787 | Loss: 0.49687275290489197\n","Epoch: 788 | Loss: 0.4967074394226074\n","Epoch: 789 | Loss: 0.4965420961380005\n","Epoch: 790 | Loss: 0.49637699127197266\n","Epoch: 791 | Loss: 0.4962118864059448\n","Epoch: 792 | Loss: 0.4960469603538513\n","Epoch: 793 | Loss: 0.49588215351104736\n","Epoch: 794 | Loss: 0.49571746587753296\n","Epoch: 795 | Loss: 0.49555283784866333\n","Epoch: 796 | Loss: 0.49538835883140564\n","Epoch: 797 | Loss: 0.4952239692211151\n","Epoch: 798 | Loss: 0.4950597584247589\n","Epoch: 799 | Loss: 0.4948956370353699\n","Epoch: 800 | Loss: 0.4947315454483032\n","Epoch: 801 | Loss: 0.4945676028728485\n","Epoch: 802 | Loss: 0.49440377950668335\n","Epoch: 803 | Loss: 0.49424007534980774\n","Epoch: 804 | Loss: 0.4940764605998993\n","Epoch: 805 | Loss: 0.4939129948616028\n","Epoch: 806 | Loss: 0.49374955892562866\n","Epoch: 807 | Loss: 0.4935862421989441\n","Epoch: 808 | Loss: 0.4934231638908386\n","Epoch: 809 | Loss: 0.49326005578041077\n","Epoch: 810 | Loss: 0.49309712648391724\n","Epoch: 811 | Loss: 0.49293431639671326\n","Epoch: 812 | Loss: 0.49277156591415405\n","Epoch: 813 | Loss: 0.4926089644432068\n","Epoch: 814 | Loss: 0.4924464225769043\n","Epoch: 815 | Loss: 0.4922840893268585\n","Epoch: 816 | Loss: 0.49212175607681274\n","Epoch: 817 | Loss: 0.4919595420360565\n","Epoch: 818 | Loss: 0.491797536611557\n","Epoch: 819 | Loss: 0.4916355311870575\n","Epoch: 820 | Loss: 0.4914736747741699\n","Epoch: 821 | Loss: 0.4913119375705719\n","Epoch: 822 | Loss: 0.49115025997161865\n","Epoch: 823 | Loss: 0.49098873138427734\n","Epoch: 824 | Loss: 0.49082741141319275\n","Epoch: 825 | Loss: 0.4906660318374634\n","Epoch: 826 | Loss: 0.4905048608779907\n","Epoch: 827 | Loss: 0.49034371972084045\n","Epoch: 828 | Loss: 0.4901827573776245\n","Epoch: 829 | Loss: 0.49002185463905334\n","Epoch: 830 | Loss: 0.48986107110977173\n","Epoch: 831 | Loss: 0.48970043659210205\n","Epoch: 832 | Loss: 0.48953989148139954\n","Epoch: 833 | Loss: 0.4893794059753418\n","Epoch: 834 | Loss: 0.48921912908554077\n","Epoch: 835 | Loss: 0.48905885219573975\n","Epoch: 836 | Loss: 0.48889869451522827\n","Epoch: 837 | Loss: 0.4887387156486511\n","Epoch: 838 | Loss: 0.48857876658439636\n","Epoch: 839 | Loss: 0.48841896653175354\n","Epoch: 840 | Loss: 0.48825931549072266\n","Epoch: 841 | Loss: 0.4880996644496918\n","Epoch: 842 | Loss: 0.4879401922225952\n","Epoch: 843 | Loss: 0.48778077960014343\n","Epoch: 844 | Loss: 0.487621545791626\n","Epoch: 845 | Loss: 0.4874624013900757\n","Epoch: 846 | Loss: 0.4873032867908478\n","Epoch: 847 | Loss: 0.4871443808078766\n","Epoch: 848 | Loss: 0.48698559403419495\n","Epoch: 849 | Loss: 0.48682674765586853\n","Epoch: 850 | Loss: 0.4866681396961212\n","Epoch: 851 | Loss: 0.48650965094566345\n","Epoch: 852 | Loss: 0.4863511919975281\n","Epoch: 853 | Loss: 0.486192911863327\n","Epoch: 854 | Loss: 0.48603469133377075\n","Epoch: 855 | Loss: 0.48587659001350403\n","Epoch: 856 | Loss: 0.4857185482978821\n","Epoch: 857 | Loss: 0.48556068539619446\n","Epoch: 858 | Loss: 0.4854028820991516\n","Epoch: 859 | Loss: 0.4852451980113983\n","Epoch: 860 | Loss: 0.48508766293525696\n","Epoch: 861 | Loss: 0.4849301874637604\n","Epoch: 862 | Loss: 0.4847727417945862\n","Epoch: 863 | Loss: 0.4846155047416687\n","Epoch: 864 | Loss: 0.4844583570957184\n","Epoch: 865 | Loss: 0.48430129885673523\n","Epoch: 866 | Loss: 0.4841443598270416\n","Epoch: 867 | Loss: 0.4839874804019928\n","Epoch: 868 | Loss: 0.4838308095932007\n","Epoch: 869 | Loss: 0.48367413878440857\n","Epoch: 870 | Loss: 0.4835175573825836\n","Epoch: 871 | Loss: 0.48336121439933777\n","Epoch: 872 | Loss: 0.48320484161376953\n","Epoch: 873 | Loss: 0.48304861783981323\n","Epoch: 874 | Loss: 0.4828925132751465\n","Epoch: 875 | Loss: 0.4827364981174469\n","Epoch: 876 | Loss: 0.4825805723667145\n","Epoch: 877 | Loss: 0.4824247360229492\n","Epoch: 878 | Loss: 0.48226916790008545\n","Epoch: 879 | Loss: 0.4821135103702545\n","Epoch: 880 | Loss: 0.4819580018520355\n","Epoch: 881 | Loss: 0.4818026125431061\n","Epoch: 882 | Loss: 0.4816472828388214\n","Epoch: 883 | Loss: 0.48149216175079346\n","Epoch: 884 | Loss: 0.4813370704650879\n","Epoch: 885 | Loss: 0.4811820983886719\n","Epoch: 886 | Loss: 0.4810272455215454\n","Epoch: 887 | Loss: 0.4808724522590637\n","Epoch: 888 | Loss: 0.4807177782058716\n","Epoch: 889 | Loss: 0.480563223361969\n","Epoch: 890 | Loss: 0.48040875792503357\n","Epoch: 891 | Loss: 0.4802543818950653\n","Epoch: 892 | Loss: 0.480100154876709\n","Epoch: 893 | Loss: 0.4799460172653198\n","Epoch: 894 | Loss: 0.4797919988632202\n","Epoch: 895 | Loss: 0.4796379804611206\n","Epoch: 896 | Loss: 0.47948411107063293\n","Epoch: 897 | Loss: 0.4793304204940796\n","Epoch: 898 | Loss: 0.47917675971984863\n","Epoch: 899 | Loss: 0.4790232181549072\n","Epoch: 900 | Loss: 0.47886979579925537\n","Epoch: 901 | Loss: 0.4787164628505707\n","Epoch: 902 | Loss: 0.47856321930885315\n","Epoch: 903 | Loss: 0.4784100651741028\n","Epoch: 904 | Loss: 0.47825703024864197\n","Epoch: 905 | Loss: 0.4781041741371155\n","Epoch: 906 | Loss: 0.4779512584209442\n","Epoch: 907 | Loss: 0.47779858112335205\n","Epoch: 908 | Loss: 0.4776459038257599\n","Epoch: 909 | Loss: 0.47749340534210205\n","Epoch: 910 | Loss: 0.4773409962654114\n","Epoch: 911 | Loss: 0.4771886467933655\n","Epoch: 912 | Loss: 0.4770364463329315\n","Epoch: 913 | Loss: 0.47688430547714233\n","Epoch: 914 | Loss: 0.4767322540283203\n","Epoch: 915 | Loss: 0.4765803813934326\n","Epoch: 916 | Loss: 0.4764285385608673\n","Epoch: 917 | Loss: 0.47627681493759155\n","Epoch: 918 | Loss: 0.47612521052360535\n","Epoch: 919 | Loss: 0.4759736657142639\n","Epoch: 920 | Loss: 0.47582221031188965\n","Epoch: 921 | Loss: 0.4756709039211273\n","Epoch: 922 | Loss: 0.47551971673965454\n","Epoch: 923 | Loss: 0.47536858916282654\n","Epoch: 924 | Loss: 0.4752175807952881\n","Epoch: 925 | Loss: 0.4750666320323944\n","Epoch: 926 | Loss: 0.47491586208343506\n","Epoch: 927 | Loss: 0.4747651219367981\n","Epoch: 928 | Loss: 0.4746145009994507\n","Epoch: 929 | Loss: 0.47446393966674805\n","Epoch: 930 | Loss: 0.47431352734565735\n","Epoch: 931 | Loss: 0.4741631746292114\n","Epoch: 932 | Loss: 0.474013090133667\n","Epoch: 933 | Loss: 0.473862886428833\n","Epoch: 934 | Loss: 0.4737129211425781\n","Epoch: 935 | Loss: 0.47356295585632324\n","Epoch: 936 | Loss: 0.4734131395816803\n","Epoch: 937 | Loss: 0.47326335310935974\n","Epoch: 938 | Loss: 0.4731137454509735\n","Epoch: 939 | Loss: 0.47296419739723206\n","Epoch: 940 | Loss: 0.4728148281574249\n","Epoch: 941 | Loss: 0.4726654291152954\n","Epoch: 942 | Loss: 0.4725162386894226\n","Epoch: 943 | Loss: 0.4723670482635498\n","Epoch: 944 | Loss: 0.47221803665161133\n","Epoch: 945 | Loss: 0.4720690846443176\n","Epoch: 946 | Loss: 0.4719202518463135\n","Epoch: 947 | Loss: 0.4717714786529541\n","Epoch: 948 | Loss: 0.47162288427352905\n","Epoch: 949 | Loss: 0.4714743196964264\n","Epoch: 950 | Loss: 0.47132590413093567\n","Epoch: 951 | Loss: 0.47117751836776733\n","Epoch: 952 | Loss: 0.47102925181388855\n","Epoch: 953 | Loss: 0.4708811044692993\n","Epoch: 954 | Loss: 0.47073304653167725\n","Epoch: 955 | Loss: 0.47058507800102234\n","Epoch: 956 | Loss: 0.4704371690750122\n","Epoch: 957 | Loss: 0.4702894389629364\n","Epoch: 958 | Loss: 0.47014176845550537\n","Epoch: 959 | Loss: 0.4699941575527191\n","Epoch: 960 | Loss: 0.4698466360569\n","Epoch: 961 | Loss: 0.46969929337501526\n","Epoch: 962 | Loss: 0.46955206990242004\n","Epoch: 963 | Loss: 0.46940484642982483\n","Epoch: 964 | Loss: 0.46925780177116394\n","Epoch: 965 | Loss: 0.46911078691482544\n","Epoch: 966 | Loss: 0.4689638614654541\n","Epoch: 967 | Loss: 0.4688170254230499\n","Epoch: 968 | Loss: 0.4686703681945801\n","Epoch: 969 | Loss: 0.4685237407684326\n","Epoch: 970 | Loss: 0.4683772921562195\n","Epoch: 971 | Loss: 0.46823081374168396\n","Epoch: 972 | Loss: 0.4680844843387604\n","Epoch: 973 | Loss: 0.46793830394744873\n","Epoch: 974 | Loss: 0.4677921533584595\n","Epoch: 975 | Loss: 0.46764612197875977\n","Epoch: 976 | Loss: 0.4675001800060272\n","Epoch: 977 | Loss: 0.46735435724258423\n","Epoch: 978 | Loss: 0.467208594083786\n","Epoch: 979 | Loss: 0.46706292033195496\n","Epoch: 980 | Loss: 0.46691739559173584\n","Epoch: 981 | Loss: 0.4667719602584839\n","Epoch: 982 | Loss: 0.4666266143321991\n","Epoch: 983 | Loss: 0.4664812982082367\n","Epoch: 984 | Loss: 0.46633613109588623\n","Epoch: 985 | Loss: 0.46619102358818054\n","Epoch: 986 | Loss: 0.4660460352897644\n","Epoch: 987 | Loss: 0.4659011662006378\n","Epoch: 988 | Loss: 0.4657563865184784\n","Epoch: 989 | Loss: 0.46561169624328613\n","Epoch: 990 | Loss: 0.46546706557273865\n","Epoch: 991 | Loss: 0.4653225839138031\n","Epoch: 992 | Loss: 0.46517810225486755\n","Epoch: 993 | Loss: 0.46503376960754395\n","Epoch: 994 | Loss: 0.4648895561695099\n","Epoch: 995 | Loss: 0.46474549174308777\n","Epoch: 996 | Loss: 0.4646014869213104\n","Epoch: 997 | Loss: 0.46445751190185547\n","Epoch: 998 | Loss: 0.4643136262893677\n","Epoch: 999 | Loss: 0.46416985988616943\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2BCZDqOvaSV","executionInfo":{"status":"ok","timestamp":1632754543835,"user_tz":-540,"elapsed":59,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"d9770c8a-7a09-4880-9df3-db434823664e"},"source":["# After training\n","print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n","hour_var = model(tensor([[1.0]]))\n","print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n","hour_var = model(tensor([[7.0]]))\n","print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Let's predict the hours need to score above 50%\n","==================================================\n","Prediction after 1 hour of training: 0.3903 | Above 50%: False\n","Prediction after 7 hours of training: 0.9677 | Above 50%: True\n"]}]}]}
